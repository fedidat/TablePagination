{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Pagination Criteria Notebook\n",
    "\n",
    "This notebook generates pagination criteria for each table using three methods: naive (full-table fetch), LLM-based (OpenRouter), and statistical cardinality estimation. Results are saved as JSON files in a timestamped folder under `processing/1_pagination/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate Latest Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'processing/0_data/'\n",
    "folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]\n",
    "latest_folder = sorted(folders)[-1] if folders else None\n",
    "data_path = os.path.join(data_root, latest_folder) if latest_folder else None\n",
    "assert data_path and os.path.exists(data_path), 'No data folder found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Table JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob(os.path.join(data_path, '*.json'))\n",
    "tables = []\n",
    "for jf in json_files:\n",
    "    with open(jf, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    # Find corresponding CSV\n",
    "    csv_path = jf.replace('.json', '.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f'Missing CSV for {jf}')\n",
    "        continue\n",
    "    tables.append({'path': jf, 'meta': obj['meta'], 'csv': csv_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pagination Criteria Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_criteria(table):\n",
    "    return [{\n",
    "        'criteria': 'none',\n",
    "        'expected_page_size': len(table),\n",
    "        'estimated_pages': 1,\n",
    "        'expected_page_variance': 0,\n",
    "        'pages': ['all']\n",
    "    }]\n",
    "\n",
    "def statistical_cardinality_criteria(df):\n",
    "    # Placeholder: select columns with lowest variance\n",
    "    variances = df.var(numeric_only=True)\n",
    "    if not variances.empty:\n",
    "        col = variances.idxmin()\n",
    "        return [{\n",
    "            'criteria': col,\n",
    "            'expected_page_size': int(df.shape[0] / 5),\n",
    "            'estimated_pages': 5,\n",
    "            'expected_page_variance': int(variances.min()),\n",
    "            'pages': []  # Could be filled with unique values or ranges\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run timestamp: 20250831_003454\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "RUN_CRITERIA = 'all'  # one of: 'naive', 'statistical', 'llm', 'all'\n",
    "RUN_MODELS = [\n",
    "    'google/gemini-2.5-flash-lite',\n",
    "    'deepseek/deepseek-chat-v3.1',\n",
    "    'openai/gpt-4o-mini',\n",
    " ]  # used only when RUN_CRITERIA in {'llm','all'}\n",
    "\n",
    "NUM_TABLES = 5\n",
    "\n",
    "# Restrict which tables to run (None = all). Values are table 'name' from meta.\n",
    "TABLE_WHITELIST = None  # e.g. ['south_african_class_15f_4_8_2']\n",
    "TABLE_BLACKLIST = []\n",
    "TABLE_LIMIT = None  # e.g. 5\n",
    "\n",
    "# Heuristic targets\n",
    "PAGE_SIZE_TARGET = 50\n",
    "MIN_PAGES = 3\n",
    "MAX_PAGES = 10\n",
    "MIN_ROWS_THRESHOLD = 100  # only consider tables with >100 rows\n",
    "\n",
    "# Re-run behavior\n",
    "SKIP_IF_EXISTS = False  # skip writing if output exists for given criteria/model/table\n",
    "\n",
    "# OpenRouter config (read API key from env var OPENROUTER_API_KEY)\n",
    "OPENROUTER_BASE_URL = 'https://openrouter.ai/api/v1'\n",
    "OPENROUTER_API_KEY_ENV = 'OPENROUTER_API_KEY'\n",
    "OPENROUTER_ORIGIN = 'openrouter'  # label only\n",
    "\n",
    "import os, json, time, re, math, random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_TS = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print('Run timestamp:', RUN_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API key: ***\n"
     ]
    }
   ],
   "source": [
    "# --- Load OpenRouter API key from launchctl and set in os.environ ---\n",
    "import subprocess\n",
    "def load_openrouter_api_key():\n",
    "    try:\n",
    "        result = subprocess.run(['launchctl', 'getenv', OPENROUTER_API_KEY_ENV], capture_output=True, text=True)\n",
    "        api_key = result.stdout.strip()\n",
    "        if api_key:\n",
    "            os.environ[OPENROUTER_API_KEY_ENV] = api_key\n",
    "        else:\n",
    "            print(f'API key not found in launchctl for {OPENROUTER_API_KEY_ENV}')\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        print('Error loading API key:', e)\n",
    "        return ''\n",
    "\n",
    "api_key = load_openrouter_api_key()\n",
    "print('Loaded API key:', '***' if api_key else '(none)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bf15fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM prompt for pagination criteria ---\n",
    "def build_llm_prompt(meta: dict, csv_path: str) -> str:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols = list(df.columns)\n",
    "    head = df.head(20).to_string(index=False)\n",
    "    query = meta.get('query_without_cutoff') or meta.get('table_title') or meta.get('name')\n",
    "    tmpl = f'''\n",
    "### Task\n",
    "Recommend efficient pagination strategies for the following table.\n",
    "- Aim for balanced, predictable page sizes.\n",
    "- Base on available columns when possible.\n",
    "- Include a complete list of page keys to request.\n",
    "- If the full list cannot be determined, return an empty list of recommendations.\n",
    "- Prefer around {PAGE_SIZE_TARGET} rows per page and no more than {MAX_PAGES} pages.\n",
    "\n",
    "Return raw JSON only: \n",
    "[{{\n",
    "  \"criteria\": \"comma-separated column names used for pagination\",\n",
    "  \"expected_page_size\": 50,\n",
    "  \"expected_page_variance\": 10,\n",
    "  \"estimated_pages\": 3,\n",
    "  \"pages\": [\"full list of page keys/ranges to request downstream\"]\n",
    "}}]\n",
    "\n",
    "### Input\n",
    "Query: {query}\n",
    "Columns: {', '.join(cols)}\n",
    "Table head:\n",
    "{head}\n",
    "'''\n",
    "    return tmpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Naive recommendation ---\n",
    "def naive_recommendation(meta: dict, csv_path: str):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        n = df.shape[0]\n",
    "        return {\n",
    "            'criteria': '',\n",
    "            'estimated_pages': 1,\n",
    "            'pages': ['ALL'],\n",
    "            'expected_page_size': n,\n",
    "            'expected_page_variance': 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print('Naive recommendation error:', e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistical cardinality recommendation ---\n",
    "def statistical_cardinality_recommendation(meta: dict, csv_path: str, sample_size: int = 100):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        n = df.shape[0]\n",
    "        if n == 0:\n",
    "            return {}\n",
    "        sample_n = min(sample_size, n)\n",
    "        df_sample = df.sample(n=sample_n, random_state=42) if n > sample_n else df\n",
    "        variances = df_sample.var(numeric_only=True)\n",
    "        if not variances.empty:\n",
    "            col = variances.idxmin()\n",
    "            return {\n",
    "                'criteria': col,\n",
    "                'estimated_pages': min(max(int(n / PAGE_SIZE_TARGET), MIN_PAGES), MAX_PAGES),\n",
    "                'pages': [],\n",
    "                'expected_page_size': int(n / max(1, min(max(int(n / PAGE_SIZE_TARGET), MIN_PAGES), MAX_PAGES))),\n",
    "                'expected_page_variance': int(variances.min())\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print('Statistical cardinality error:', e)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM retry 1 for model deepseek/deepseek-chat-v3.1 on table australia_demographics_1900_2010\n",
      "LLM retry 1 for model openai/gpt-4o-mini on table rock_band_downloadable_2011\n",
      "LLM retry 2 for model openai/gpt-4o-mini on table rock_band_downloadable_2011\n",
      "LLM retry 3 for model openai/gpt-4o-mini on table rock_band_downloadable_2011\n",
      "Skipping table rock_band_downloadable_2011 for model openai/gpt-4o-mini due to repeated LLM failure.\n",
      "LLM retry 1 for model deepseek/deepseek-chat-v3.1 on table living_proof_the_farewell_tour\n",
      "Done. Results saved to processing/1_pagination/20250831_003454\n"
     ]
    }
   ],
   "source": [
    "output_root = 'processing/1_pagination/'\n",
    "timestamp = RUN_TS\n",
    "output_folder = os.path.join(output_root, timestamp)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Testing mode: only run 2 tables if TESTING is True ---\n",
    "table_iter = tables[:NUM_TABLES] if NUM_TABLES else tables\n",
    "for table in table_iter:\n",
    "    meta = table['meta']\n",
    "    csv_path = table['csv']\n",
    "    if TABLE_WHITELIST and meta.get('name') not in TABLE_WHITELIST:\n",
    "        continue\n",
    "    if TABLE_BLACKLIST and meta.get('name') in TABLE_BLACKLIST:\n",
    "        continue\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f'Error loading CSV {csv_path}:', e)\n",
    "        continue\n",
    "    if df.shape[0] < MIN_ROWS_THRESHOLD:\n",
    "        continue\n",
    "    result = {}\n",
    "    if RUN_CRITERIA in {'naive', 'all'}:\n",
    "        result['naive'] = naive_recommendation(meta, csv_path)\n",
    "    if RUN_CRITERIA in {'statistical', 'all'}:\n",
    "        result['statistical'] = statistical_cardinality_recommendation(meta, csv_path)\n",
    "    if RUN_CRITERIA in {'llm', 'all'}:\n",
    "        result['llm'] = {}\n",
    "        for model in RUN_MODELS:\n",
    "            llm_result = None\n",
    "            for attempt in range(3):\n",
    "                llm_result = query_llm_for_criteria(meta, csv_path, model)\n",
    "                # Accept result if not None and not empty list\n",
    "                if llm_result is not None and (not isinstance(llm_result, list) or len(llm_result) > 0):\n",
    "                    break\n",
    "                print(f'LLM retry {attempt+1} for model {model} on table {meta.get(\"name\", \"?\")}')\n",
    "            if llm_result is not None and (not isinstance(llm_result, list) or len(llm_result) > 0):\n",
    "                result['llm'][model] = llm_result\n",
    "            else:\n",
    "                print(f'Skipping table {meta.get(\"name\", \"?\")} for model {model} due to repeated LLM failure.')\n",
    "    out_payload = {'meta': meta, 'pagination_criteria': result}\n",
    "    out_path = os.path.join(output_folder, os.path.basename(table['path']))\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(out_payload, f, indent=2)\n",
    "    # Copy CSV file to output folder\n",
    "    out_csv_path = os.path.join(output_folder, os.path.basename(csv_path))\n",
    "    df.to_csv(out_csv_path, index=False)\n",
    "print('Done. Results saved to', output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
