{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a6b9e4",
   "metadata": {},
   "source": [
    "# Stage 1: Pagination Criteria Notebook\n",
    "\n",
    "This notebook generates pagination criteria for each table using three methods: naive (full-table fetch), LLM-based (OpenRouter), and statistical cardinality estimation. Results are saved as JSON files in a timestamped folder under `processing/1_pagination/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d9c13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d12df",
   "metadata": {},
   "source": [
    "## Locate Latest Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a00a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'processing/0_data/'\n",
    "folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]\n",
    "latest_folder = sorted(folders)[-1] if folders else None\n",
    "data_path = os.path.join(data_root, latest_folder) if latest_folder else None\n",
    "assert data_path and os.path.exists(data_path), 'No data folder found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577d6ed",
   "metadata": {},
   "source": [
    "## Load Table JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf54264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob(os.path.join(data_path, '*.json'))\n",
    "tables = []\n",
    "for jf in json_files:\n",
    "    with open(jf, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    # Find corresponding CSV\n",
    "    csv_path = jf.replace('.json', '.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f'Missing CSV for {jf}')\n",
    "        continue\n",
    "    tables.append({'path': jf, 'meta': obj['meta'], 'csv': csv_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4602d60",
   "metadata": {},
   "source": [
    "## Define Pagination Criteria Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ed75593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_criteria(table):\n",
    "    return [{\n",
    "        'criteria': 'none',\n",
    "        'expected_page_size': len(table),\n",
    "        'estimated_pages': 1,\n",
    "        'expected_page_variance': 0,\n",
    "        'pages': ['all']\n",
    "    }]\n",
    "\n",
    "def statistical_cardinality_criteria(df):\n",
    "    # Placeholder: select columns with lowest variance\n",
    "    variances = df.var(numeric_only=True)\n",
    "    if not variances.empty:\n",
    "        col = variances.idxmin()\n",
    "        return [{\n",
    "            'criteria': col,\n",
    "            'expected_page_size': int(df.shape[0] / 5),\n",
    "            'estimated_pages': 5,\n",
    "            'expected_page_variance': int(variances.min()),\n",
    "            'pages': []  # Could be filled with unique values or ranges\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a415718",
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run timestamp: 20250903_223321\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "RUN_CRITERIA = 'all'  # one of: 'naive', 'statistical', 'llm', 'all'\n",
    "RUN_MODELS = [\n",
    "    'google/gemini-2.5-flash-lite',\n",
    "    'deepseek/deepseek-chat-v3.1',\n",
    "    'openai/gpt-4o-mini',\n",
    " ]  # used only when RUN_CRITERIA in {'llm','all'}\n",
    "\n",
    "# Restrict which tables to run (None = all). Values are table 'name' from meta.\n",
    "TABLE_WHITELIST = None  # e.g. ['south_african_class_15f_4_8_2']\n",
    "TABLE_BLACKLIST = []\n",
    "TABLE_LIMIT = None  # e.g. 5\n",
    "\n",
    "# Heuristic targets\n",
    "PAGE_SIZE_TARGET = 50\n",
    "MIN_PAGES = 3\n",
    "MAX_PAGES = 10\n",
    "MIN_ROWS_THRESHOLD = 100  # only consider tables with >100 rows\n",
    "\n",
    "# Re-run behavior\n",
    "SKIP_IF_EXISTS = False  # skip writing if output exists for given criteria/model/table\n",
    "\n",
    "# OpenRouter config (read API key from env var OPENROUTER_API_KEY)\n",
    "OPENROUTER_BASE_URL = 'https://openrouter.ai/api/v1'\n",
    "# OPENROUTER_API_KEY_ENV = 'OPENROUTER_API_KEY'\n",
    "OPENROUTER_ORIGIN = 'openrouter'  # label only\n",
    "\n",
    "import os, json, time, re, math, random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_TS = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print('Run timestamp:', RUN_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6acda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf15fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM prompt for pagination criteria ---\n",
    "def build_llm_prompt(meta: dict, csv_path: str) -> str:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols = list(df.columns)\n",
    "    # Use 20 random rows instead of head\n",
    "    if df.shape[0] > 20:\n",
    "        sample_df = df.sample(n=20, random_state=42)\n",
    "    else:\n",
    "        sample_df = df\n",
    "    head = sample_df.to_string(index=False)\n",
    "    query = meta.get('query_without_cutoff') or meta.get('table_title') or meta.get('name')\n",
    "    tmpl = '''\n",
    "### **Task Explanation**\n",
    "\n",
    "You are a data scientist specializing in tabular data retrieval and result organization.\n",
    "Your task is to recommend efficient **pagination strategies** for large tables.\n",
    "\n",
    "Pagination should:\n",
    "\n",
    "* Allow balanced, predictable page sizes.\n",
    "* Be based on available columns whenever possible.\n",
    "* Include a **complete list of page keys to request**.\n",
    "* **If the full list of pages cannot be determined, omit the criteria entirely.**\n",
    "* Keep in mind that the table head is very partial, and the full table may have many more rows and a wider range of values.\n",
    "* Aim for an **average** page size of around %d rows** and no more than 5 pages. Ranges are acceptable e.g \"1960-1965\".\n",
    "\n",
    "---\n",
    "\n",
    "### **Output Format**\n",
    "\n",
    "Return up to **3 recommended pagination criteria**, ordered from best to worst.\n",
    "Output must be **raw JSON** in the following format:\n",
    "\n",
    "```json\n",
    "[\n",
    "{\n",
    "    \"criteria\": \"comma-separated column names used for pagination\",\n",
    "    \"expected_page_size\": \"approximate number of rows per page and variance, integer ONLY\",\n",
    "    \"estimated_pages\": \"approximate number of total pages, integer ONLY\",\n",
    "    \"expected_page_variance\": \"approximate variance in page size, integer ONLY\",\n",
    "    \"pages\": [\"full list of page keys/ranges to request downstream\"]\n",
    "}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Input**\n",
    "\n",
    "```\n",
    "Query: Retrieve all Nobel Prize winners in Physics  \n",
    "Columns: year, laureate_name, country  \n",
    "Table head:  \n",
    "year | laureate_name   | country  \n",
    "1901 | Wilhelm RÃ¶ntgen | Germany  \n",
    "1902 | Hendrik Lorentz | Netherlands  \n",
    "1902 | Pieter Zeeman   | Netherlands  \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Output**\n",
    "\n",
    "```json\n",
    "[\n",
    "{\n",
    "    \"criteria\": \"year\",\n",
    "    \"expected_page_size\": \"50\",\n",
    "    \"expected_page_variance\": \"10\",\n",
    "    \"estimated_pages\": 3,\n",
    "    \"pages\": [\"1901-1920\", \"1921-1940\", \"1941-1960\"]\n",
    "},\n",
    "{\n",
    "    \"criteria\": \"first_letter_of_laureate_name\",\n",
    "    \"expected_page_size\": \"30\",\n",
    "    \"expected_page_variance\": \"15\",\n",
    "    \"estimated_pages\": 9,\n",
    "    \"pages\": [\"A-C\", \"D-F\", \"G-I\", \"J-L\", \"M-O\", \"P-R\", \"S-U\", \"V-X\", \"Y-Z\"]\n",
    "}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Actual Input**\n",
    "'''\n",
    "    query = f'''\n",
    "### Input\n",
    "Query: {query}\n",
    "Columns: {', '.join(cols)}\n",
    "Table head:\n",
    "{head}\n",
    "'''\n",
    "    return tmpl + query\n",
    "\n",
    "def strip_code_fences(txt):\n",
    "    return txt.replace('```json', '').replace('```', '').strip()\n",
    "\n",
    "def parse_llm_recommendations(txt: str):\n",
    "    if not txt:\n",
    "        return []\n",
    "    s = strip_code_fences(txt)\n",
    "    try:\n",
    "        data = json.loads(s)\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "        # sanitize numeric fields if strings\n",
    "        out = []\n",
    "        for r in data:\n",
    "            if not isinstance(r, dict):\n",
    "                continue\n",
    "            rec = {\n",
    "                'criteria': r.get('criteria', ''),\n",
    "                'expected_page_size': int(str(r.get('expected_page_size', 0)).strip().split()[0] or 0),\n",
    "                'expected_page_variance': int(str(r.get('expected_page_variance', 0)).strip().split()[0] or 0),\n",
    "                'estimated_pages': int(str(r.get('estimated_pages', 0)).strip().split()[0] or 0),\n",
    "                'pages': r.get('pages', []) or []\n",
    "            }\n",
    "            out.append(rec)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f'Failed to parse LLM JSON with {txt}:', e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86c405a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Naive recommendation ---\n",
    "def naive_recommendation(meta: dict, csv_path: str):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        n = df.shape[0]\n",
    "        return {\n",
    "            'criteria': '',\n",
    "            'estimated_pages': 1,\n",
    "            'pages': ['ALL'],\n",
    "            'expected_page_size': n,\n",
    "            'expected_page_variance': 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print('Naive recommendation error:', e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5ecd81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM querying logic ---\n",
    "import requests\n",
    "\n",
    "def query_llm_for_criteria(meta: dict, df: pd.DataFrame, model: str):\n",
    "    # api_key = os.environ.get(OPENROUTER_API_KEY_ENV, '')\n",
    "    if not api_key:\n",
    "        print('No OpenRouter API key found in environment.')\n",
    "        return None, None, None\n",
    "    prompt = build_llm_prompt(meta, df)\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'max_tokens': 1024,\n",
    "        'temperature': 0\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f'{OPENROUTER_BASE_URL}/chat/completions', headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        content = response.json()['choices'][0]['message']['content']\n",
    "        return prompt, content, parse_llm_recommendations(content)\n",
    "    except Exception as e:\n",
    "        print(f'LLM query failed for table {meta.get(\"name\", \"?\")}, model {model}, query: {prompt}', e)\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed65948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistical cardinality recommendation ---\n",
    "def get_llm_page_keys(meta, col, df, model='openai/gpt-4o-mini'):\n",
    "    head = df.head(20).to_string(index=False)\n",
    "    query = meta.get('query_without_cutoff') or meta.get('table_title') or meta.get('name')\n",
    "    prompt = f'''\n",
    "Given the following table and the column(s) [{col}],\n",
    "enumerate all possible values (or value ranges) for these columns that would be useful for pagination.\n",
    "Return a JSON list of values or ranges, suitable for requesting pages downstream.\n",
    "If the column is numeric, suggest reasonable ranges or bins.\n",
    "If categorical, list all unique values. If the list is too long, summarize or group as needed.\n",
    "\n",
    "Table head:\n",
    "{head}\n",
    "\n",
    "Column(s): {col}\n",
    "Query: {query}\n",
    "Return raw JSON only: [\"value1\", \"value2\", ...]\n",
    "'''\n",
    "    import requests, os\n",
    "    headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': [{'role': 'user', 'content': prompt}],\n",
    "        'max_tokens': 512,\n",
    "        'temperature': 0\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        content = response.json()['choices'][0]['message']['content']\n",
    "        # Remove code fences if present\n",
    "        content = content.replace('```json', '').replace('```', '').strip()\n",
    "        page_keys = json.loads(content)\n",
    "        if isinstance(page_keys, dict):\n",
    "            page_keys = list(page_keys.values())\n",
    "        return page_keys\n",
    "    except Exception as e:\n",
    "        print(f'LLM page key query failed for column {col}:', e)\n",
    "        return []\n",
    "\n",
    "def statistical_cardinality_recommendation(meta: dict, csv_path: str, sample_size: int = 100):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        n = df.shape[0]\n",
    "        if n == 0:\n",
    "            return {}\n",
    "        sample_n = min(sample_size, n)\n",
    "        df_sample = df.sample(n=sample_n, random_state=42) if n > sample_n else df\n",
    "        variances = df_sample.var(numeric_only=True)\n",
    "        if not variances.empty:\n",
    "            col = variances.idxmin()\n",
    "            # Query LLM for page keys if none found\n",
    "            page_keys = get_llm_page_keys(meta, col, df)\n",
    "            return {\n",
    "                'criteria': col,\n",
    "                'estimated_pages': min(max(int(n / PAGE_SIZE_TARGET), MIN_PAGES), MAX_PAGES),\n",
    "                'pages': page_keys,\n",
    "                'expected_page_size': int(n / max(1, min(max(int(n / PAGE_SIZE_TARGET), MIN_PAGES), MAX_PAGES))),\n",
    "                'expected_page_variance': int(variances.min())\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print('Statistical cardinality error:', e)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30a8c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: english_latin_rivalry_1887_2012, CSV: processing/0_data/20250903_223317/25_english_latin_rivalry_1887_2012.csv\n",
      "Processing table: english_latin_rivalry_1887_2012, CSV: processing/0_data/20250903_223317/25_english_latin_rivalry_1887_2012.csv\n",
      "Processing table: australia_demographics_1900_2010, CSV: processing/0_data/20250903_223317/3_australia_demographics_1900_2010.csv\n",
      "Processing table: australia_demographics_1900_2010, CSV: processing/0_data/20250903_223317/3_australia_demographics_1900_2010.csv\n",
      "Processing table: elements, CSV: processing/0_data/20250903_223317/15_elements.csv\n",
      "Processing table: elements, CSV: processing/0_data/20250903_223317/15_elements.csv\n",
      "Processing table: rock_band_downloadable_2011, CSV: processing/0_data/20250903_223317/12_rock_band_downloadable_2011.csv\n",
      "Processing table: rock_band_downloadable_2011, CSV: processing/0_data/20250903_223317/12_rock_band_downloadable_2011.csv\n",
      "Processing table: living_proof_the_farewell_tour, CSV: processing/0_data/20250903_223317/19_living_proof_the_farewell_tour.csv\n",
      "Processing table: living_proof_the_farewell_tour, CSV: processing/0_data/20250903_223317/19_living_proof_the_farewell_tour.csv\n",
      "Processing table: portuguese_grape_varieties, CSV: processing/0_data/20250903_223317/33_portuguese_grape_varieties.csv\n",
      "Processing table: portuguese_grape_varieties, CSV: processing/0_data/20250903_223317/33_portuguese_grape_varieties.csv\n",
      "Processing table: republican_straw_polls_2012, CSV: processing/0_data/20250903_223317/0_republican_straw_polls_2012.csv\n",
      "Processing table: republican_straw_polls_2012, CSV: processing/0_data/20250903_223317/0_republican_straw_polls_2012.csv\n",
      "Processing table: miss_new_york_usa_delegates_2012, CSV: processing/0_data/20250903_223317/59_miss_new_york_usa_delegates_2012.csv\n",
      "Processing table: miss_new_york_usa_delegates_2012, CSV: processing/0_data/20250903_223317/59_miss_new_york_usa_delegates_2012.csv\n",
      "Processing table: cross_country_junior_women_1996, CSV: processing/0_data/20250903_223317/52_cross_country_junior_women_1996.csv\n",
      "Processing table: cross_country_junior_women_1996, CSV: processing/0_data/20250903_223317/52_cross_country_junior_women_1996.csv\n",
      "Processing table: tour_de_france_2009, CSV: processing/0_data/20250903_223317/9_tour_de_france_2009.csv\n",
      "Processing table: tour_de_france_2009, CSV: processing/0_data/20250903_223317/9_tour_de_france_2009.csv\n",
      "Processing table: anaheim_ducks_draft_picks_1998_2013, CSV: processing/0_data/20250903_223317/7_anaheim_ducks_draft_picks_1998_2013.csv\n",
      "Processing table: anaheim_ducks_draft_picks_1998_2013, CSV: processing/0_data/20250903_223317/7_anaheim_ducks_draft_picks_1998_2013.csv\n",
      "Processing table: belgium_demographics_1900_2011, CSV: processing/0_data/20250903_223317/2_belgium_demographics_1900_2011.csv\n",
      "Processing table: belgium_demographics_1900_2011, CSV: processing/0_data/20250903_223317/2_belgium_demographics_1900_2011.csv\n",
      "Processing table: men_butterfly_100m_2009, CSV: processing/0_data/20250903_223317/10_men_butterfly_100m_2009.csv\n",
      "Processing table: men_butterfly_100m_2009, CSV: processing/0_data/20250903_223317/10_men_butterfly_100m_2009.csv\n",
      "Processing table: tennessee_vanderbilt_rivalry_1900_2012, CSV: processing/0_data/20250903_223317/29_tennessee_vanderbilt_rivalry_1900_2012.csv\n",
      "Processing table: tennessee_vanderbilt_rivalry_1900_2012, CSV: processing/0_data/20250903_223317/29_tennessee_vanderbilt_rivalry_1900_2012.csv\n",
      "Processing table: ice_hockey_2006, CSV: processing/0_data/20250903_223317/5_ice_hockey_2006.csv\n",
      "Processing table: ice_hockey_2006, CSV: processing/0_data/20250903_223317/5_ice_hockey_2006.csv\n",
      "Processing table: scottish_football_transfers_summer_2011, CSV: processing/0_data/20250903_223317/17_scottish_football_transfers_summer_2011.csv\n",
      "Processing table: scottish_football_transfers_summer_2011, CSV: processing/0_data/20250903_223317/17_scottish_football_transfers_summer_2011.csv\n",
      "Processing table: udaykumar_films, CSV: processing/0_data/20250903_223317/31_udaykumar_films.csv\n",
      "Processing table: udaykumar_films, CSV: processing/0_data/20250903_223317/31_udaykumar_films.csv\n",
      "Processing table: ramsar_convention_parties, CSV: processing/0_data/20250903_223317/34_ramsar_convention_parties.csv\n",
      "Processing table: ramsar_convention_parties, CSV: processing/0_data/20250903_223317/34_ramsar_convention_parties.csv\n",
      "Processing table: figure_skating_ladies_2009_2010, CSV: processing/0_data/20250903_223317/13_figure_skating_ladies_2009_2010.csv\n",
      "Processing table: figure_skating_ladies_2009_2010, CSV: processing/0_data/20250903_223317/13_figure_skating_ladies_2009_2010.csv\n",
      "Processing table: liechtenstein_demographics_1901_2011, CSV: processing/0_data/20250903_223317/21_liechtenstein_demographics_1901_2011.csv\n",
      "Processing table: liechtenstein_demographics_1901_2011, CSV: processing/0_data/20250903_223317/21_liechtenstein_demographics_1901_2011.csv\n",
      "Processing table: new_brunswick_parishes_2006_2011, CSV: processing/0_data/20250903_223317/4_new_brunswick_parishes_2006_2011.csv\n",
      "Processing table: new_brunswick_parishes_2006_2011, CSV: processing/0_data/20250903_223317/4_new_brunswick_parishes_2006_2011.csv\n",
      "Processing table: curling_teams_women_2013_2014, CSV: processing/0_data/20250903_223317/16_curling_teams_women_2013_2014.csv\n",
      "Processing table: curling_teams_women_2013_2014, CSV: processing/0_data/20250903_223317/16_curling_teams_women_2013_2014.csv\n",
      "Processing table: south_african_class_15f_4_8_2, CSV: processing/0_data/20250903_223317/8_south_african_class_15f_4_8_2.csv\n",
      "Processing table: south_african_class_15f_4_8_2, CSV: processing/0_data/20250903_223317/8_south_african_class_15f_4_8_2.csv\n",
      "Processing table: playstation_3_cooperative_games, CSV: processing/0_data/20250903_223317/11_playstation_3_cooperative_games.csv\n",
      "Processing table: playstation_3_cooperative_games, CSV: processing/0_data/20250903_223317/11_playstation_3_cooperative_games.csv\n",
      "Processing table: fa_cup_qualifying_rounds_1999_2000, CSV: processing/0_data/20250903_223317/32_fa_cup_qualifying_rounds_1999_2000.csv\n",
      "Processing table: fa_cup_qualifying_rounds_1999_2000, CSV: processing/0_data/20250903_223317/32_fa_cup_qualifying_rounds_1999_2000.csv\n",
      "Processing table: minor_planets_discovered_by_nikolai_chernykh, CSV: processing/0_data/20250903_223317/14_minor_planets_discovered_by_nikolai_chernykh.csv\n",
      "Processing table: minor_planets_discovered_by_nikolai_chernykh, CSV: processing/0_data/20250903_223317/14_minor_planets_discovered_by_nikolai_chernykh.csv\n",
      "Done. Results saved to processing/1_pagination/20250903_223321\n"
     ]
    }
   ],
   "source": [
    "output_root = 'processing/1_pagination/'\n",
    "timestamp = RUN_TS\n",
    "output_folder = os.path.join(output_root, timestamp)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "table_iter = tables[:TABLE_LIMIT] if TABLE_LIMIT else tables\n",
    "for table in table_iter:\n",
    "    meta = table['meta']\n",
    "    csv_path = table['csv']\n",
    "    print(f'Processing table: {meta.get(\"name\", \"?\")}, CSV: {csv_path}')\n",
    "    if TABLE_WHITELIST and meta.get('name') not in TABLE_WHITELIST:\n",
    "        continue\n",
    "    if TABLE_BLACKLIST and meta.get('name') in TABLE_BLACKLIST:\n",
    "        continue\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f'Error loading CSV {csv_path}:', e)\n",
    "        continue\n",
    "    if df.shape[0] < MIN_ROWS_THRESHOLD:\n",
    "        continue\n",
    "    result = {}\n",
    "    print(f'Processing table: {meta.get(\"name\", \"?\")}, CSV: {csv_path}')\n",
    "    if RUN_CRITERIA in {'naive', 'all'}:\n",
    "        result['naive'] = naive_recommendation(meta, csv_path)\n",
    "    if RUN_CRITERIA in {'statistical', 'all'}:\n",
    "        result['statistical'] = statistical_cardinality_recommendation(meta, csv_path)\n",
    "    if RUN_CRITERIA in {'llm', 'all'}:\n",
    "        result['llm'] = {}\n",
    "        for model in RUN_MODELS:\n",
    "            llm_result = None\n",
    "            for attempt in range(3):\n",
    "                query, response, llm_result = query_llm_for_criteria(meta, csv_path, model)\n",
    "                # Accept result if not None and not empty list\n",
    "                if llm_result is not None and (not isinstance(llm_result, list) or len(llm_result) > 0):\n",
    "                    break\n",
    "                print(f'LLM retry {attempt+1} for model {model} on table {meta.get(\"name\", \"?\")} with query {query} and result {llm_result}')\n",
    "            if llm_result is not None and (not isinstance(llm_result, list) or len(llm_result) > 0):\n",
    "                result['llm'][model] = llm_result\n",
    "            else:\n",
    "                print(f'Skipping table {meta.get(\"name\", \"?\")} for model {model} due to repeated LLM failure.')\n",
    "    out_payload = {'meta': meta, 'pagination_criteria': result}\n",
    "    out_path = os.path.join(output_folder, os.path.basename(table['path']))\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(out_payload, f, indent=2)\n",
    "    # Copy CSV file to output folder\n",
    "    out_csv_path = os.path.join(output_folder, os.path.basename(csv_path))\n",
    "    df.to_csv(out_csv_path, index=False)\n",
    "print('Done. Results saved to', output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
