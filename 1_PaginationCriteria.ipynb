{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a6b9e4",
   "metadata": {},
   "source": [
    "# Stage 1: Pagination Criteria Notebook\n",
    "\n",
    "This notebook generates pagination criteria for each table using three methods: naive (full-table fetch), LLM-based (OpenRouter), and statistical cardinality estimation. Results are saved as JSON files in a timestamped folder under `processing/1_pagination/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d9c13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d12df",
   "metadata": {},
   "source": [
    "## Locate Latest Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a00a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'processing/0_data/'\n",
    "folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]\n",
    "latest_folder = sorted(folders)[-1] if folders else None\n",
    "data_path = os.path.join(data_root, latest_folder) if latest_folder else None\n",
    "assert data_path and os.path.exists(data_path), 'No data folder found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577d6ed",
   "metadata": {},
   "source": [
    "## Load Table JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf54264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob(os.path.join(data_path, '*.json'))\n",
    "tables = []\n",
    "for jf in json_files:\n",
    "    with open(jf, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    # Find corresponding CSV\n",
    "    csv_path = jf.replace('.json', '.csv')\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f'Missing CSV for {jf}')\n",
    "        continue\n",
    "    tables.append({'path': jf, 'meta': obj['meta'], 'csv': csv_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4602d60",
   "metadata": {},
   "source": [
    "## Define Pagination Criteria Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ed75593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_criteria(table):\n",
    "    return [{\n",
    "        'criteria': 'none',\n",
    "        'expected_page_size': len(table),\n",
    "        'estimated_pages': 1,\n",
    "        'expected_page_variance': 0,\n",
    "        'pages': ['all']\n",
    "    }]\n",
    "\n",
    "def statistical_cardinality_criteria(df):\n",
    "    # Placeholder: select columns with lowest variance\n",
    "    variances = df.var(numeric_only=True)\n",
    "    if not variances.empty:\n",
    "        col = variances.idxmin()\n",
    "        return [{\n",
    "            'criteria': col,\n",
    "            'expected_page_size': int(df.shape[0] / 5),\n",
    "            'estimated_pages': 5,\n",
    "            'expected_page_variance': int(variances.min()),\n",
    "            'pages': []  # Could be filled with unique values or ranges\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a415718",
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run timestamp: 20250917_121546\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "RUN_CRITERIA = 'all'  # one of: 'naive', 'statistical', 'llm', 'all'\n",
    "RUN_MODELS = [\n",
    "    'google/gemini-2.5-flash-lite',\n",
    "    'deepseek/deepseek-chat-v3.1',\n",
    "    'openai/gpt-4o-mini',\n",
    " ]  # used only when RUN_CRITERIA in {'llm','all'}\n",
    "\n",
    "# Restrict which tables to run (None = all). Values are table 'name' from meta.\n",
    "TABLE_WHITELIST = None  # e.g. ['south_african_class_15f_4_8_2']\n",
    "TABLE_BLACKLIST = []\n",
    "TABLE_LIMIT = None  # e.g. 5\n",
    "\n",
    "# Heuristic targets\n",
    "PAGE_SIZE_TARGET = 50\n",
    "MIN_PAGES = 3\n",
    "MAX_PAGES = 10\n",
    "MIN_ROWS_THRESHOLD = 100  # only consider tables with >100 rows\n",
    "\n",
    "# Re-run behavior\n",
    "SKIP_IF_EXISTS = False  # skip writing if output exists for given criteria/model/table\n",
    "\n",
    "# OpenRouter config (read API key from env var OPENROUTER_API_KEY)\n",
    "OPENROUTER_BASE_URL = 'https://openrouter.ai/api/v1'\n",
    "# OPENROUTER_API_KEY_ENV = 'OPENROUTER_API_KEY'\n",
    "OPENROUTER_ORIGIN = 'openrouter'  # label only\n",
    "\n",
    "import os, json, time, re, math, random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_TS = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print('Run timestamp:', RUN_TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6acda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf15fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM prompt for pagination criteria ---\n",
    "def build_llm_prompt(meta: dict, csv_path: str) -> str:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols = list(df.columns)\n",
    "    # Use 20 random rows instead of head\n",
    "    if df.shape[0] > 20:\n",
    "        sample_df = df.sample(n=20, random_state=42)\n",
    "    else:\n",
    "        sample_df = df\n",
    "    head = sample_df.to_string(index=False)\n",
    "    query = meta.get('query_without_cutoff') or meta.get('table_title') or meta.get('name')\n",
    "    tmpl = '''\n",
    "### **Task Explanation**\n",
    "\n",
    "You are a data scientist specializing in tabular data retrieval and result organization.\n",
    "Your task is to recommend efficient **pagination strategies** for large tables.\n",
    "\n",
    "Pagination should:\n",
    "\n",
    "* Allow balanced, predictable page sizes.\n",
    "* Be based on available columns whenever possible.\n",
    "* Include a **complete list of page keys to request**.\n",
    "* **If the full list of pages cannot be determined, omit the criteria entirely.**\n",
    "* Keep in mind that the table head is very partial, and the full table may have many more rows and a wider range of values.\n",
    "* Aim for an **average** page size of around %d rows** and no more than 5 pages. Ranges are acceptable e.g \"1960-1965\".\n",
    "\n",
    "---\n",
    "\n",
    "### **Output Format**\n",
    "\n",
    "Return up to **3 recommended pagination criteria**, ordered from best to worst.\n",
    "Output must be **raw JSON** in the following format:\n",
    "\n",
    "```json\n",
    "[\n",
    "{\n",
    "    \"criteria\": \"comma-separated column names used for pagination\",\n",
    "    \"expected_page_size\": \"approximate number of rows per page and variance, integer ONLY\",\n",
    "    \"estimated_pages\": \"approximate number of total pages, integer ONLY\",\n",
    "    \"expected_page_variance\": \"approximate variance in page size, integer ONLY\",\n",
    "    \"pages\": [\"full list of page keys/ranges to request downstream\"]\n",
    "}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Input**\n",
    "\n",
    "```\n",
    "Query: Retrieve all Nobel Prize winners in Physics  \n",
    "Columns: year, laureate_name, country  \n",
    "Table head:  \n",
    "year | laureate_name   | country  \n",
    "1901 | Wilhelm RÃ¶ntgen | Germany  \n",
    "1902 | Hendrik Lorentz | Netherlands  \n",
    "1902 | Pieter Zeeman   | Netherlands  \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Output**\n",
    "\n",
    "```json\n",
    "[\n",
    "{\n",
    "    \"criteria\": \"year\",\n",
    "    \"expected_page_size\": \"50\",\n",
    "    \"expected_page_variance\": \"10\",\n",
    "    \"estimated_pages\": 3,\n",
    "    \"pages\": [\"1901-1920\", \"1921-1940\", \"1941-1960\"]\n",
    "},\n",
    "{\n",
    "    \"criteria\": \"first_letter_of_laureate_name\",\n",
    "    \"expected_page_size\": \"30\",\n",
    "    \"expected_page_variance\": \"15\",\n",
    "    \"estimated_pages\": 9,\n",
    "    \"pages\": [\"A-C\", \"D-F\", \"G-I\", \"J-L\", \"M-O\", \"P-R\", \"S-U\", \"V-X\", \"Y-Z\"]\n",
    "}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Actual Input**\n",
    "'''\n",
    "    query = f'''\n",
    "### Input\n",
    "Query: {query}\n",
    "Columns: {', '.join(cols)}\n",
    "Table head:\n",
    "{head}\n",
    "'''\n",
    "    return tmpl + query\n",
    "\n",
    "def strip_code_fences(txt):\n",
    "    return txt.replace('```json', '').replace('```', '').strip()\n",
    "\n",
    "def parse_llm_recommendations(txt: str):\n",
    "    if not txt:\n",
    "        return []\n",
    "    s = strip_code_fences(txt)\n",
    "    try:\n",
    "        data = json.loads(s)\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "        # sanitize numeric fields if strings\n",
    "        out = []\n",
    "        for r in data:\n",
    "            if not isinstance(r, dict):\n",
    "                continue\n",
    "            rec = {\n",
    "                'criteria': r.get('criteria', ''),\n",
    "                'expected_page_size': int(str(r.get('expected_page_size', 0)).strip().split()[0] or 0),\n",
    "                'expected_page_variance': int(str(r.get('expected_page_variance', 0)).strip().split()[0] or 0),\n",
    "                'estimated_pages': int(str(r.get('estimated_pages', 0)).strip().split()[0] or 0),\n",
    "                'pages': r.get('pages', []) or []\n",
    "            }\n",
    "            out.append(rec)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f'Failed to parse LLM JSON with {txt}:', e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86c405a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Naive recommendation ---\n",
    "def naive_recommendation(meta: dict, csv_path: str):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        n = df.shape[0]\n",
    "        return {\n",
    "            'criteria': '',\n",
    "            'estimated_pages': 1,\n",
    "            'pages': ['ALL'],\n",
    "            'expected_page_size': n,\n",
    "            'expected_page_variance': 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print('Naive recommendation error:', e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5ecd81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM querying logic ---\n",
    "import requests\n",
    "\n",
    "def query_llm_for_criteria(meta: dict, df: pd.DataFrame, model: str):\n",
    "    # api_key = os.environ.get(OPENROUTER_API_KEY_ENV, '')\n",
    "    if not api_key:\n",
    "        print('No OpenRouter API key found in environment.')\n",
    "        return None, None, None\n",
    "    prompt = build_llm_prompt(meta, df)\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'max_tokens': 1024,\n",
    "        'temperature': 0\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(f'{OPENROUTER_BASE_URL}/chat/completions', headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        content = response.json()['choices'][0]['message']['content']\n",
    "        return prompt, content, parse_llm_recommendations(content)\n",
    "    except Exception as e:\n",
    "        print(f'LLM query failed for table {meta.get(\"name\", \"?\")}, model {model}, query: {prompt}', e)\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed65948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistical cardinality recommendation ---\n",
    "def get_llm_page_keys(meta, col, df, model='openai/gpt-4o-mini'):\n",
    "    head = df.head(20).to_string(index=False)\n",
    "    query = meta.get('query_without_cutoff') or meta.get('table_title') or meta.get('name')\n",
    "    prompt = f'''\n",
    "Given the following table and the column(s) [{col}],\n",
    "enumerate all possible values (or value ranges) for these columns that would be useful for pagination.\n",
    "Return a JSON list of values or ranges, suitable for requesting pages downstream.\n",
    "If the column is numeric, suggest reasonable ranges or bins.\n",
    "If categorical, list all unique values. If the list is too long, summarize or group as needed.\n",
    "\n",
    "Table head:\n",
    "{head}\n",
    "\n",
    "Column(s): {col}\n",
    "Query: {query}\n",
    "Return raw JSON only: [\"value1\", \"value2\", ...]\n",
    "'''\n",
    "    import requests, os\n",
    "    headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': [{'role': 'user', 'content': prompt}],\n",
    "        'max_tokens': 512,\n",
    "        'temperature': 0\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        content = response.json()['choices'][0]['message']['content']\n",
    "        # Remove code fences if present\n",
    "        content = content.replace('```json', '').replace('```', '').strip()\n",
    "        page_keys = json.loads(content)\n",
    "        if isinstance(page_keys, dict):\n",
    "            page_keys = list(page_keys.values())\n",
    "        return page_keys\n",
    "    except Exception as e:\n",
    "        print(f'LLM page key query failed for column {col}:', e)\n",
    "        return []\n",
    "\n",
    "def statistical_cardinality_recommendation(meta: dict, csv_path: str, sample_size: int = 100):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        n = df.shape[0]\n",
    "        if n == 0:\n",
    "            return {}\n",
    "        sample_n = min(sample_size, n)\n",
    "        df_sample = df.sample(n=sample_n, random_state=42) if n > sample_n else df\n",
    "        variances = df_sample.var(numeric_only=True)\n",
    "        if not variances.empty:\n",
    "            col = variances.idxmin()\n",
    "            # Query LLM for page keys if none found\n",
    "            page_keys = get_llm_page_keys(meta, col, df)\n",
    "            return {\n",
    "                'criteria': col,\n",
    "                'estimated_pages': min(max(int(n / PAGE_SIZE_TARGET), MIN_PAGES), MAX_PAGES),\n",
    "                'pages': page_keys,\n",
    "                'expected_page_size': int(n / max(1, min(max(int(n / PAGE_SIZE_TARGET), MIN_PAGES), MAX_PAGES))),\n",
    "                'expected_page_variance': int(variances.min())\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print('Statistical cardinality error:', e)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12d52b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Oracle single row recommendation ---\n",
    "def oracle_single_row_recommendation(meta: dict, csv_path: str):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        keys = meta.get('keys', [])\n",
    "        if not keys:\n",
    "            print(f'No keys found in meta for table {meta.get(\"name\", \"?\")}')\n",
    "            return {}\n",
    "        # Drop rows with missing key values\n",
    "        df_keys = df[keys].dropna()\n",
    "        # Get all unique key combinations as dicts\n",
    "        unique_rows = df_keys.drop_duplicates().to_dict(orient='records')\n",
    "        pages = unique_rows\n",
    "        return {\n",
    "            'criteria': ','.join(keys),\n",
    "            'estimated_pages': len(pages),\n",
    "            'pages': pages,\n",
    "            'expected_page_size': 1,\n",
    "            'expected_page_variance': 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print('Oracle single row recommendation error:', e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30a8c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: english_latin_rivalry_1887_2012, CSV: processing/0_data/20250917_120040/25_english_latin_rivalry_1887_2012.csv\n",
      "Processing table: english_latin_rivalry_1887_2012, CSV: processing/0_data/20250917_120040/25_english_latin_rivalry_1887_2012.csv\n",
      "Processing table: australia_demographics_1900_2010, CSV: processing/0_data/20250917_120040/3_australia_demographics_1900_2010.csv\n",
      "Processing table: australia_demographics_1900_2010, CSV: processing/0_data/20250917_120040/3_australia_demographics_1900_2010.csv\n",
      "Processing table: australia_demographics_1900_2010, CSV: processing/0_data/20250917_120040/3_australia_demographics_1900_2010.csv\n",
      "Processing table: australia_demographics_1900_2010, CSV: processing/0_data/20250917_120040/3_australia_demographics_1900_2010.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m llm_result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     query, response, llm_result = \u001b[43mquery_llm_for_criteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Accept result if not None and not empty list\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m llm_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm_result, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(llm_result) > \u001b[32m0\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mquery_llm_for_criteria\u001b[39m\u001b[34m(meta, df, model)\u001b[39m\n\u001b[32m     14\u001b[39m data = {\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: model,\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m\n\u001b[32m     21\u001b[39m }\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOPENROUTER_BASE_URL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     response.raise_for_status()\n\u001b[32m     25\u001b[39m     content = response.json()[\u001b[33m'\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/sessions.py:747\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    744\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:899\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    897\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    898\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.iter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    903\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:816\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    817\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:931\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m    915\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    916\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m    917\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    928\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:1071\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1073\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:999\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    998\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m line = \u001b[38;5;28mself\u001b[39m._fp.fp.readline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1000\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "output_root = 'processing/1_pagination/'\n",
    "timestamp = RUN_TS\n",
    "output_folder = os.path.join(output_root, timestamp)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "table_iter = tables[:TABLE_LIMIT] if TABLE_LIMIT else tables\n",
    "for table in table_iter:\n",
    "    meta = table['meta']\n",
    "    csv_path = table['csv']\n",
    "    print(f'Processing table: {meta.get(\"name\", \"?\")}, CSV: {csv_path}')\n",
    "    if TABLE_WHITELIST and meta.get('name') not in TABLE_WHITELIST:\n",
    "        continue\n",
    "    if TABLE_BLACKLIST and meta.get('name') in TABLE_BLACKLIST:\n",
    "        continue\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f'Error loading CSV {csv_path}:', e)\n",
    "        continue\n",
    "    if df.shape[0] < MIN_ROWS_THRESHOLD:\n",
    "        continue\n",
    "    result = {}\n",
    "    print(f'Processing table: {meta.get(\"name\", \"?\")}, CSV: {csv_path}')\n",
    "    if RUN_CRITERIA in {'naive', 'all'}:\n",
    "        result['naive'] = naive_recommendation(meta, csv_path)\n",
    "    if RUN_CRITERIA in {'statistical', 'all'}:\n",
    "        result['statistical'] = statistical_cardinality_recommendation(meta, csv_path)\n",
    "    if RUN_CRITERIA in {'oracle_single_row', 'all'}:\n",
    "        result['oracle_single_row'] = oracle_single_row_recommendation(meta, csv_path)\n",
    "    if RUN_CRITERIA in {'llm', 'all'}:\n",
    "        result['llm'] = {}\n",
    "        for model in RUN_MODELS:\n",
    "            llm_result = None\n",
    "            for attempt in range(3):\n",
    "                query, response, llm_result = query_llm_for_criteria(meta, csv_path, model)\n",
    "                # Accept result if not None and not empty list\n",
    "                if llm_result is not None and (not isinstance(llm_result, list) or len(llm_result) > 0):\n",
    "                    break\n",
    "                print(f'LLM retry {attempt+1} for model {model} on table {meta.get(\"name\", \"?\")} with query {query} and result {llm_result}')\n",
    "            if llm_result is not None and (not isinstance(llm_result, list) or len(llm_result) > 0):\n",
    "                result['llm'][model] = llm_result\n",
    "            else:\n",
    "                print(f'Skipping table {meta.get(\"name\", \"?\")} for model {model} due to repeated LLM failure.')\n",
    "    out_payload = {'meta': meta, 'pagination_criteria': result}\n",
    "    out_path = os.path.join(output_folder, os.path.basename(table['path']))\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(out_payload, f, indent=2)\n",
    "    # Copy CSV file to output folder\n",
    "    out_csv_path = os.path.join(output_folder, os.path.basename(csv_path))\n",
    "    df.to_csv(out_csv_path, index=False)\n",
    "print('Done. Results saved to', output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
