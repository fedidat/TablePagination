{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e236f174",
   "metadata": {},
   "source": [
    "# Strategy Planning for Pagination\n",
    "\n",
    "This notebook determines the pagination strategy for each table and outputs a plan for the next step (fetching pages).\n",
    "\n",
    "## Strategies Implemented:\n",
    "1. **Full Table** - Fetch entire table in one query\n",
    "2. **Row by Row** - Fetch all keys, then fetch each row individually\n",
    "3. **Attribute-based** - Ask LLM which column to partition by, then fetch pages by distinct values\n",
    "4. **Classic Pagination** - Offset-based pagination with configurable page size\n",
    "5. **Range-based** - Ask LLM for column + bucketing criteria, fetch by ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165da3fd",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad6ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output base: /Users/bef/Desktop/TablePagination/processing/1_strategy\n",
      "Data directory: /Users/bef/Desktop/TablePagination/processing/0_data/20251004_213355\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openai\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# MAX_TABLES: Set to a number to process only that many tables (for testing/sampling)\n",
    "# Set to None to process all tables\n",
    "MAX_TABLES = 1  # e.g., 1 for single table test, 3 for sampling first 3 tables, None for all\n",
    "\n",
    "# PARALLEL_STRATEGIES: Set to True to run strategies in parallel (faster)\n",
    "PARALLEL_STRATEGIES = True\n",
    "MAX_WORKERS = 5  # Number of parallel strategy workers (set to number of strategies)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "PROCESSING_ROOT = ROOT / 'processing'\n",
    "DATA_DIR = PROCESSING_ROOT / '0_data'\n",
    "OUTPUT_ROOT = PROCESSING_ROOT / '1_strategy'\n",
    "\n",
    "# Find the most recent data directory\n",
    "data_subdirs = sorted([d for d in DATA_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not data_subdirs:\n",
    "    raise FileNotFoundError(f\"No data found in {DATA_DIR}\")\n",
    "\n",
    "LATEST_DATA_DIR = data_subdirs[0]\n",
    "\n",
    "print('Output base:', OUTPUT_ROOT.resolve())\n",
    "print('Data directory:', LATEST_DATA_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efd02f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenRouter\n",
    "# Make sure to set your API key\n",
    "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY', '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')\n",
    "\n",
    "# Create OpenAI client configured for OpenRouter\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"# Model to use for LLM queries\n",
    ")\n",
    "\n",
    "MODEL = 'openai/gpt-4o-mini'  # OpenRouter format: provider/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3818b2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48 tables\n",
      "Sample table ID: 10_men_butterfly_100m_2009\n"
     ]
    }
   ],
   "source": [
    "# Load all table data from step 0\n",
    "def load_all_tables(data_dir: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load all JSON files from the data directory.\"\"\"\n",
    "    tables = []\n",
    "    for json_file in sorted(data_dir.glob('*.json')):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            # Add file info\n",
    "            data['file_path'] = str(json_file)\n",
    "            data['table_id'] = json_file.stem  # e.g., \"0_republican_straw_polls_2012\"\n",
    "            tables.append(data)\n",
    "    return tables\n",
    "\n",
    "tables = load_all_tables(LATEST_DATA_DIR)\n",
    "print(f'Loaded {len(tables)} tables')\n",
    "print(f'Sample table ID: {tables[0][\"table_id\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753aa21",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "654fb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, response_format: str = \"text\") -> str:\n",
    "    \"\"\"Make a simple LLM call and return the response.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_field(s: str) -> str:\n",
    "    \"\"\"Normalize field names (from ChatGPT35_RowByRow_FirstExample).\"\"\"\n",
    "    import re\n",
    "    s = s.lower().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"\").replace(\",\", \"_\")\\\n",
    "            .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"', '').replace(\"'\", \"\")\\\n",
    "            .replace(\"/\", \"\")\n",
    "    return re.sub('_+', '_', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b154f",
   "metadata": {},
   "source": [
    "## Strategy 1: Full Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d7b515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_full_table(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Fetch the entire table in one query.\n",
    "    No LLM calls needed.\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"full_table\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": meta.get('table_title', ''),\n",
    "            \"columns\": meta.get('columns', []),\n",
    "            \"key_columns\": meta.get('keys', [])\n",
    "        },\n",
    "        \"pagination_config\": {}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce03667",
   "metadata": {},
   "source": [
    "## Strategy 2: Row by Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebab05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_row_by_row(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Fetch all key values first, then fetch each row individually.\n",
    "    Makes 1 LLM call to get all key combinations.\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    table_title = meta.get('table_title', '')\n",
    "    keys = meta.get('keys', [])\n",
    "    \n",
    "    if not keys:\n",
    "        print(f\"Warning: No keys defined for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Normalize key names\n",
    "    norm_keys = [normalize_field(k) for k in keys]\n",
    "    \n",
    "    # Build prompt to fetch all keys (inspired by ChatGPT35_RowByRow_FirstExample)\n",
    "    key_columns_desc = f\"The key column{'s' if len(keys) > 1 else ''} in the table {'are' if len(keys) > 1 else 'is'} {', '.join(keys)}\"\n",
    "    \n",
    "    keys_json_format = ', '.join([f'\"{nk}\": \"{nk}\"' for nk in norm_keys])\n",
    "    \n",
    "    keys_prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to create a table with the detailed information about {table_title}.\n",
    "{key_columns_desc}.\n",
    "List all {', '.join(keys)} entities for the table.\n",
    "The response will be formatted as JSON list shown below.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {keys_json_format}\n",
    "}}]\"\"\"\n",
    "    \n",
    "    print(f\"Fetching keys for {table_data['table_id']}...\")\n",
    "    response = call_llm(keys_prompt)\n",
    "    \n",
    "    if not response:\n",
    "        print(f\"Failed to fetch keys for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the response to extract key values\n",
    "    try:\n",
    "        # Clean up response to extract JSON\n",
    "        if not response.startswith(\"[\") and \"[\" in response:\n",
    "            response = response[response.find(\"[\"):]\n",
    "        if not response.endswith(\"]\") and \"]\" in response:\n",
    "            response = response[:response.rfind(\"]\") + 1]\n",
    "        \n",
    "        key_values = json.loads(response)\n",
    "        \n",
    "        if not isinstance(key_values, list):\n",
    "            print(f\"Invalid response format for {table_data['table_id']}\")\n",
    "            return None\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse keys response for {table_data['table_id']}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"row_by_row\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": table_title,\n",
    "            \"columns\": meta.get('columns', []),\n",
    "            \"key_columns\": keys\n",
    "        },\n",
    "        \"pagination_config\": {\n",
    "            \"key_columns\": keys,\n",
    "            \"key_values\": key_values,\n",
    "            \"total_rows\": len(key_values)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c51e8",
   "metadata": {},
   "source": [
    "## Strategy 3: Attribute-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58a7b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_attribute_based(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Ask LLM which column to partition by, then get distinct values.\n",
    "    Makes 2 LLM calls:\n",
    "    1. Ask which column to use for partitioning\n",
    "    2. Get all distinct values for that column\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    table_title = meta.get('table_title', '')\n",
    "    columns = meta.get('columns', [])\n",
    "    \n",
    "    if not columns:\n",
    "        print(f\"Warning: No columns defined for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Call 1: Ask which column to partition by\n",
    "    partition_prompt = f\"\"\"You are helping to paginate a table about {table_title}.\n",
    "The table has the following columns: {', '.join(columns)}.\n",
    "\n",
    "Which single column would be best to use for partitioning/grouping this table's data?\n",
    "Choose a column that would create meaningful, balanced groups.\n",
    "\n",
    "Respond with ONLY the column name, nothing else.\"\"\"\n",
    "    \n",
    "    print(f\"Asking LLM for partition column for {table_data['table_id']}...\")\n",
    "    partition_column = call_llm(partition_prompt)\n",
    "    \n",
    "    if not partition_column or partition_column not in columns:\n",
    "        print(f\"Invalid partition column '{partition_column}' for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Call 2: Get all distinct values for that column\n",
    "    values_prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to paginate a table about {table_title}.\n",
    "List all distinct values of the column \"{partition_column}\" in this table.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[\"{partition_column}_value1\", \"{partition_column}_value2\", ...]\"\"\"\n",
    "    \n",
    "    print(f\"Fetching distinct values for column '{partition_column}'...\")\n",
    "    response = call_llm(values_prompt)\n",
    "    \n",
    "    if not response:\n",
    "        print(f\"Failed to fetch values for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the response\n",
    "    try:\n",
    "        if not response.startswith(\"[\") and \"[\" in response:\n",
    "            response = response[response.find(\"[\"):]\n",
    "        if not response.endswith(\"]\") and \"]\" in response:\n",
    "            response = response[:response.rfind(\"]\") + 1]\n",
    "        \n",
    "        partition_values = json.loads(response)\n",
    "        \n",
    "        if not isinstance(partition_values, list):\n",
    "            print(f\"Invalid response format for {table_data['table_id']}\")\n",
    "            return None\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse values response for {table_data['table_id']}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"attribute_based\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": table_title,\n",
    "            \"columns\": columns,\n",
    "            \"key_columns\": meta.get('keys', [])\n",
    "        },\n",
    "        \"pagination_config\": {\n",
    "            \"partition_column\": partition_column,\n",
    "            \"partition_values\": partition_values,\n",
    "            \"total_partitions\": len(partition_values)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53949aef",
   "metadata": {},
   "source": [
    "## Strategy 4: Classic Pagination (Offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f3b8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_classic_pagination(table_data: Dict[str, Any], page_size: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Classic offset-based pagination.\n",
    "    No LLM calls - just configuration.\n",
    "    The fetch notebook will iteratively fetch pages.\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    keys = meta.get('keys', [])\n",
    "    \n",
    "    if not keys:\n",
    "        print(f\"Warning: No keys defined for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Default sort order: ascending by key columns\n",
    "    sort_order = [f\"{key} ASC\" for key in keys]\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"classic_pagination\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": meta.get('table_title', ''),\n",
    "            \"columns\": meta.get('columns', []),\n",
    "            \"key_columns\": keys\n",
    "        },\n",
    "        \"pagination_config\": {\n",
    "            \"page_size\": page_size,\n",
    "            \"primary_keys\": keys,\n",
    "            \"sort_order\": sort_order\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b58976",
   "metadata": {},
   "source": [
    "## Strategy 5: Range-based Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035156f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_range_based(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Ask LLM for column and bucketing criteria, then define ranges.\n",
    "    Makes 1 LLM call to determine column and bucketing strategy.\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    table_title = meta.get('table_title', '')\n",
    "    columns = meta.get('columns', [])\n",
    "    \n",
    "    if not columns:\n",
    "        print(f\"Warning: No columns defined for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Ask LLM for column and bucketing strategy\n",
    "    range_prompt = f\"\"\"You are helping to paginate a table about {table_title}.\n",
    "The table has the following columns: {', '.join(columns)}.\n",
    "\n",
    "Suggest the best column to use for range-based pagination and describe how to bucket the data.\n",
    "For example: \"year, by decade\" or \"price, by $100 ranges\" or \"date, by month\".\n",
    "\n",
    "Respond in the format: \"<column_name>, <bucketing_description>\"\n",
    "Example: \"year, by decade\"\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Asking LLM for range strategy for {table_data['table_id']}...\")\n",
    "    response = call_llm(range_prompt)\n",
    "    \n",
    "    if not response:\n",
    "        print(f\"Failed to get range strategy for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse response (expected format: \"column, bucketing\")\n",
    "    parts = response.split(',', 1)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"Invalid range response format for {table_data['table_id']}: {response}\")\n",
    "        return None\n",
    "    \n",
    "    partition_column = parts[0].strip().strip('\"').strip(\"'\")\n",
    "    bucketing_criteria = parts[1].strip().strip('\"').strip(\"'\")\n",
    "    \n",
    "    # Now ask for the actual ranges\n",
    "    ranges_prompt = f\"\"\"You are a retriever of facts.\n",
    "For a table about {table_title}, we want to paginate by {partition_column} using {bucketing_criteria}.\n",
    "\n",
    "List all the ranges needed. For each range, provide the lower bound (inclusive) and upper bound (exclusive).\n",
    "\n",
    "RESPONSE FORMAT (JSON array of objects):\n",
    "[\n",
    "    {{\"gte\": \"lower_value\", \"lt\": \"upper_value\"}},\n",
    "    {{\"gte\": \"lower_value\", \"lt\": \"upper_value\"}}\n",
    "]\n",
    "\n",
    "Example for \"year by decade\":\n",
    "[\n",
    "    {{\"gte\": \"1980\", \"lt\": \"1990\"}},\n",
    "    {{\"gte\": \"1990\", \"lt\": \"2000\"}}\n",
    "]\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Fetching ranges for {partition_column} {bucketing_criteria}...\")\n",
    "    ranges_response = call_llm(ranges_prompt)\n",
    "    \n",
    "    if not ranges_response:\n",
    "        print(f\"Failed to get ranges for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse ranges\n",
    "    try:\n",
    "        if not ranges_response.startswith(\"[\") and \"[\" in ranges_response:\n",
    "            ranges_response = ranges_response[ranges_response.find(\"[\"):]\n",
    "        if not ranges_response.endswith(\"]\") and \"]\" in ranges_response:\n",
    "            ranges_response = ranges_response[:ranges_response.rfind(\"]\") + 1]\n",
    "        \n",
    "        ranges = json.loads(ranges_response)\n",
    "        \n",
    "        if not isinstance(ranges, list):\n",
    "            print(f\"Invalid ranges format for {table_data['table_id']}\")\n",
    "            return None\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse ranges for {table_data['table_id']}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"range_based\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": table_title,\n",
    "            \"columns\": columns,\n",
    "            \"key_columns\": meta.get('keys', [])\n",
    "        },\n",
    "        \"pagination_config\": {\n",
    "            \"partition_column\": partition_column,\n",
    "            \"bucketing_criteria\": bucketing_criteria,\n",
    "            \"ranges\": ranges,\n",
    "            \"total_ranges\": len(ranges)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb1de0",
   "metadata": {},
   "source": [
    "## Main Execution: Generate Plans for All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ebb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TEST RUN MODE: Processing only 1 table\n",
      "Active strategies: ['full_table', 'row_by_row', 'attribute_based', 'classic_pagination', 'range_based']\n",
      "Processing 1 table(s)...\n"
     ]
    }
   ],
   "source": [
    "# Configuration: which strategies to run\n",
    "STRATEGIES_TO_RUN = {\n",
    "    'full_table': plan_full_table,\n",
    "    'row_by_row': plan_row_by_row,\n",
    "    'attribute_based': plan_attribute_based,\n",
    "    'classic_pagination': plan_classic_pagination,\n",
    "    'range_based': plan_range_based\n",
    "}\n",
    "\n",
    "# Choose which strategies to execute (comment out ones you don't want)\n",
    "ACTIVE_STRATEGIES = [\n",
    "    'full_table',\n",
    "    'row_by_row',\n",
    "    'attribute_based',\n",
    "    'classic_pagination',\n",
    "    'range_based',\n",
    "]\n",
    "\n",
    "# Apply MAX_TABLES limit if set\n",
    "if MAX_TABLES is not None:\n",
    "    print(f\"📊 LIMITED RUN: Processing first {MAX_TABLES} table(s)\")\n",
    "    tables_to_process = tables[:MAX_TABLES]\n",
    "else:\n",
    "    tables_to_process = tables\n",
    "\n",
    "print(f\"Active strategies: {ACTIVE_STRATEGIES}\")\n",
    "print(f\"Processing {len(tables_to_process)} table(s) out of {len(tables)} total...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e40a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: processing/1_strategy/20251004_213906\n",
      "\n",
      "============================================================\n",
      "Running strategy: FULL_TABLE\n",
      "============================================================\n",
      "\n",
      "[1/1] Processing 10_men_butterfly_100m_2009...\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed full_table: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "Running strategy: ROW_BY_ROW\n",
      "============================================================\n",
      "\n",
      "[1/1] Processing 10_men_butterfly_100m_2009...\n",
      "Fetching keys for 10_men_butterfly_100m_2009...\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed row_by_row: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "Running strategy: ATTRIBUTE_BASED\n",
      "============================================================\n",
      "\n",
      "[1/1] Processing 10_men_butterfly_100m_2009...\n",
      "Asking LLM for partition column for 10_men_butterfly_100m_2009...\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed row_by_row: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "Running strategy: ATTRIBUTE_BASED\n",
      "============================================================\n",
      "\n",
      "[1/1] Processing 10_men_butterfly_100m_2009...\n",
      "Asking LLM for partition column for 10_men_butterfly_100m_2009...\n",
      "Fetching distinct values for column 'Heat'...\n",
      "Fetching distinct values for column 'Heat'...\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed attribute_based: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "Running strategy: CLASSIC_PAGINATION\n",
      "============================================================\n",
      "\n",
      "[1/1] Processing 10_men_butterfly_100m_2009...\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed classic_pagination: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "Running strategy: RANGE_BASED\n",
      "============================================================\n",
      "\n",
      "[1/1] Processing 10_men_butterfly_100m_2009...\n",
      "Asking LLM for range strategy for 10_men_butterfly_100m_2009...\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed attribute_based: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "Running strategy: CLASSIC_PAGINATION\n",
      "============================================================\n",
      "\n",
      "[1/1] Processing 10_men_butterfly_100m_2009...\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed classic_pagination: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "Running strategy: RANGE_BASED\n",
      "============================================================\n",
      "\n",
      "[1/1] Processing 10_men_butterfly_100m_2009...\n",
      "Asking LLM for range strategy for 10_men_butterfly_100m_2009...\n",
      "Fetching ranges for \"Time by 1-second ranges\"...\n",
      "Fetching ranges for \"Time by 1-second ranges\"...\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed range_based: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "full_table: 1 tables processed\n",
      "row_by_row: 1 tables processed\n",
      "attribute_based: 1 tables processed\n",
      "classic_pagination: 1 tables processed\n",
      "range_based: 1 tables processed\n",
      "============================================================\n",
      "  ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "\n",
      "Completed range_based: 1 successes, 0 errors\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "full_table: 1 tables processed\n",
      "row_by_row: 1 tables processed\n",
      "attribute_based: 1 tables processed\n",
      "classic_pagination: 1 tables processed\n",
      "range_based: 1 tables processed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = OUTPUT_ROOT / timestamp\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create subdirectories for each strategy upfront\n",
    "for strategy_name in ACTIVE_STRATEGIES:\n",
    "    strategy_dir = output_dir / strategy_name\n",
    "    strategy_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Process all tables for each strategy\n",
    "results = {strategy: [] for strategy in ACTIVE_STRATEGIES}\n",
    "errors = {strategy: [] for strategy in ACTIVE_STRATEGIES}\n",
    "\n",
    "def process_strategy(strategy_name: str):\n",
    "    \"\"\"Process all tables for a single strategy.\"\"\"\n",
    "    strategy_results = []\n",
    "    strategy_errors = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running strategy: {strategy_name.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    strategy_func = STRATEGIES_TO_RUN[strategy_name]\n",
    "    strategy_dir = output_dir / strategy_name\n",
    "    \n",
    "    for i, table in enumerate(tables_to_process):\n",
    "        print(f\"[{strategy_name}] [{i+1}/{len(tables_to_process)}] Processing {table['table_id']}...\")\n",
    "        \n",
    "        try:\n",
    "            plan = strategy_func(table)\n",
    "            \n",
    "            if plan:\n",
    "                strategy_results.append(plan)\n",
    "                \n",
    "                # Save immediately after successful processing\n",
    "                table_id = plan['table_id']\n",
    "                output_file = strategy_dir / f\"{table_id}.json\"\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(plan, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                print(f\"[{strategy_name}]   ✓ Success - Saved to {output_file.name}\")\n",
    "            else:\n",
    "                strategy_errors.append({\n",
    "                    'table_id': table['table_id'],\n",
    "                    'error': 'Function returned None'\n",
    "                })\n",
    "                print(f\"[{strategy_name}]   ✗ Failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            strategy_errors.append({\n",
    "                'table_id': table['table_id'],\n",
    "                'error': str(e)\n",
    "            })\n",
    "            print(f\"[{strategy_name}]   ✗ Error: {e}\")\n",
    "        \n",
    "        # Save errors incrementally too\n",
    "        if strategy_errors:\n",
    "            errors_file = strategy_dir / '_errors.json'\n",
    "            with open(errors_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(strategy_errors, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n[{strategy_name}] Completed: {len(strategy_results)} successes, {len(strategy_errors)} errors\")\n",
    "    return strategy_name, strategy_results, strategy_errors\n",
    "\n",
    "# Run strategies in parallel or sequentially\n",
    "if PARALLEL_STRATEGIES:\n",
    "    print(f\"\\n⚡ Running {len(ACTIVE_STRATEGIES)} strategies in PARALLEL with {MAX_WORKERS} workers\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all strategy tasks\n",
    "        future_to_strategy = {\n",
    "            executor.submit(process_strategy, strategy_name): strategy_name \n",
    "            for strategy_name in ACTIVE_STRATEGIES\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_strategy):\n",
    "            strategy_name, strategy_results, strategy_errors = future.result()\n",
    "            results[strategy_name] = strategy_results\n",
    "            errors[strategy_name] = strategy_errors\n",
    "else:\n",
    "    print(f\"\\n🔄 Running {len(ACTIVE_STRATEGIES)} strategies SEQUENTIALLY\")\n",
    "    \n",
    "    for strategy_name in ACTIVE_STRATEGIES:\n",
    "        strategy_name, strategy_results, strategy_errors = process_strategy(strategy_name)\n",
    "        results[strategy_name] = strategy_results\n",
    "        errors[strategy_name] = strategy_errors\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for strategy_name in ACTIVE_STRATEGIES:\n",
    "    print(f\"{strategy_name}: {len(results[strategy_name])} tables processed\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e0841",
   "metadata": {},
   "source": [
    "## Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9110bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final summary saved to processing/1_strategy/20251004_213906/_summary.json\n",
      "\n",
      "All done! Results saved to processing/1_strategy/20251004_213906\n"
     ]
    }
   ],
   "source": [
    "# All individual files have been saved incrementally during processing\n",
    "# Now just save the final summary\n",
    "\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'max_tables_limit': MAX_TABLES,\n",
    "    'total_tables': len(tables),\n",
    "    'processed_tables': len(tables_to_process),\n",
    "    'strategies': {\n",
    "        strategy_name: {\n",
    "            'success_count': len(results[strategy_name]),\n",
    "            'error_count': len(errors[strategy_name])\n",
    "        }\n",
    "        for strategy_name in ACTIVE_STRATEGIES\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_file = output_dir / '_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nFinal summary saved to {summary_file}\")\n",
    "print(f\"\\nAll done! Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10395346",
   "metadata": {},
   "source": [
    "## Sample Output Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e133d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample output for FULL_TABLE\n",
      "============================================================\n",
      "{\n",
      "  \"table_id\": \"10_men_butterfly_100m_2009\",\n",
      "  \"table_name\": \"men_butterfly_100m_2009\",\n",
      "  \"strategy\": \"full_table\",\n",
      "  \"metadata\": {\n",
      "    \"table_title\": \"men's 100 metre butterfly results in heats at the 2009 World Aquatics Championships\",\n",
      "    \"columns\": [\n",
      "      \"Name\",\n",
      "      \"Nationality\",\n",
      "      \"Time\",\n",
      "      \"Heat\",\n",
      "      \"Lane\"\n",
      "    ],\n",
      "    \"key_columns\": [\n",
      "      \"Name\"\n",
      "    ]\n",
      "  },\n",
      "  \"pagination_config\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect a sample output from each strategy\n",
    "for strategy_name in ACTIVE_STRATEGIES:\n",
    "    if results[strategy_name]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Sample output for {strategy_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        sample = results[strategy_name][0]\n",
    "        print(json.dumps(sample, indent=2))\n",
    "        break  # Show just one example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c20d2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The pagination plans have been generated and saved to `processing/1_strategy/<timestamp>/`.\n",
    "\n",
    "Each strategy creates a subdirectory with JSON files for each table containing:\n",
    "- **table_id**: Unique identifier\n",
    "- **strategy**: The pagination approach used\n",
    "- **metadata**: Table information (title, columns, keys)\n",
    "- **pagination_config**: Strategy-specific configuration for the fetch notebook\n",
    "\n",
    "### For the next notebook (2_FetchPages.ipynb):\n",
    "1. Load these JSON files\n",
    "2. For each pagination plan, execute the appropriate fetching logic:\n",
    "   - **full_table**: One query for entire table\n",
    "   - **row_by_row**: Use key_values to fetch each row\n",
    "   - **attribute_based**: Use partition_column and partition_values to fetch filtered pages\n",
    "   - **classic_pagination**: Iteratively fetch pages using page_size and primary_keys\n",
    "   - **range_based**: Use ranges to fetch data in buckets\n",
    "\n",
    "### To enable more strategies:\n",
    "Uncomment the strategies you want in the `ACTIVE_STRATEGIES` list above and re-run the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
