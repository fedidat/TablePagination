{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e236f174",
   "metadata": {},
   "source": [
    "# Strategy Planning for Pagination\n",
    "\n",
    "This notebook determines the pagination strategy for each table and outputs a plan for the next step (fetching pages).\n",
    "\n",
    "## Strategies Implemented:\n",
    "1. **Full Table** - Fetch entire table in one query\n",
    "2. **Row by Row** - Fetch all keys, then fetch each row individually\n",
    "3. **Attribute-based** - Ask LLM which column to partition by, then fetch pages by distinct values\n",
    "4. **Classic Pagination** - Offset-based pagination with configurable page size\n",
    "5. **Range-based** - Ask LLM for column + bucketing criteria, fetch by ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165da3fd",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adad6ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output base: /Users/bef/Desktop/TablePagination/processing/1_strategy\n",
      "Data directory: /Users/bef/Desktop/TablePagination/processing/0_data/20251004_213355\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openai\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# MAX_TABLES: Set to a number to process only that many tables (for testing/sampling)\n",
    "# Set to None to process all tables\n",
    "MAX_TABLES = None  # e.g., 1 for single table test, 3 for sampling first 3 tables, None for all\n",
    "\n",
    "# PARALLEL_STRATEGIES: Set to True to run strategies in parallel (faster)\n",
    "PARALLEL_STRATEGIES = True\n",
    "MAX_WORKERS = 5  # Number of parallel strategy workers (set to number of strategies)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "PROCESSING_ROOT = ROOT / 'processing'\n",
    "DATA_DIR = PROCESSING_ROOT / '0_data'\n",
    "OUTPUT_ROOT = PROCESSING_ROOT / '1_strategy'\n",
    "\n",
    "# Find the most recent data directory\n",
    "data_subdirs = sorted([d for d in DATA_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not data_subdirs:\n",
    "    raise FileNotFoundError(f\"No data found in {DATA_DIR}\")\n",
    "\n",
    "LATEST_DATA_DIR = data_subdirs[0]\n",
    "\n",
    "print('Output base:', OUTPUT_ROOT.resolve())\n",
    "print('Data directory:', LATEST_DATA_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efd02f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API key loaded from api_key.txt\n",
      "Using model: openai/gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "# Configure OpenRouter\n",
    "# Read API key from file\n",
    "api_key_file = ROOT / 'api_key.txt'\n",
    "if not api_key_file.exists():\n",
    "    raise ValueError('No API key found. Please create api_key.txt or set OPENROUTER_API_KEY environment variable')\n",
    "with open(api_key_file, 'r') as f:\n",
    "    OPENROUTER_API_KEY = f.read().strip()\n",
    "print('✓ API key loaded from api_key.txt')\n",
    "\n",
    "# Create OpenAI client configured for OpenRouter\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Model to use for LLM queries\n",
    "MODEL = 'openai/gpt-4.1-mini'  # OpenRouter format: provider/model\n",
    "print(f'Using model: {MODEL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3818b2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 48 tables\n",
      "Sample table ID: 10_men_butterfly_100m_2009\n"
     ]
    }
   ],
   "source": [
    "# Load all table data from step 0\n",
    "def load_all_tables(data_dir: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load all JSON files from the data directory.\"\"\"\n",
    "    tables = []\n",
    "    for json_file in sorted(data_dir.glob('*.json')):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            # Add file info\n",
    "            data['file_path'] = str(json_file)\n",
    "            data['table_id'] = json_file.stem  # e.g., \"0_republican_straw_polls_2012\"\n",
    "            tables.append(data)\n",
    "    return tables\n",
    "\n",
    "tables = load_all_tables(LATEST_DATA_DIR)\n",
    "print(f'Loaded {len(tables)} tables')\n",
    "print(f'Sample table ID: {tables[0][\"table_id\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753aa21",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "654fb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, response_format: str = \"text\") -> str:\n",
    "    \"\"\"Make a simple LLM call and return the response.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_field(s: str) -> str:\n",
    "    \"\"\"Normalize field names (from ChatGPT35_RowByRow_FirstExample).\"\"\"\n",
    "    import re\n",
    "    s = s.lower().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"\").replace(\",\", \"_\")\\\n",
    "            .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"', '').replace(\"'\", \"\")\\\n",
    "            .replace(\"/\", \"\")\n",
    "    return re.sub('_+', '_', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b154f",
   "metadata": {},
   "source": [
    "## Strategy 1: Full Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d7b515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_full_table(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Fetch the entire table in one query.\n",
    "    No LLM calls needed.\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"full_table\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": meta.get('table_title', ''),\n",
    "            \"columns\": meta.get('columns', []),\n",
    "            \"key_columns\": meta.get('keys', [])\n",
    "        },\n",
    "        \"pagination_config\": {}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce03667",
   "metadata": {},
   "source": [
    "## Strategy 2: Row by Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebab05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_row_by_row(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Fetch all key values first, then fetch each row individually.\n",
    "    Makes 1 LLM call to get all key combinations.\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    table_title = meta.get('table_title', '')\n",
    "    keys = meta.get('keys', [])\n",
    "    \n",
    "    if not keys:\n",
    "        print(f\"Warning: No keys defined for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Normalize key names\n",
    "    norm_keys = [normalize_field(k) for k in keys]\n",
    "    \n",
    "    # Build prompt to fetch all keys (inspired by ChatGPT35_RowByRow_FirstExample)\n",
    "    key_columns_desc = f\"The key column{'s' if len(keys) > 1 else ''} in the table {'are' if len(keys) > 1 else 'is'} {', '.join(keys)}\"\n",
    "    \n",
    "    keys_json_format = ', '.join([f'\"{nk}\": \"{nk}\"' for nk in norm_keys])\n",
    "    \n",
    "    keys_prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to create a table with the detailed information about {table_title}.\n",
    "{key_columns_desc}.\n",
    "List all {', '.join(keys)} entities for the table.\n",
    "The response will be formatted as JSON list shown below.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {keys_json_format}\n",
    "}}]\"\"\"\n",
    "    \n",
    "    print(f\"Fetching keys for {table_data['table_id']}...\")\n",
    "    response = call_llm(keys_prompt)\n",
    "    \n",
    "    if not response:\n",
    "        print(f\"Failed to fetch keys for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the response to extract key values\n",
    "    try:\n",
    "        # Clean up response to extract JSON\n",
    "        if not response.startswith(\"[\") and \"[\" in response:\n",
    "            response = response[response.find(\"[\"):]\n",
    "        if not response.endswith(\"]\") and \"]\" in response:\n",
    "            response = response[:response.rfind(\"]\") + 1]\n",
    "        \n",
    "        key_values = json.loads(response)\n",
    "        \n",
    "        if not isinstance(key_values, list):\n",
    "            print(f\"Invalid response format for {table_data['table_id']}\")\n",
    "            return None\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse keys response for {table_data['table_id']}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"row_by_row\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": table_title,\n",
    "            \"columns\": meta.get('columns', []),\n",
    "            \"key_columns\": keys\n",
    "        },\n",
    "        \"pagination_config\": {\n",
    "            \"key_columns\": keys,\n",
    "            \"key_values\": key_values,\n",
    "            \"total_rows\": len(key_values)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c51e8",
   "metadata": {},
   "source": [
    "## Strategy 3: Attribute-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58a7b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_attribute_based(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Ask LLM which column to partition by, then get distinct values.\n",
    "    Makes 2 LLM calls:\n",
    "    1. Ask which column to use for partitioning\n",
    "    2. Get all distinct values for that column\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    table_title = meta.get('table_title', '')\n",
    "    columns = meta.get('columns', [])\n",
    "    \n",
    "    if not columns:\n",
    "        print(f\"Warning: No columns defined for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Call 1: Ask which column to partition by\n",
    "    partition_prompt = f\"\"\"You are helping to paginate a table about {table_title}.\n",
    "The table has the following columns: {', '.join(columns)}.\n",
    "\n",
    "Which single column would be best to use for partitioning/grouping this table's data?\n",
    "Choose a column that would create meaningful, balanced groups.\n",
    "\n",
    "Respond with ONLY the column name, nothing else.\"\"\"\n",
    "    \n",
    "    print(f\"Asking LLM for partition column for {table_data['table_id']}...\")\n",
    "    partition_column = call_llm(partition_prompt)\n",
    "    \n",
    "    if not partition_column or partition_column not in columns:\n",
    "        print(f\"Invalid partition column '{partition_column}' for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Call 2: Get all distinct values for that column\n",
    "    values_prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to paginate a table about {table_title}.\n",
    "List all distinct values of the column \"{partition_column}\" in this table.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[\"{partition_column}_value1\", \"{partition_column}_value2\", ...]\"\"\"\n",
    "    \n",
    "    print(f\"Fetching distinct values for column '{partition_column}'...\")\n",
    "    response = call_llm(values_prompt)\n",
    "    \n",
    "    if not response:\n",
    "        print(f\"Failed to fetch values for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the response\n",
    "    try:\n",
    "        if not response.startswith(\"[\") and \"[\" in response:\n",
    "            response = response[response.find(\"[\"):]\n",
    "        if not response.endswith(\"]\") and \"]\" in response:\n",
    "            response = response[:response.rfind(\"]\") + 1]\n",
    "        \n",
    "        partition_values = json.loads(response)\n",
    "        \n",
    "        if not isinstance(partition_values, list):\n",
    "            print(f\"Invalid response format for {table_data['table_id']}\")\n",
    "            return None\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse values response for {table_data['table_id']}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"attribute_based\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": table_title,\n",
    "            \"columns\": columns,\n",
    "            \"key_columns\": meta.get('keys', [])\n",
    "        },\n",
    "        \"pagination_config\": {\n",
    "            \"partition_column\": partition_column,\n",
    "            \"partition_values\": partition_values,\n",
    "            \"total_partitions\": len(partition_values)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53949aef",
   "metadata": {},
   "source": [
    "## Strategy 4: Classic Pagination (Offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f3b8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_classic_pagination(table_data: Dict[str, Any], page_size: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Classic offset-based pagination.\n",
    "    No LLM calls - just configuration.\n",
    "    The fetch notebook will iteratively fetch pages.\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    keys = meta.get('keys', [])\n",
    "    \n",
    "    if not keys:\n",
    "        print(f\"Warning: No keys defined for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Default sort order: ascending by key columns\n",
    "    sort_order = [f\"{key} ASC\" for key in keys]\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"classic_pagination\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": meta.get('table_title', ''),\n",
    "            \"columns\": meta.get('columns', []),\n",
    "            \"key_columns\": keys\n",
    "        },\n",
    "        \"pagination_config\": {\n",
    "            \"page_size\": page_size,\n",
    "            \"primary_keys\": keys,\n",
    "            \"sort_order\": sort_order\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b58976",
   "metadata": {},
   "source": [
    "## Strategy 5: Range-based Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "035156f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_range_based(table_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Strategy: Ask LLM for column and bucketing criteria, then define ranges.\n",
    "    Makes 1 LLM call to determine column and bucketing strategy.\n",
    "    \"\"\"\n",
    "    meta = table_data['meta']\n",
    "    table_title = meta.get('table_title', '')\n",
    "    columns = meta.get('columns', [])\n",
    "    \n",
    "    if not columns:\n",
    "        print(f\"Warning: No columns defined for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Ask LLM for column and bucketing strategy\n",
    "    range_prompt = f\"\"\"You are helping to paginate a table about {table_title}.\n",
    "The table has the following columns: {', '.join(columns)}.\n",
    "\n",
    "Suggest the best column to use for range-based pagination and describe how to bucket the data.\n",
    "For example: \"year, by decade\" or \"price, by $100 ranges\" or \"date, by month\".\n",
    "\n",
    "Respond in the format: \"<column_name>, <bucketing_description>\"\n",
    "Example: \"year, by decade\"\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Asking LLM for range strategy for {table_data['table_id']}...\")\n",
    "    response = call_llm(range_prompt)\n",
    "    \n",
    "    if not response:\n",
    "        print(f\"Failed to get range strategy for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse response (expected format: \"column, bucketing\")\n",
    "    parts = response.split(',', 1)\n",
    "    if len(parts) != 2:\n",
    "        print(f\"Invalid range response format for {table_data['table_id']}: {response}\")\n",
    "        return None\n",
    "    \n",
    "    partition_column = parts[0].strip().strip('\"').strip(\"'\")\n",
    "    bucketing_criteria = parts[1].strip().strip('\"').strip(\"'\")\n",
    "    \n",
    "    # Now ask for the actual ranges\n",
    "    ranges_prompt = f\"\"\"You are a retriever of facts.\n",
    "For a table about {table_title}, we want to paginate by {partition_column} using {bucketing_criteria}.\n",
    "\n",
    "List all the ranges needed. For each range, provide the lower bound (inclusive) and upper bound (exclusive).\n",
    "\n",
    "RESPONSE FORMAT (JSON array of objects):\n",
    "[\n",
    "    {{\"gte\": \"lower_value\", \"lt\": \"upper_value\"}},\n",
    "    {{\"gte\": \"lower_value\", \"lt\": \"upper_value\"}}\n",
    "]\n",
    "\n",
    "Example for \"year by decade\":\n",
    "[\n",
    "    {{\"gte\": \"1980\", \"lt\": \"1990\"}},\n",
    "    {{\"gte\": \"1990\", \"lt\": \"2000\"}}\n",
    "]\n",
    "\"\"\"\n",
    "    \n",
    "    print(f\"Fetching ranges for {partition_column} {bucketing_criteria}...\")\n",
    "    ranges_response = call_llm(ranges_prompt)\n",
    "    \n",
    "    if not ranges_response:\n",
    "        print(f\"Failed to get ranges for {table_data['table_id']}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse ranges\n",
    "    try:\n",
    "        if not ranges_response.startswith(\"[\") and \"[\" in ranges_response:\n",
    "            ranges_response = ranges_response[ranges_response.find(\"[\"):]\n",
    "        if not ranges_response.endswith(\"]\") and \"]\" in ranges_response:\n",
    "            ranges_response = ranges_response[:ranges_response.rfind(\"]\") + 1]\n",
    "        \n",
    "        ranges = json.loads(ranges_response)\n",
    "        \n",
    "        if not isinstance(ranges, list):\n",
    "            print(f\"Invalid ranges format for {table_data['table_id']}\")\n",
    "            return None\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse ranges for {table_data['table_id']}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_data['table_id'],\n",
    "        \"table_name\": meta.get('name', ''),\n",
    "        \"strategy\": \"range_based\",\n",
    "        \"metadata\": {\n",
    "            \"table_title\": table_title,\n",
    "            \"columns\": columns,\n",
    "            \"key_columns\": meta.get('keys', [])\n",
    "        },\n",
    "        \"pagination_config\": {\n",
    "            \"partition_column\": partition_column,\n",
    "            \"bucketing_criteria\": bucketing_criteria,\n",
    "            \"ranges\": ranges,\n",
    "            \"total_ranges\": len(ranges)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb1de0",
   "metadata": {},
   "source": [
    "## Main Execution: Generate Plans for All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d48ebb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active strategies: ['full_table', 'row_by_row', 'attribute_based', 'classic_pagination', 'range_based']\n",
      "Processing 48 table(s) out of 48 total...\n"
     ]
    }
   ],
   "source": [
    "# Configuration: which strategies to run\n",
    "STRATEGIES_TO_RUN = {\n",
    "    'full_table': plan_full_table,\n",
    "    'row_by_row': plan_row_by_row,\n",
    "    'attribute_based': plan_attribute_based,\n",
    "    'classic_pagination': plan_classic_pagination,\n",
    "    'range_based': plan_range_based\n",
    "}\n",
    "\n",
    "# Choose which strategies to execute (comment out ones you don't want)\n",
    "ACTIVE_STRATEGIES = [\n",
    "    'full_table',\n",
    "    'row_by_row',\n",
    "    'attribute_based',\n",
    "    'classic_pagination',\n",
    "    'range_based',\n",
    "]\n",
    "\n",
    "# Apply MAX_TABLES limit if set\n",
    "if MAX_TABLES is not None:\n",
    "    print(f\"📊 LIMITED RUN: Processing first {MAX_TABLES} table(s)\")\n",
    "    tables_to_process = tables[:MAX_TABLES]\n",
    "else:\n",
    "    tables_to_process = tables\n",
    "\n",
    "print(f\"Active strategies: {ACTIVE_STRATEGIES}\")\n",
    "print(f\"Processing {len(tables_to_process)} table(s) out of {len(tables)} total...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9e40a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: processing/1_strategy/20251005_004530\n",
      "\n",
      "⚡ Running 5 strategies in PARALLEL with 5 workers\n",
      "\n",
      "============================================================\n",
      "Running strategy: FULL_TABLE\n",
      "============================================================\n",
      "\n",
      "[full_table] [1/48] Processing 10_men_butterfly_100m_2009...\n",
      "\n",
      "============================================================\n",
      "Running strategy: ROW_BY_ROW\n",
      "============================================================\n",
      "\n",
      "[row_by_row] [1/48] Processing 10_men_butterfly_100m_2009...\n",
      "Fetching keys for 10_men_butterfly_100m_2009...\n",
      "\n",
      "============================================================\n",
      "Running strategy: ATTRIBUTE_BASED\n",
      "============================================================\n",
      "\n",
      "[attribute_based] [1/48] Processing 10_men_butterfly_100m_2009...\n",
      "Asking LLM for partition column for 10_men_butterfly_100m_2009...\n",
      "\n",
      "============================================================\n",
      "Running strategy: CLASSIC_PAGINATION\n",
      "============================================================\n",
      "\n",
      "[classic_pagination] [1/48] Processing 10_men_butterfly_100m_2009...\n",
      "\n",
      "============================================================\n",
      "Running strategy: RANGE_BASED\n",
      "============================================================\n",
      "\n",
      "[range_based] [1/48] Processing 10_men_butterfly_100m_2009...\n",
      "Asking LLM for range strategy for 10_men_butterfly_100m_2009...\n",
      "[full_table]   ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "[full_table] [2/48] Processing 11_playstation_3_cooperative_games...\n",
      "[classic_pagination]   ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "[classic_pagination] [2/48] Processing 11_playstation_3_cooperative_games...\n",
      "[full_table]   ✓ Success - Saved to 11_playstation_3_cooperative_games.json\n",
      "[full_table] [3/48] Processing 12_rock_band_downloadable_2011...\n",
      "[classic_pagination]   ✓ Success - Saved to 11_playstation_3_cooperative_games.json\n",
      "[classic_pagination] [3/48] Processing 12_rock_band_downloadable_2011...\n",
      "[full_table]   ✓ Success - Saved to 12_rock_band_downloadable_2011.json\n",
      "[full_table] [4/48] Processing 13_figure_skating_ladies_2009_2010...\n",
      "[classic_pagination]   ✓ Success - Saved to 12_rock_band_downloadable_2011.json\n",
      "[classic_pagination] [4/48] Processing 13_figure_skating_ladies_2009_2010...\n",
      "[full_table]   ✓ Success - Saved to 13_figure_skating_ladies_2009_2010.json\n",
      "[full_table] [5/48] Processing 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "[full_table]   ✓ Success - Saved to 14_minor_planets_discovered_by_nikolai_chernykh.json\n",
      "[full_table] [6/48] Processing 16_curling_teams_women_2013_2014...\n",
      "[classic_pagination]   ✓ Success - Saved to 13_figure_skating_ladies_2009_2010.json\n",
      "[classic_pagination] [5/48] Processing 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "[full_table]   ✓ Success - Saved to 16_curling_teams_women_2013_2014.json\n",
      "[full_table] [7/48] Processing 17_scottish_football_transfers_summer_2011...\n",
      "[classic_pagination]   ✓ Success - Saved to 14_minor_planets_discovered_by_nikolai_chernykh.json\n",
      "[classic_pagination] [6/48] Processing 16_curling_teams_women_2013_2014...\n",
      "[full_table]   ✓ Success - Saved to 17_scottish_football_transfers_summer_2011.json\n",
      "[full_table] [8/48] Processing 19_living_proof_the_farewell_tour...\n",
      "[classic_pagination]   ✓ Success - Saved to 16_curling_teams_women_2013_2014.json\n",
      "[classic_pagination] [7/48] Processing 17_scottish_football_transfers_summer_2011...\n",
      "[full_table]   ✓ Success - Saved to 19_living_proof_the_farewell_tour.json\n",
      "[full_table] [9/48] Processing 20_new_zealand_demographics_1921_2011...\n",
      "[classic_pagination]   ✓ Success - Saved to 17_scottish_football_transfers_summer_2011.json\n",
      "[classic_pagination] [8/48] Processing 19_living_proof_the_farewell_tour...\n",
      "[full_table]   ✓ Success - Saved to 20_new_zealand_demographics_1921_2011.json\n",
      "[full_table] [10/48] Processing 21_liechtenstein_demographics_1901_2011...\n",
      "[classic_pagination]   ✓ Success - Saved to 19_living_proof_the_farewell_tour.json\n",
      "[classic_pagination] [9/48] Processing 20_new_zealand_demographics_1921_2011...\n",
      "[full_table]   ✓ Success - Saved to 21_liechtenstein_demographics_1901_2011.json\n",
      "[full_table] [11/48] Processing 22_usa_demographics_1935_2010...\n",
      "[classic_pagination]   ✓ Success - Saved to 20_new_zealand_demographics_1921_2011.json\n",
      "[classic_pagination] [10/48] Processing 21_liechtenstein_demographics_1901_2011...\n",
      "[full_table]   ✓ Success - Saved to 22_usa_demographics_1935_2010.json\n",
      "[full_table] [12/48] Processing 23_andorra_demographics_1948_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 21_liechtenstein_demographics_1901_2011.json\n",
      "[classic_pagination] [11/48] Processing 22_usa_demographics_1935_2010...\n",
      "[full_table]   ✓ Success - Saved to 23_andorra_demographics_1948_2012.json\n",
      "[full_table] [13/48] Processing 25_english_latin_rivalry_1887_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 22_usa_demographics_1935_2010.json\n",
      "[classic_pagination] [12/48] Processing 23_andorra_demographics_1948_2012...\n",
      "[full_table]   ✓ Success - Saved to 25_english_latin_rivalry_1887_2012.json\n",
      "[full_table] [14/48] Processing 28_equestrian_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 23_andorra_demographics_1948_2012.json\n",
      "[classic_pagination] [13/48] Processing 25_english_latin_rivalry_1887_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 25_english_latin_rivalry_1887_2012.json\n",
      "[classic_pagination] [14/48] Processing 28_equestrian_2012...\n",
      "[full_table]   ✓ Success - Saved to 28_equestrian_2012.json\n",
      "[full_table] [15/48] Processing 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 28_equestrian_2012.json\n",
      "[classic_pagination] [15/48] Processing 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "[full_table]   ✓ Success - Saved to 29_tennessee_vanderbilt_rivalry_1900_2012.json\n",
      "[full_table] [16/48] Processing 2_belgium_demographics_1900_2011...\n",
      "[classic_pagination]   ✓ Success - Saved to 29_tennessee_vanderbilt_rivalry_1900_2012.json\n",
      "[classic_pagination] [16/48] Processing 2_belgium_demographics_1900_2011...\n",
      "[full_table]   ✓ Success - Saved to 2_belgium_demographics_1900_2011.json\n",
      "[full_table] [17/48] Processing 30_classic_100_ten_years_on...\n",
      "[classic_pagination]   ✓ Success - Saved to 2_belgium_demographics_1900_2011.json\n",
      "[classic_pagination] [17/48] Processing 30_classic_100_ten_years_on...\n",
      "[classic_pagination]   ✓ Success - Saved to 30_classic_100_ten_years_on.json\n",
      "[classic_pagination] [18/48] Processing 31_udaykumar_films...\n",
      "[full_table]   ✓ Success - Saved to 30_classic_100_ten_years_on.json\n",
      "[full_table] [18/48] Processing 31_udaykumar_films...\n",
      "[classic_pagination]   ✓ Success - Saved to 31_udaykumar_films.json\n",
      "[classic_pagination] [19/48] Processing 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "[full_table]   ✓ Success - Saved to 31_udaykumar_films.json\n",
      "[full_table] [19/48] Processing 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "[classic_pagination]   ✓ Success - Saved to 32_fa_cup_qualifying_rounds_1999_2000.json\n",
      "[classic_pagination] [20/48] Processing 33_portuguese_grape_varieties...\n",
      "[full_table]   ✓ Success - Saved to 32_fa_cup_qualifying_rounds_1999_2000.json\n",
      "[full_table] [20/48] Processing 33_portuguese_grape_varieties...\n",
      "[classic_pagination]   ✓ Success - Saved to 33_portuguese_grape_varieties.json\n",
      "[classic_pagination] [21/48] Processing 34_ramsar_convention_parties...\n",
      "[full_table]   ✓ Success - Saved to 33_portuguese_grape_varieties.json\n",
      "[full_table] [21/48] Processing 34_ramsar_convention_parties...\n",
      "[classic_pagination]   ✓ Success - Saved to 34_ramsar_convention_parties.json\n",
      "[classic_pagination] [22/48] Processing 35_guitar_hero_5_songs...\n",
      "[full_table]   ✓ Success - Saved to 34_ramsar_convention_parties.json\n",
      "[full_table] [22/48] Processing 35_guitar_hero_5_songs...\n",
      "[classic_pagination]   ✓ Success - Saved to 35_guitar_hero_5_songs.json\n",
      "[classic_pagination] [23/48] Processing 36_south_cambridgeshire_district_council_1973_2012...\n",
      "[full_table]   ✓ Success - Saved to 35_guitar_hero_5_songs.json\n",
      "[full_table] [23/48] Processing 36_south_cambridgeshire_district_council_1973_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 36_south_cambridgeshire_district_council_1973_2012.json\n",
      "[classic_pagination] [24/48] Processing 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "[full_table]   ✓ Success - Saved to 36_south_cambridgeshire_district_council_1973_2012.json\n",
      "[full_table] [24/48] Processing 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "[classic_pagination]   ✓ Success - Saved to 37_dublin_maternity_hospital_mortality_rates_1784_1849.json\n",
      "[classic_pagination] [25/48] Processing 38_ship_launches_january_1944...\n",
      "[full_table]   ✓ Success - Saved to 37_dublin_maternity_hospital_mortality_rates_1784_1849.json\n",
      "[full_table] [25/48] Processing 38_ship_launches_january_1944...\n",
      "[classic_pagination]   ✓ Success - Saved to 38_ship_launches_january_1944.json\n",
      "[classic_pagination] [26/48] Processing 39_uk_demographics_1960_2012...\n",
      "[full_table]   ✓ Success - Saved to 38_ship_launches_january_1944.json\n",
      "[full_table] [26/48] Processing 39_uk_demographics_1960_2012...\n",
      "[full_table]   ✓ Success - Saved to 39_uk_demographics_1960_2012.json\n",
      "[full_table] [27/48] Processing 3_australia_demographics_1900_2010...\n",
      "[classic_pagination]   ✓ Success - Saved to 39_uk_demographics_1960_2012.json\n",
      "[classic_pagination] [27/48] Processing 3_australia_demographics_1900_2010...\n",
      "[full_table]   ✓ Success - Saved to 3_australia_demographics_1900_2010.json\n",
      "[full_table] [28/48] Processing 40_fukushima_plant_operating_history_1970_2009...\n",
      "[classic_pagination]   ✓ Success - Saved to 3_australia_demographics_1900_2010.json\n",
      "[classic_pagination] [28/48] Processing 40_fukushima_plant_operating_history_1970_2009...\n",
      "[full_table]   ✓ Success - Saved to 40_fukushima_plant_operating_history_1970_2009.json\n",
      "[full_table] [29/48] Processing 41_new_zealand_football_results_1922_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 40_fukushima_plant_operating_history_1970_2009.json\n",
      "[classic_pagination] [29/48] Processing 41_new_zealand_football_results_1922_2012...\n",
      "[full_table]   ✓ Success - Saved to 41_new_zealand_football_results_1922_2012.json\n",
      "[full_table] [30/48] Processing 42_jack_nicklaus_achievements_1962_2005...\n",
      "[classic_pagination]   ✓ Success - Saved to 41_new_zealand_football_results_1922_2012.json\n",
      "[classic_pagination] [30/48] Processing 42_jack_nicklaus_achievements_1962_2005...\n",
      "[full_table]   ✓ Success - Saved to 42_jack_nicklaus_achievements_1962_2005.json\n",
      "[full_table] [31/48] Processing 47_european_countries_gdp_2007_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 42_jack_nicklaus_achievements_1962_2005.json\n",
      "[classic_pagination] [31/48] Processing 47_european_countries_gdp_2007_2012...\n",
      "[full_table]   ✓ Success - Saved to 47_european_countries_gdp_2007_2012.json\n",
      "[full_table] [32/48] Processing 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "[classic_pagination]   ✓ Success - Saved to 47_european_countries_gdp_2007_2012.json\n",
      "[classic_pagination] [32/48] Processing 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "[full_table]   ✓ Success - Saved to 48_royal_dulton_figurines_HN4100_HN4199.json\n",
      "[full_table] [33/48] Processing 49_adaalat_episodes_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 48_royal_dulton_figurines_HN4100_HN4199.json\n",
      "[classic_pagination] [33/48] Processing 49_adaalat_episodes_2012...\n",
      "[full_table]   ✓ Success - Saved to 49_adaalat_episodes_2012.json\n",
      "[full_table] [34/48] Processing 4_new_brunswick_parishes_2006_2011...\n",
      "[classic_pagination]   ✓ Success - Saved to 49_adaalat_episodes_2012.json\n",
      "[classic_pagination] [34/48] Processing 4_new_brunswick_parishes_2006_2011...\n",
      "[full_table]   ✓ Success - Saved to 4_new_brunswick_parishes_2006_2011.json\n",
      "[full_table] [35/48] Processing 51_just_dance_kids_2_tracks...\n",
      "[classic_pagination]   ✓ Success - Saved to 4_new_brunswick_parishes_2006_2011.json\n",
      "[classic_pagination] [35/48] Processing 51_just_dance_kids_2_tracks...\n",
      "[full_table]   ✓ Success - Saved to 51_just_dance_kids_2_tracks.json\n",
      "[full_table] [36/48] Processing 52_cross_country_junior_women_1996...\n",
      "[classic_pagination]   ✓ Success - Saved to 51_just_dance_kids_2_tracks.json\n",
      "[classic_pagination] [36/48] Processing 52_cross_country_junior_women_1996...\n",
      "[full_table]   ✓ Success - Saved to 52_cross_country_junior_women_1996.json\n",
      "[full_table] [37/48] Processing 53_metropolitan_opera_us_premieres...\n",
      "[classic_pagination]   ✓ Success - Saved to 52_cross_country_junior_women_1996.json\n",
      "[classic_pagination] [37/48] Processing 53_metropolitan_opera_us_premieres...\n",
      "[full_table]   ✓ Success - Saved to 53_metropolitan_opera_us_premieres.json\n",
      "[full_table] [38/48] Processing 54_kasparov_kramnik_1993_2004...\n",
      "[classic_pagination]   ✓ Success - Saved to 53_metropolitan_opera_us_premieres.json\n",
      "[classic_pagination] [38/48] Processing 54_kasparov_kramnik_1993_2004...\n",
      "[full_table]   ✓ Success - Saved to 54_kasparov_kramnik_1993_2004.json\n",
      "[full_table] [39/48] Processing 55_decathlon_top50_1999...\n",
      "[classic_pagination]   ✓ Success - Saved to 54_kasparov_kramnik_1993_2004.json\n",
      "[classic_pagination] [39/48] Processing 55_decathlon_top50_1999...\n",
      "[full_table]   ✓ Success - Saved to 55_decathlon_top50_1999.json\n",
      "[full_table] [40/48] Processing 56_minor_planets_152601_152700...\n",
      "[classic_pagination]   ✓ Success - Saved to 55_decathlon_top50_1999.json\n",
      "[classic_pagination] [40/48] Processing 56_minor_planets_152601_152700...\n",
      "[full_table]   ✓ Success - Saved to 56_minor_planets_152601_152700.json\n",
      "[full_table] [41/48] Processing 59_miss_new_york_usa_delegates_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 56_minor_planets_152601_152700.json\n",
      "[classic_pagination] [41/48] Processing 59_miss_new_york_usa_delegates_2012...\n",
      "[full_table]   ✓ Success - Saved to 59_miss_new_york_usa_delegates_2012.json\n",
      "[full_table] [42/48] Processing 5_ice_hockey_2006...\n",
      "[classic_pagination]   ✓ Success - Saved to 59_miss_new_york_usa_delegates_2012.json\n",
      "[classic_pagination] [42/48] Processing 5_ice_hockey_2006...\n",
      "[full_table]   ✓ Success - Saved to 5_ice_hockey_2006.json\n",
      "[full_table] [43/48] Processing 67_london_heathrow_busiest_routes_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 5_ice_hockey_2006.json\n",
      "[classic_pagination] [43/48] Processing 67_london_heathrow_busiest_routes_2012...\n",
      "[classic_pagination]   ✓ Success - Saved to 67_london_heathrow_busiest_routes_2012.json\n",
      "[classic_pagination] [44/48] Processing 71_us_president_elections_idaho_2008...\n",
      "[full_table]   ✓ Success - Saved to 67_london_heathrow_busiest_routes_2012.json\n",
      "[full_table] [44/48] Processing 71_us_president_elections_idaho_2008...\n",
      "[classic_pagination]   ✓ Success - Saved to 71_us_president_elections_idaho_2008.json\n",
      "[classic_pagination] [45/48] Processing 78_bafta_best_actor_leading_role_2000s...\n",
      "[full_table]   ✓ Success - Saved to 71_us_president_elections_idaho_2008.json\n",
      "[full_table] [45/48] Processing 78_bafta_best_actor_leading_role_2000s...\n",
      "[classic_pagination]   ✓ Success - Saved to 78_bafta_best_actor_leading_role_2000s.json\n",
      "[classic_pagination] [46/48] Processing 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "[full_table]   ✓ Success - Saved to 78_bafta_best_actor_leading_role_2000s.json\n",
      "[full_table] [46/48] Processing 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "[classic_pagination]   ✓ Success - Saved to 7_anaheim_ducks_draft_picks_1998_2013.json\n",
      "[classic_pagination] [47/48] Processing 8_south_african_class_15f_4_8_2...\n",
      "[full_table]   ✓ Success - Saved to 7_anaheim_ducks_draft_picks_1998_2013.json\n",
      "[full_table] [47/48] Processing 8_south_african_class_15f_4_8_2...\n",
      "[classic_pagination]   ✓ Success - Saved to 8_south_african_class_15f_4_8_2.json\n",
      "[classic_pagination] [48/48] Processing 9_tour_de_france_2009...\n",
      "[full_table]   ✓ Success - Saved to 8_south_african_class_15f_4_8_2.json\n",
      "[full_table] [48/48] Processing 9_tour_de_france_2009...\n",
      "[classic_pagination]   ✓ Success - Saved to 9_tour_de_france_2009.json\n",
      "\n",
      "[classic_pagination] Completed: 48 successes, 0 errors\n",
      "[full_table]   ✓ Success - Saved to 9_tour_de_france_2009.json\n",
      "\n",
      "[full_table] Completed: 48 successes, 0 errors\n",
      "Fetching distinct values for column 'Heat'...\n",
      "Fetching distinct values for column 'Heat'...\n",
      "Fetching ranges for Time by 0.5-second intervals...\n",
      "Fetching ranges for Time by 0.5-second intervals...\n",
      "[attribute_based]   ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "[attribute_based] [2/48] Processing 11_playstation_3_cooperative_games...\n",
      "Asking LLM for partition column for 11_playstation_3_cooperative_games...\n",
      "[attribute_based]   ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "[attribute_based] [2/48] Processing 11_playstation_3_cooperative_games...\n",
      "Asking LLM for partition column for 11_playstation_3_cooperative_games...\n",
      "Fetching distinct values for column 'Co-op campaign'...\n",
      "Fetching distinct values for column 'Co-op campaign'...\n",
      "[attribute_based]   ✓ Success - Saved to 11_playstation_3_cooperative_games.json\n",
      "[attribute_based] [3/48] Processing 12_rock_band_downloadable_2011...\n",
      "Asking LLM for partition column for 12_rock_band_downloadable_2011...\n",
      "[attribute_based]   ✓ Success - Saved to 11_playstation_3_cooperative_games.json\n",
      "[attribute_based] [3/48] Processing 12_rock_band_downloadable_2011...\n",
      "Asking LLM for partition column for 12_rock_band_downloadable_2011...\n",
      "[row_by_row]   ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "[row_by_row] [2/48] Processing 11_playstation_3_cooperative_games...\n",
      "Fetching keys for 11_playstation_3_cooperative_games...\n",
      "[row_by_row]   ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "[row_by_row] [2/48] Processing 11_playstation_3_cooperative_games...\n",
      "Fetching keys for 11_playstation_3_cooperative_games...\n",
      "Fetching distinct values for column 'Release date'...\n",
      "Fetching distinct values for column 'Release date'...\n",
      "[range_based]   ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "[range_based] [2/48] Processing 11_playstation_3_cooperative_games...\n",
      "Asking LLM for range strategy for 11_playstation_3_cooperative_games...\n",
      "[range_based]   ✓ Success - Saved to 10_men_butterfly_100m_2009.json\n",
      "[range_based] [2/48] Processing 11_playstation_3_cooperative_games...\n",
      "Asking LLM for range strategy for 11_playstation_3_cooperative_games...\n",
      "Fetching ranges for Release Date by year and then by quarter...\n",
      "Fetching ranges for Release Date by year and then by quarter...\n",
      "[row_by_row]   ✓ Success - Saved to 11_playstation_3_cooperative_games.json\n",
      "[row_by_row] [3/48] Processing 12_rock_band_downloadable_2011...\n",
      "Fetching keys for 12_rock_band_downloadable_2011...\n",
      "[row_by_row]   ✓ Success - Saved to 11_playstation_3_cooperative_games.json\n",
      "[row_by_row] [3/48] Processing 12_rock_band_downloadable_2011...\n",
      "Fetching keys for 12_rock_band_downloadable_2011...\n",
      "[attribute_based]   ✓ Success - Saved to 12_rock_band_downloadable_2011.json\n",
      "[attribute_based] [4/48] Processing 13_figure_skating_ladies_2009_2010...\n",
      "Asking LLM for partition column for 13_figure_skating_ladies_2009_2010...\n",
      "[attribute_based]   ✓ Success - Saved to 12_rock_band_downloadable_2011.json\n",
      "[attribute_based] [4/48] Processing 13_figure_skating_ladies_2009_2010...\n",
      "Asking LLM for partition column for 13_figure_skating_ladies_2009_2010...\n",
      "Fetching distinct values for column 'Event'...\n",
      "Fetching distinct values for column 'Event'...\n",
      "[row_by_row]   ✓ Success - Saved to 12_rock_band_downloadable_2011.json\n",
      "[row_by_row] [4/48] Processing 13_figure_skating_ladies_2009_2010...\n",
      "Fetching keys for 13_figure_skating_ladies_2009_2010...\n",
      "[row_by_row]   ✓ Success - Saved to 12_rock_band_downloadable_2011.json\n",
      "[row_by_row] [4/48] Processing 13_figure_skating_ladies_2009_2010...\n",
      "Fetching keys for 13_figure_skating_ladies_2009_2010...\n",
      "[attribute_based]   ✓ Success - Saved to 13_figure_skating_ladies_2009_2010.json\n",
      "[attribute_based] [5/48] Processing 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "Asking LLM for partition column for 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "[attribute_based]   ✓ Success - Saved to 13_figure_skating_ladies_2009_2010.json\n",
      "[attribute_based] [5/48] Processing 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "Asking LLM for partition column for 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "[row_by_row]   ✓ Success - Saved to 13_figure_skating_ladies_2009_2010.json\n",
      "[row_by_row] [5/48] Processing 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "Fetching keys for 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "[row_by_row]   ✓ Success - Saved to 13_figure_skating_ladies_2009_2010.json\n",
      "[row_by_row] [5/48] Processing 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "Fetching keys for 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "Fetching distinct values for column 'Discovery Date'...\n",
      "Fetching distinct values for column 'Discovery Date'...\n",
      "[range_based]   ✓ Success - Saved to 11_playstation_3_cooperative_games.json\n",
      "[range_based] [3/48] Processing 12_rock_band_downloadable_2011...\n",
      "Asking LLM for range strategy for 12_rock_band_downloadable_2011...\n",
      "[range_based]   ✓ Success - Saved to 11_playstation_3_cooperative_games.json\n",
      "[range_based] [3/48] Processing 12_rock_band_downloadable_2011...\n",
      "Asking LLM for range strategy for 12_rock_band_downloadable_2011...\n",
      "Fetching ranges for Release date by month...\n",
      "Fetching ranges for Release date by month...\n",
      "[attribute_based]   ✓ Success - Saved to 14_minor_planets_discovered_by_nikolai_chernykh.json\n",
      "[attribute_based] [6/48] Processing 16_curling_teams_women_2013_2014...\n",
      "Asking LLM for partition column for 16_curling_teams_women_2013_2014...\n",
      "[attribute_based]   ✓ Success - Saved to 14_minor_planets_discovered_by_nikolai_chernykh.json\n",
      "[attribute_based] [6/48] Processing 16_curling_teams_women_2013_2014...\n",
      "Asking LLM for partition column for 16_curling_teams_women_2013_2014...\n",
      "[row_by_row]   ✓ Success - Saved to 14_minor_planets_discovered_by_nikolai_chernykh.json\n",
      "[row_by_row] [6/48] Processing 16_curling_teams_women_2013_2014...\n",
      "Fetching keys for 16_curling_teams_women_2013_2014...\n",
      "[row_by_row]   ✓ Success - Saved to 14_minor_planets_discovered_by_nikolai_chernykh.json\n",
      "[row_by_row] [6/48] Processing 16_curling_teams_women_2013_2014...\n",
      "Fetching keys for 16_curling_teams_women_2013_2014...\n",
      "Fetching distinct values for column 'Locale'...\n",
      "Fetching distinct values for column 'Locale'...\n",
      "[range_based]   ✓ Success - Saved to 12_rock_band_downloadable_2011.json\n",
      "[range_based] [4/48] Processing 13_figure_skating_ladies_2009_2010...\n",
      "Asking LLM for range strategy for 13_figure_skating_ladies_2009_2010...\n",
      "[range_based]   ✓ Success - Saved to 12_rock_band_downloadable_2011.json\n",
      "[range_based] [4/48] Processing 13_figure_skating_ladies_2009_2010...\n",
      "Asking LLM for range strategy for 13_figure_skating_ladies_2009_2010...\n",
      "[attribute_based]   ✓ Success - Saved to 16_curling_teams_women_2013_2014.json\n",
      "[attribute_based] [7/48] Processing 17_scottish_football_transfers_summer_2011...\n",
      "Asking LLM for partition column for 17_scottish_football_transfers_summer_2011...\n",
      "Fetching ranges for Points by 10-point ranges...\n",
      "[attribute_based]   ✓ Success - Saved to 16_curling_teams_women_2013_2014.json\n",
      "[attribute_based] [7/48] Processing 17_scottish_football_transfers_summer_2011...\n",
      "Asking LLM for partition column for 17_scottish_football_transfers_summer_2011...\n",
      "Fetching ranges for Points by 10-point ranges...\n",
      "[row_by_row]   ✓ Success - Saved to 16_curling_teams_women_2013_2014.json\n",
      "[row_by_row] [7/48] Processing 17_scottish_football_transfers_summer_2011...\n",
      "Fetching keys for 17_scottish_football_transfers_summer_2011...\n",
      "Fetching distinct values for column 'Moving from'...\n",
      "[row_by_row]   ✓ Success - Saved to 16_curling_teams_women_2013_2014.json\n",
      "[row_by_row] [7/48] Processing 17_scottish_football_transfers_summer_2011...\n",
      "Fetching keys for 17_scottish_football_transfers_summer_2011...\n",
      "Fetching distinct values for column 'Moving from'...\n",
      "[row_by_row]   ✓ Success - Saved to 17_scottish_football_transfers_summer_2011.json\n",
      "[row_by_row] [8/48] Processing 19_living_proof_the_farewell_tour...\n",
      "Fetching keys for 19_living_proof_the_farewell_tour...\n",
      "[row_by_row]   ✓ Success - Saved to 17_scottish_football_transfers_summer_2011.json\n",
      "[row_by_row] [8/48] Processing 19_living_proof_the_farewell_tour...\n",
      "Fetching keys for 19_living_proof_the_farewell_tour...\n",
      "[range_based]   ✓ Success - Saved to 13_figure_skating_ladies_2009_2010.json\n",
      "[range_based] [5/48] Processing 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "Asking LLM for range strategy for 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "[range_based]   ✓ Success - Saved to 13_figure_skating_ladies_2009_2010.json\n",
      "[range_based] [5/48] Processing 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "Asking LLM for range strategy for 14_minor_planets_discovered_by_nikolai_chernykh...\n",
      "Fetching ranges for Discovery Date by year...\n",
      "Fetching ranges for Discovery Date by year...\n",
      "[range_based]   ✓ Success - Saved to 14_minor_planets_discovered_by_nikolai_chernykh.json\n",
      "[range_based] [6/48] Processing 16_curling_teams_women_2013_2014...\n",
      "Asking LLM for range strategy for 16_curling_teams_women_2013_2014...\n",
      "[range_based]   ✓ Success - Saved to 14_minor_planets_discovered_by_nikolai_chernykh.json\n",
      "[range_based] [6/48] Processing 16_curling_teams_women_2013_2014...\n",
      "Asking LLM for range strategy for 16_curling_teams_women_2013_2014...\n",
      "[row_by_row]   ✓ Success - Saved to 19_living_proof_the_farewell_tour.json\n",
      "[row_by_row] [9/48] Processing 20_new_zealand_demographics_1921_2011...\n",
      "Fetching keys for 20_new_zealand_demographics_1921_2011...\n",
      "[row_by_row]   ✓ Success - Saved to 19_living_proof_the_farewell_tour.json\n",
      "[row_by_row] [9/48] Processing 20_new_zealand_demographics_1921_2011...\n",
      "Fetching keys for 20_new_zealand_demographics_1921_2011...\n",
      "Fetching ranges for Skip alphabetically by the first letter of the skip's last name...\n",
      "Fetching ranges for Skip alphabetically by the first letter of the skip's last name...\n",
      "[row_by_row]   ✓ Success - Saved to 20_new_zealand_demographics_1921_2011.json\n",
      "[row_by_row] [10/48] Processing 21_liechtenstein_demographics_1901_2011...\n",
      "Fetching keys for 21_liechtenstein_demographics_1901_2011...\n",
      "[row_by_row]   ✓ Success - Saved to 20_new_zealand_demographics_1921_2011.json\n",
      "[row_by_row] [10/48] Processing 21_liechtenstein_demographics_1901_2011...\n",
      "Fetching keys for 21_liechtenstein_demographics_1901_2011...\n",
      "[range_based]   ✓ Success - Saved to 16_curling_teams_women_2013_2014.json\n",
      "[range_based] [7/48] Processing 17_scottish_football_transfers_summer_2011...\n",
      "Asking LLM for range strategy for 17_scottish_football_transfers_summer_2011...\n",
      "[range_based]   ✓ Success - Saved to 16_curling_teams_women_2013_2014.json\n",
      "[range_based] [7/48] Processing 17_scottish_football_transfers_summer_2011...\n",
      "Asking LLM for range strategy for 17_scottish_football_transfers_summer_2011...\n",
      "Fetching ranges for Name alphabetically by first letter of the player's last name...\n",
      "Fetching ranges for Name alphabetically by first letter of the player's last name...\n",
      "[range_based]   ✓ Success - Saved to 17_scottish_football_transfers_summer_2011.json\n",
      "[range_based] [8/48] Processing 19_living_proof_the_farewell_tour...\n",
      "Asking LLM for range strategy for 19_living_proof_the_farewell_tour...\n",
      "[range_based]   ✓ Success - Saved to 17_scottish_football_transfers_summer_2011.json\n",
      "[range_based] [8/48] Processing 19_living_proof_the_farewell_tour...\n",
      "Asking LLM for range strategy for 19_living_proof_the_farewell_tour...\n",
      "Fetching ranges for Date by month...\n",
      "Fetching ranges for Date by month...\n",
      "[row_by_row]   ✓ Success - Saved to 21_liechtenstein_demographics_1901_2011.json\n",
      "[row_by_row] [11/48] Processing 22_usa_demographics_1935_2010...\n",
      "Fetching keys for 22_usa_demographics_1935_2010...\n",
      "[row_by_row]   ✓ Success - Saved to 21_liechtenstein_demographics_1901_2011.json\n",
      "[row_by_row] [11/48] Processing 22_usa_demographics_1935_2010...\n",
      "Fetching keys for 22_usa_demographics_1935_2010...\n",
      "[range_based]   ✓ Success - Saved to 19_living_proof_the_farewell_tour.json\n",
      "[range_based] [9/48] Processing 20_new_zealand_demographics_1921_2011...\n",
      "Asking LLM for range strategy for 20_new_zealand_demographics_1921_2011...\n",
      "[range_based]   ✓ Success - Saved to 19_living_proof_the_farewell_tour.json\n",
      "[range_based] [9/48] Processing 20_new_zealand_demographics_1921_2011...\n",
      "Asking LLM for range strategy for 20_new_zealand_demographics_1921_2011...\n",
      "Fetching ranges for Year by decade...\n",
      "Fetching ranges for Year by decade...\n",
      "[range_based]   ✓ Success - Saved to 20_new_zealand_demographics_1921_2011.json\n",
      "[range_based] [10/48] Processing 21_liechtenstein_demographics_1901_2011...\n",
      "Asking LLM for range strategy for 21_liechtenstein_demographics_1901_2011...\n",
      "[range_based]   ✓ Success - Saved to 20_new_zealand_demographics_1921_2011.json\n",
      "[range_based] [10/48] Processing 21_liechtenstein_demographics_1901_2011...\n",
      "Asking LLM for range strategy for 21_liechtenstein_demographics_1901_2011...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[range_based]   ✓ Success - Saved to 21_liechtenstein_demographics_1901_2011.json\n",
      "[range_based] [11/48] Processing 22_usa_demographics_1935_2010...\n",
      "Asking LLM for range strategy for 22_usa_demographics_1935_2010...\n",
      "[range_based]   ✓ Success - Saved to 21_liechtenstein_demographics_1901_2011.json\n",
      "[range_based] [11/48] Processing 22_usa_demographics_1935_2010...\n",
      "Asking LLM for range strategy for 22_usa_demographics_1935_2010...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[row_by_row]   ✓ Success - Saved to 22_usa_demographics_1935_2010.json\n",
      "[row_by_row] [12/48] Processing 23_andorra_demographics_1948_2012...\n",
      "Fetching keys for 23_andorra_demographics_1948_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 22_usa_demographics_1935_2010.json\n",
      "[row_by_row] [12/48] Processing 23_andorra_demographics_1948_2012...\n",
      "Fetching keys for 23_andorra_demographics_1948_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 23_andorra_demographics_1948_2012.json\n",
      "[row_by_row] [13/48] Processing 25_english_latin_rivalry_1887_2012...\n",
      "Fetching keys for 25_english_latin_rivalry_1887_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 23_andorra_demographics_1948_2012.json\n",
      "[row_by_row] [13/48] Processing 25_english_latin_rivalry_1887_2012...\n",
      "Fetching keys for 25_english_latin_rivalry_1887_2012...\n",
      "[range_based]   ✓ Success - Saved to 22_usa_demographics_1935_2010.json\n",
      "[range_based] [12/48] Processing 23_andorra_demographics_1948_2012...\n",
      "Asking LLM for range strategy for 23_andorra_demographics_1948_2012...\n",
      "[range_based]   ✓ Success - Saved to 22_usa_demographics_1935_2010.json\n",
      "[range_based] [12/48] Processing 23_andorra_demographics_1948_2012...\n",
      "Asking LLM for range strategy for 23_andorra_demographics_1948_2012...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[range_based]   ✓ Success - Saved to 23_andorra_demographics_1948_2012.json\n",
      "[range_based] [13/48] Processing 25_english_latin_rivalry_1887_2012...\n",
      "Asking LLM for range strategy for 25_english_latin_rivalry_1887_2012...\n",
      "[range_based]   ✓ Success - Saved to 23_andorra_demographics_1948_2012.json\n",
      "[range_based] [13/48] Processing 25_english_latin_rivalry_1887_2012...\n",
      "Asking LLM for range strategy for 25_english_latin_rivalry_1887_2012...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[range_based]   ✓ Success - Saved to 25_english_latin_rivalry_1887_2012.json\n",
      "[range_based] [14/48] Processing 28_equestrian_2012...\n",
      "Asking LLM for range strategy for 28_equestrian_2012...\n",
      "[range_based]   ✓ Success - Saved to 25_english_latin_rivalry_1887_2012.json\n",
      "[range_based] [14/48] Processing 28_equestrian_2012...\n",
      "Asking LLM for range strategy for 28_equestrian_2012...\n",
      "Fetching ranges for Rank by groups of 10 ranks...\n",
      "Fetching ranges for Rank by groups of 10 ranks...\n",
      "[range_based]   ✓ Success - Saved to 28_equestrian_2012.json\n",
      "[range_based] [15/48] Processing 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "Asking LLM for range strategy for 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "[range_based]   ✓ Success - Saved to 28_equestrian_2012.json\n",
      "[range_based] [15/48] Processing 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "Asking LLM for range strategy for 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 25_english_latin_rivalry_1887_2012.json\n",
      "[row_by_row] [14/48] Processing 28_equestrian_2012...\n",
      "Fetching keys for 28_equestrian_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 25_english_latin_rivalry_1887_2012.json\n",
      "[row_by_row] [14/48] Processing 28_equestrian_2012...\n",
      "Fetching keys for 28_equestrian_2012...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[range_based]   ✓ Success - Saved to 29_tennessee_vanderbilt_rivalry_1900_2012.json\n",
      "[range_based] [16/48] Processing 2_belgium_demographics_1900_2011...\n",
      "Asking LLM for range strategy for 2_belgium_demographics_1900_2011...\n",
      "[range_based]   ✓ Success - Saved to 29_tennessee_vanderbilt_rivalry_1900_2012.json\n",
      "[range_based] [16/48] Processing 2_belgium_demographics_1900_2011...\n",
      "Asking LLM for range strategy for 2_belgium_demographics_1900_2011...\n",
      "[row_by_row]   ✓ Success - Saved to 28_equestrian_2012.json\n",
      "[row_by_row] [15/48] Processing 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "Fetching keys for 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 28_equestrian_2012.json\n",
      "[row_by_row] [15/48] Processing 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "Fetching keys for 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "Fetching ranges for Year by decade...\n",
      "Fetching ranges for Year by decade...\n",
      "[range_based]   ✓ Success - Saved to 2_belgium_demographics_1900_2011.json\n",
      "[range_based] [17/48] Processing 30_classic_100_ten_years_on...\n",
      "Asking LLM for range strategy for 30_classic_100_ten_years_on...\n",
      "[range_based]   ✓ Success - Saved to 2_belgium_demographics_1900_2011.json\n",
      "[range_based] [17/48] Processing 30_classic_100_ten_years_on...\n",
      "Asking LLM for range strategy for 30_classic_100_ten_years_on...\n",
      "Fetching ranges for Rank by ranges of 10 (e.g., 1-10, 11-20, 21-30)...\n",
      "Fetching ranges for Rank by ranges of 10 (e.g., 1-10, 11-20, 21-30)...\n",
      "[range_based]   ✓ Success - Saved to 30_classic_100_ten_years_on.json\n",
      "[range_based] [18/48] Processing 31_udaykumar_films...\n",
      "Asking LLM for range strategy for 31_udaykumar_films...\n",
      "[range_based]   ✓ Success - Saved to 30_classic_100_ten_years_on.json\n",
      "[range_based] [18/48] Processing 31_udaykumar_films...\n",
      "Asking LLM for range strategy for 31_udaykumar_films...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[range_based]   ✓ Success - Saved to 31_udaykumar_films.json\n",
      "[range_based] [19/48] Processing 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "Asking LLM for range strategy for 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "[range_based]   ✓ Success - Saved to 31_udaykumar_films.json\n",
      "[range_based] [19/48] Processing 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "Asking LLM for range strategy for 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "Fetching ranges for Tie by round or stage of the qualifying rounds...\n",
      "Fetching ranges for Tie by round or stage of the qualifying rounds...\n",
      "[row_by_row]   ✓ Success - Saved to 29_tennessee_vanderbilt_rivalry_1900_2012.json\n",
      "[row_by_row] [16/48] Processing 2_belgium_demographics_1900_2011...\n",
      "Fetching keys for 2_belgium_demographics_1900_2011...\n",
      "[row_by_row]   ✓ Success - Saved to 29_tennessee_vanderbilt_rivalry_1900_2012.json\n",
      "[row_by_row] [16/48] Processing 2_belgium_demographics_1900_2011...\n",
      "Fetching keys for 2_belgium_demographics_1900_2011...\n",
      "[range_based]   ✓ Success - Saved to 32_fa_cup_qualifying_rounds_1999_2000.json\n",
      "[range_based] [20/48] Processing 33_portuguese_grape_varieties...\n",
      "Asking LLM for range strategy for 33_portuguese_grape_varieties...\n",
      "[range_based]   ✓ Success - Saved to 32_fa_cup_qualifying_rounds_1999_2000.json\n",
      "[range_based] [20/48] Processing 33_portuguese_grape_varieties...\n",
      "Asking LLM for range strategy for 33_portuguese_grape_varieties...\n",
      "Fetching ranges for Grape by alphabetical ranges (e.g., A-D, E-H, I-L, M-P, Q-T, U-Z)...\n",
      "Fetching ranges for Grape by alphabetical ranges (e.g., A-D, E-H, I-L, M-P, Q-T, U-Z)...\n",
      "[range_based]   ✓ Success - Saved to 33_portuguese_grape_varieties.json\n",
      "[range_based] [21/48] Processing 34_ramsar_convention_parties...\n",
      "Asking LLM for range strategy for 34_ramsar_convention_parties...\n",
      "[range_based]   ✓ Success - Saved to 33_portuguese_grape_varieties.json\n",
      "[range_based] [21/48] Processing 34_ramsar_convention_parties...\n",
      "Asking LLM for range strategy for 34_ramsar_convention_parties...\n",
      "Fetching ranges for Entry date by year...\n",
      "Fetching ranges for Entry date by year...\n",
      "[range_based]   ✓ Success - Saved to 34_ramsar_convention_parties.json\n",
      "[range_based] [22/48] Processing 35_guitar_hero_5_songs...\n",
      "Asking LLM for range strategy for 35_guitar_hero_5_songs...\n",
      "[range_based]   ✓ Success - Saved to 34_ramsar_convention_parties.json\n",
      "[range_based] [22/48] Processing 35_guitar_hero_5_songs...\n",
      "Asking LLM for range strategy for 35_guitar_hero_5_songs...\n",
      "Fetching ranges for Year by decade...\n",
      "Fetching ranges for Year by decade...\n",
      "[row_by_row]   ✓ Success - Saved to 2_belgium_demographics_1900_2011.json\n",
      "[row_by_row] [17/48] Processing 30_classic_100_ten_years_on...\n",
      "Fetching keys for 30_classic_100_ten_years_on...\n",
      "[row_by_row]   ✓ Success - Saved to 2_belgium_demographics_1900_2011.json\n",
      "[row_by_row] [17/48] Processing 30_classic_100_ten_years_on...\n",
      "Fetching keys for 30_classic_100_ten_years_on...\n",
      "[range_based]   ✓ Success - Saved to 35_guitar_hero_5_songs.json\n",
      "[range_based] [23/48] Processing 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Asking LLM for range strategy for 36_south_cambridgeshire_district_council_1973_2012...\n",
      "[range_based]   ✓ Success - Saved to 35_guitar_hero_5_songs.json\n",
      "[range_based] [23/48] Processing 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Asking LLM for range strategy for 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Fetching ranges for Year by decade...\n",
      "Fetching ranges for Year by decade...\n",
      "[row_by_row]   ✓ Success - Saved to 30_classic_100_ten_years_on.json\n",
      "[row_by_row] [18/48] Processing 31_udaykumar_films...\n",
      "Fetching keys for 31_udaykumar_films...\n",
      "[range_based]   ✓ Success - Saved to 36_south_cambridgeshire_district_council_1973_2012.json\n",
      "[range_based] [24/48] Processing 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "Asking LLM for range strategy for 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "[row_by_row]   ✓ Success - Saved to 30_classic_100_ten_years_on.json\n",
      "[row_by_row] [18/48] Processing 31_udaykumar_films...\n",
      "Fetching keys for 31_udaykumar_films...\n",
      "[range_based]   ✓ Success - Saved to 36_south_cambridgeshire_district_council_1973_2012.json\n",
      "[range_based] [24/48] Processing 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "Asking LLM for range strategy for 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[row_by_row]   ✓ Success - Saved to 31_udaykumar_films.json\n",
      "[row_by_row] [19/48] Processing 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "Fetching keys for 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "[range_based]   ✓ Success - Saved to 37_dublin_maternity_hospital_mortality_rates_1784_1849.json\n",
      "[range_based] [25/48] Processing 38_ship_launches_january_1944...\n",
      "Asking LLM for range strategy for 38_ship_launches_january_1944...\n",
      "[row_by_row]   ✓ Success - Saved to 31_udaykumar_films.json\n",
      "[row_by_row] [19/48] Processing 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "Fetching keys for 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "[range_based]   ✓ Success - Saved to 37_dublin_maternity_hospital_mortality_rates_1784_1849.json\n",
      "[range_based] [25/48] Processing 38_ship_launches_january_1944...\n",
      "Asking LLM for range strategy for 38_ship_launches_january_1944...\n",
      "Fetching ranges for Date by week in January 1944...\n",
      "Fetching ranges for Date by week in January 1944...\n",
      "[range_based]   ✓ Success - Saved to 38_ship_launches_january_1944.json\n",
      "[range_based] [26/48] Processing 39_uk_demographics_1960_2012...\n",
      "Asking LLM for range strategy for 39_uk_demographics_1960_2012...\n",
      "[range_based]   ✓ Success - Saved to 38_ship_launches_january_1944.json\n",
      "[range_based] [26/48] Processing 39_uk_demographics_1960_2012...\n",
      "Asking LLM for range strategy for 39_uk_demographics_1960_2012...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[row_by_row]   ✓ Success - Saved to 32_fa_cup_qualifying_rounds_1999_2000.json\n",
      "[row_by_row] [20/48] Processing 33_portuguese_grape_varieties...\n",
      "Fetching keys for 33_portuguese_grape_varieties...\n",
      "[row_by_row]   ✓ Success - Saved to 32_fa_cup_qualifying_rounds_1999_2000.json\n",
      "[row_by_row] [20/48] Processing 33_portuguese_grape_varieties...\n",
      "Fetching keys for 33_portuguese_grape_varieties...\n",
      "[range_based]   ✓ Success - Saved to 39_uk_demographics_1960_2012.json\n",
      "[range_based] [27/48] Processing 3_australia_demographics_1900_2010...\n",
      "Asking LLM for range strategy for 3_australia_demographics_1900_2010...\n",
      "[range_based]   ✓ Success - Saved to 39_uk_demographics_1960_2012.json\n",
      "[range_based] [27/48] Processing 3_australia_demographics_1900_2010...\n",
      "Asking LLM for range strategy for 3_australia_demographics_1900_2010...\n",
      "Fetching ranges for Year by decade...\n",
      "Fetching ranges for Year by decade...\n",
      "[row_by_row]   ✓ Success - Saved to 33_portuguese_grape_varieties.json\n",
      "[row_by_row] [21/48] Processing 34_ramsar_convention_parties...\n",
      "Fetching keys for 34_ramsar_convention_parties...\n",
      "[row_by_row]   ✓ Success - Saved to 33_portuguese_grape_varieties.json\n",
      "[row_by_row] [21/48] Processing 34_ramsar_convention_parties...\n",
      "Fetching keys for 34_ramsar_convention_parties...\n",
      "[range_based]   ✓ Success - Saved to 3_australia_demographics_1900_2010.json\n",
      "[range_based] [28/48] Processing 40_fukushima_plant_operating_history_1970_2009...\n",
      "Asking LLM for range strategy for 40_fukushima_plant_operating_history_1970_2009...\n",
      "[range_based]   ✓ Success - Saved to 3_australia_demographics_1900_2010.json\n",
      "[range_based] [28/48] Processing 40_fukushima_plant_operating_history_1970_2009...\n",
      "Asking LLM for range strategy for 40_fukushima_plant_operating_history_1970_2009...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[range_based]   ✓ Success - Saved to 40_fukushima_plant_operating_history_1970_2009.json\n",
      "[range_based] [29/48] Processing 41_new_zealand_football_results_1922_2012...\n",
      "Asking LLM for range strategy for 41_new_zealand_football_results_1922_2012...\n",
      "[range_based]   ✓ Success - Saved to 40_fukushima_plant_operating_history_1970_2009.json\n",
      "[range_based] [29/48] Processing 41_new_zealand_football_results_1922_2012...\n",
      "Asking LLM for range strategy for 41_new_zealand_football_results_1922_2012...\n",
      "Fetching ranges for year by decade...\n",
      "Fetching ranges for year by decade...\n",
      "[range_based]   ✓ Success - Saved to 41_new_zealand_football_results_1922_2012.json\n",
      "[range_based] [30/48] Processing 42_jack_nicklaus_achievements_1962_2005...\n",
      "Asking LLM for range strategy for 42_jack_nicklaus_achievements_1962_2005...\n",
      "[range_based]   ✓ Success - Saved to 41_new_zealand_football_results_1922_2012.json\n",
      "[range_based] [30/48] Processing 42_jack_nicklaus_achievements_1962_2005...\n",
      "Asking LLM for range strategy for 42_jack_nicklaus_achievements_1962_2005...\n",
      "Fetching ranges for Year by decade...\n",
      "Fetching ranges for Year by decade...\n",
      "[range_based]   ✓ Success - Saved to 42_jack_nicklaus_achievements_1962_2005.json\n",
      "[range_based] [31/48] Processing 47_european_countries_gdp_2007_2012...\n",
      "Asking LLM for range strategy for 47_european_countries_gdp_2007_2012...\n",
      "[range_based]   ✓ Success - Saved to 42_jack_nicklaus_achievements_1962_2005.json\n",
      "[range_based] [31/48] Processing 47_european_countries_gdp_2007_2012...\n",
      "Asking LLM for range strategy for 47_european_countries_gdp_2007_2012...\n",
      "Fetching ranges for 2012 Rank by ranges of 10 (e.g., 1-10, 11-20, 21-30)...\n",
      "Fetching ranges for 2012 Rank by ranges of 10 (e.g., 1-10, 11-20, 21-30)...\n",
      "[range_based]   ✓ Success - Saved to 47_european_countries_gdp_2007_2012.json\n",
      "[range_based] [32/48] Processing 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "Asking LLM for range strategy for 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "[range_based]   ✓ Success - Saved to 47_european_countries_gdp_2007_2012.json\n",
      "[range_based] [32/48] Processing 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "Asking LLM for range strategy for 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "Fetching ranges for HN Number by groups of 10 (e.g., HN4100-4109, HN4110-4119, etc.)...\n",
      "Fetching ranges for HN Number by groups of 10 (e.g., HN4100-4109, HN4110-4119, etc.)...\n",
      "[range_based]   ✓ Success - Saved to 48_royal_dulton_figurines_HN4100_HN4199.json\n",
      "[range_based] [33/48] Processing 49_adaalat_episodes_2012...\n",
      "Asking LLM for range strategy for 49_adaalat_episodes_2012...\n",
      "[range_based]   ✓ Success - Saved to 48_royal_dulton_figurines_HN4100_HN4199.json\n",
      "[range_based] [33/48] Processing 49_adaalat_episodes_2012...\n",
      "Asking LLM for range strategy for 49_adaalat_episodes_2012...\n",
      "Fetching ranges for Original Air Date by month...\n",
      "Fetching ranges for Original Air Date by month...\n",
      "[row_by_row]   ✓ Success - Saved to 34_ramsar_convention_parties.json\n",
      "[row_by_row] [22/48] Processing 35_guitar_hero_5_songs...\n",
      "Fetching keys for 35_guitar_hero_5_songs...\n",
      "[row_by_row]   ✓ Success - Saved to 34_ramsar_convention_parties.json\n",
      "[row_by_row] [22/48] Processing 35_guitar_hero_5_songs...\n",
      "Fetching keys for 35_guitar_hero_5_songs...\n",
      "[range_based]   ✓ Success - Saved to 49_adaalat_episodes_2012.json\n",
      "[range_based] [34/48] Processing 4_new_brunswick_parishes_2006_2011...\n",
      "Asking LLM for range strategy for 4_new_brunswick_parishes_2006_2011...\n",
      "[range_based]   ✓ Success - Saved to 49_adaalat_episodes_2012.json\n",
      "[range_based] [34/48] Processing 4_new_brunswick_parishes_2006_2011...\n",
      "Asking LLM for range strategy for 4_new_brunswick_parishes_2006_2011...\n",
      "Fetching ranges for Population (2011) by 500 or 1000 population ranges...\n",
      "Fetching ranges for Population (2011) by 500 or 1000 population ranges...\n",
      "[range_based]   ✓ Success - Saved to 4_new_brunswick_parishes_2006_2011.json\n",
      "[range_based] [35/48] Processing 51_just_dance_kids_2_tracks...\n",
      "Asking LLM for range strategy for 51_just_dance_kids_2_tracks...\n",
      "[range_based]   ✓ Success - Saved to 4_new_brunswick_parishes_2006_2011.json\n",
      "[range_based] [35/48] Processing 51_just_dance_kids_2_tracks...\n",
      "Asking LLM for range strategy for 51_just_dance_kids_2_tracks...\n",
      "Fetching ranges for Year by individual year or by 2-year ranges...\n",
      "Fetching ranges for Year by individual year or by 2-year ranges...\n",
      "[row_by_row]   ✓ Success - Saved to 35_guitar_hero_5_songs.json\n",
      "[row_by_row] [23/48] Processing 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Fetching keys for 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Failed to parse ranges for 51_just_dance_kids_2_tracks: Extra data: line 28 column 1 (char 844)\n",
      "[range_based]   ⚠ Attempt 1/3 failed (returned None), retrying...\n",
      "Asking LLM for range strategy for 51_just_dance_kids_2_tracks...\n",
      "[row_by_row]   ✓ Success - Saved to 35_guitar_hero_5_songs.json\n",
      "[row_by_row] [23/48] Processing 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Fetching keys for 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Failed to parse ranges for 51_just_dance_kids_2_tracks: Extra data: line 28 column 1 (char 844)\n",
      "[range_based]   ⚠ Attempt 1/3 failed (returned None), retrying...\n",
      "Asking LLM for range strategy for 51_just_dance_kids_2_tracks...\n",
      "Fetching ranges for Year by individual year or by 2-year ranges...\n",
      "Fetching ranges for Year by individual year or by 2-year ranges...\n",
      "[row_by_row]   ✓ Success - Saved to 36_south_cambridgeshire_district_council_1973_2012.json\n",
      "[row_by_row] [24/48] Processing 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "Fetching keys for 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "[row_by_row]   ✓ Success - Saved to 36_south_cambridgeshire_district_council_1973_2012.json\n",
      "[row_by_row] [24/48] Processing 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "Fetching keys for 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "[range_based]   ✓ Success (after 2 attempts) - Saved to 51_just_dance_kids_2_tracks.json\n",
      "[range_based] [36/48] Processing 52_cross_country_junior_women_1996...\n",
      "Asking LLM for range strategy for 52_cross_country_junior_women_1996...\n",
      "[range_based]   ✓ Success (after 2 attempts) - Saved to 51_just_dance_kids_2_tracks.json\n",
      "[range_based] [36/48] Processing 52_cross_country_junior_women_1996...\n",
      "Asking LLM for range strategy for 52_cross_country_junior_women_1996...\n",
      "Fetching ranges for Rank by ranges of 10 (e.g., 1-10, 11-20, 21-30)...\n",
      "Fetching ranges for Rank by ranges of 10 (e.g., 1-10, 11-20, 21-30)...\n",
      "[range_based]   ✓ Success - Saved to 52_cross_country_junior_women_1996.json\n",
      "[range_based] [37/48] Processing 53_metropolitan_opera_us_premieres...\n",
      "Asking LLM for range strategy for 53_metropolitan_opera_us_premieres...\n",
      "[range_based]   ✓ Success - Saved to 52_cross_country_junior_women_1996.json\n",
      "[range_based] [37/48] Processing 53_metropolitan_opera_us_premieres...\n",
      "Asking LLM for range strategy for 53_metropolitan_opera_us_premieres...\n",
      "Fetching ranges for Date of premiere by decade...\n",
      "Fetching ranges for Date of premiere by decade...\n",
      "[row_by_row]   ✓ Success - Saved to 37_dublin_maternity_hospital_mortality_rates_1784_1849.json\n",
      "[row_by_row] [25/48] Processing 38_ship_launches_january_1944...\n",
      "Fetching keys for 38_ship_launches_january_1944...\n",
      "[row_by_row]   ✓ Success - Saved to 37_dublin_maternity_hospital_mortality_rates_1784_1849.json\n",
      "[row_by_row] [25/48] Processing 38_ship_launches_january_1944...\n",
      "Fetching keys for 38_ship_launches_january_1944...\n",
      "[range_based]   ✓ Success - Saved to 53_metropolitan_opera_us_premieres.json\n",
      "[range_based] [38/48] Processing 54_kasparov_kramnik_1993_2004...\n",
      "Asking LLM for range strategy for 54_kasparov_kramnik_1993_2004...\n",
      "[range_based]   ✓ Success - Saved to 53_metropolitan_opera_us_premieres.json\n",
      "[range_based] [38/48] Processing 54_kasparov_kramnik_1993_2004...\n",
      "Asking LLM for range strategy for 54_kasparov_kramnik_1993_2004...\n",
      "Fetching ranges for year by individual year...\n",
      "Fetching ranges for year by individual year...\n",
      "[row_by_row]   ✓ Success - Saved to 38_ship_launches_january_1944.json\n",
      "[row_by_row] [26/48] Processing 39_uk_demographics_1960_2012...\n",
      "Fetching keys for 39_uk_demographics_1960_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 38_ship_launches_january_1944.json\n",
      "[row_by_row] [26/48] Processing 39_uk_demographics_1960_2012...\n",
      "Fetching keys for 39_uk_demographics_1960_2012...\n",
      "[range_based]   ✓ Success - Saved to 54_kasparov_kramnik_1993_2004.json\n",
      "[range_based] [39/48] Processing 55_decathlon_top50_1999...\n",
      "Asking LLM for range strategy for 55_decathlon_top50_1999...\n",
      "[range_based]   ✓ Success - Saved to 54_kasparov_kramnik_1993_2004.json\n",
      "[range_based] [39/48] Processing 55_decathlon_top50_1999...\n",
      "Asking LLM for range strategy for 55_decathlon_top50_1999...\n",
      "Fetching ranges for Points by 100-point ranges...\n",
      "Fetching ranges for Points by 100-point ranges...\n",
      "[range_based]   ✓ Success - Saved to 55_decathlon_top50_1999.json\n",
      "[range_based] [40/48] Processing 56_minor_planets_152601_152700...\n",
      "Asking LLM for range strategy for 56_minor_planets_152601_152700...\n",
      "[range_based]   ✓ Success - Saved to 55_decathlon_top50_1999.json\n",
      "[range_based] [40/48] Processing 56_minor_planets_152601_152700...\n",
      "Asking LLM for range strategy for 56_minor_planets_152601_152700...\n",
      "Fetching ranges for Name by ranges of 10 (e.g., 152601–152610, 152611–152620)...\n",
      "Fetching ranges for Name by ranges of 10 (e.g., 152601–152610, 152611–152620)...\n",
      "[row_by_row]   ✓ Success - Saved to 39_uk_demographics_1960_2012.json\n",
      "[row_by_row] [27/48] Processing 3_australia_demographics_1900_2010...\n",
      "Fetching keys for 3_australia_demographics_1900_2010...\n",
      "[row_by_row]   ✓ Success - Saved to 39_uk_demographics_1960_2012.json\n",
      "[row_by_row] [27/48] Processing 3_australia_demographics_1900_2010...\n",
      "Fetching keys for 3_australia_demographics_1900_2010...\n",
      "[row_by_row]   ✓ Success - Saved to 3_australia_demographics_1900_2010.json\n",
      "[row_by_row] [28/48] Processing 40_fukushima_plant_operating_history_1970_2009...\n",
      "Fetching keys for 40_fukushima_plant_operating_history_1970_2009...\n",
      "[range_based]   ✓ Success - Saved to 56_minor_planets_152601_152700.json\n",
      "[range_based] [41/48] Processing 59_miss_new_york_usa_delegates_2012...\n",
      "Asking LLM for range strategy for 59_miss_new_york_usa_delegates_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 3_australia_demographics_1900_2010.json\n",
      "[row_by_row] [28/48] Processing 40_fukushima_plant_operating_history_1970_2009...\n",
      "Fetching keys for 40_fukushima_plant_operating_history_1970_2009...\n",
      "[range_based]   ✓ Success - Saved to 56_minor_planets_152601_152700.json\n",
      "[range_based] [41/48] Processing 59_miss_new_york_usa_delegates_2012...\n",
      "Asking LLM for range strategy for 59_miss_new_york_usa_delegates_2012...\n",
      "Fetching ranges for Candidate alphabetically by first letter of last name...\n",
      "Fetching ranges for Candidate alphabetically by first letter of last name...\n",
      "[row_by_row]   ✓ Success - Saved to 40_fukushima_plant_operating_history_1970_2009.json\n",
      "[row_by_row] [29/48] Processing 41_new_zealand_football_results_1922_2012...\n",
      "Fetching keys for 41_new_zealand_football_results_1922_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 40_fukushima_plant_operating_history_1970_2009.json\n",
      "[row_by_row] [29/48] Processing 41_new_zealand_football_results_1922_2012...\n",
      "Fetching keys for 41_new_zealand_football_results_1922_2012...\n",
      "[range_based]   ✓ Success - Saved to 59_miss_new_york_usa_delegates_2012.json\n",
      "[range_based] [42/48] Processing 5_ice_hockey_2006...\n",
      "Asking LLM for range strategy for 5_ice_hockey_2006...\n",
      "[range_based]   ✓ Success - Saved to 59_miss_new_york_usa_delegates_2012.json\n",
      "[range_based] [42/48] Processing 5_ice_hockey_2006...\n",
      "Asking LLM for range strategy for 5_ice_hockey_2006...\n",
      "Fetching ranges for Pts by 5-point ranges...\n",
      "Fetching ranges for Pts by 5-point ranges...\n",
      "[range_based]   ✓ Success - Saved to 5_ice_hockey_2006.json\n",
      "[range_based] [43/48] Processing 67_london_heathrow_busiest_routes_2012...\n",
      "Asking LLM for range strategy for 67_london_heathrow_busiest_routes_2012...\n",
      "[range_based]   ✓ Success - Saved to 5_ice_hockey_2006.json\n",
      "[range_based] [43/48] Processing 67_london_heathrow_busiest_routes_2012...\n",
      "Asking LLM for range strategy for 67_london_heathrow_busiest_routes_2012...\n",
      "Fetching ranges for Rank by ranges of 10 (e.g., 1-10, 11-20, 21-30)...\n",
      "Fetching ranges for Rank by ranges of 10 (e.g., 1-10, 11-20, 21-30)...\n",
      "[range_based]   ✓ Success - Saved to 67_london_heathrow_busiest_routes_2012.json\n",
      "[range_based] [44/48] Processing 71_us_president_elections_idaho_2008...\n",
      "Asking LLM for range strategy for 71_us_president_elections_idaho_2008...\n",
      "[range_based]   ✓ Success - Saved to 67_london_heathrow_busiest_routes_2012.json\n",
      "[range_based] [44/48] Processing 71_us_president_elections_idaho_2008...\n",
      "Asking LLM for range strategy for 71_us_president_elections_idaho_2008...\n",
      "Fetching ranges for Obama% by 10% ranges...\n",
      "Fetching ranges for Obama% by 10% ranges...\n",
      "[range_based]   ✓ Success - Saved to 71_us_president_elections_idaho_2008.json\n",
      "[range_based] [45/48] Processing 78_bafta_best_actor_leading_role_2000s...\n",
      "Asking LLM for range strategy for 78_bafta_best_actor_leading_role_2000s...\n",
      "[range_based]   ✓ Success - Saved to 71_us_president_elections_idaho_2008.json\n",
      "[range_based] [45/48] Processing 78_bafta_best_actor_leading_role_2000s...\n",
      "Asking LLM for range strategy for 78_bafta_best_actor_leading_role_2000s...\n",
      "Fetching ranges for year by individual year...\n",
      "Fetching ranges for year by individual year...\n",
      "[row_by_row]   ✓ Success - Saved to 41_new_zealand_football_results_1922_2012.json\n",
      "[row_by_row] [30/48] Processing 42_jack_nicklaus_achievements_1962_2005...\n",
      "Fetching keys for 42_jack_nicklaus_achievements_1962_2005...\n",
      "[row_by_row]   ✓ Success - Saved to 41_new_zealand_football_results_1922_2012.json\n",
      "[row_by_row] [30/48] Processing 42_jack_nicklaus_achievements_1962_2005...\n",
      "Fetching keys for 42_jack_nicklaus_achievements_1962_2005...\n",
      "[range_based]   ✓ Success - Saved to 78_bafta_best_actor_leading_role_2000s.json\n",
      "[range_based] [46/48] Processing 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "Asking LLM for range strategy for 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "[range_based]   ✓ Success - Saved to 78_bafta_best_actor_leading_role_2000s.json\n",
      "[range_based] [46/48] Processing 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "Asking LLM for range strategy for 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "Fetching ranges for Draft by individual year...\n",
      "Fetching ranges for Draft by individual year...\n",
      "[row_by_row]   ✓ Success - Saved to 42_jack_nicklaus_achievements_1962_2005.json\n",
      "[row_by_row] [31/48] Processing 47_european_countries_gdp_2007_2012...\n",
      "Fetching keys for 47_european_countries_gdp_2007_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 42_jack_nicklaus_achievements_1962_2005.json\n",
      "[row_by_row] [31/48] Processing 47_european_countries_gdp_2007_2012...\n",
      "Fetching keys for 47_european_countries_gdp_2007_2012...\n",
      "[range_based]   ✓ Success - Saved to 7_anaheim_ducks_draft_picks_1998_2013.json\n",
      "[range_based] [47/48] Processing 8_south_african_class_15f_4_8_2...\n",
      "Asking LLM for range strategy for 8_south_african_class_15f_4_8_2...\n",
      "[range_based]   ✓ Success - Saved to 7_anaheim_ducks_draft_picks_1998_2013.json\n",
      "[range_based] [47/48] Processing 8_south_african_class_15f_4_8_2...\n",
      "Asking LLM for range strategy for 8_south_african_class_15f_4_8_2...\n",
      "Fetching ranges for SAR No. by ranges of 100 (e.g., 2900-2999, 3000-3099)...\n",
      "Fetching ranges for SAR No. by ranges of 100 (e.g., 2900-2999, 3000-3099)...\n",
      "[range_based]   ✓ Success - Saved to 8_south_african_class_15f_4_8_2.json\n",
      "[range_based] [48/48] Processing 9_tour_de_france_2009...\n",
      "Asking LLM for range strategy for 9_tour_de_france_2009...\n",
      "[range_based]   ✓ Success - Saved to 8_south_african_class_15f_4_8_2.json\n",
      "[range_based] [48/48] Processing 9_tour_de_france_2009...\n",
      "Asking LLM for range strategy for 9_tour_de_france_2009...\n",
      "Fetching ranges for Age by 5-year ranges...\n",
      "Fetching ranges for Age by 5-year ranges...\n",
      "[range_based]   ✓ Success - Saved to 9_tour_de_france_2009.json\n",
      "\n",
      "[range_based] Completed: 48 successes, 0 errors\n",
      "[range_based]   ✓ Success - Saved to 9_tour_de_france_2009.json\n",
      "\n",
      "[range_based] Completed: 48 successes, 0 errors\n",
      "[row_by_row]   ✓ Success - Saved to 47_european_countries_gdp_2007_2012.json\n",
      "[row_by_row] [32/48] Processing 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "Fetching keys for 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "[row_by_row]   ✓ Success - Saved to 47_european_countries_gdp_2007_2012.json\n",
      "[row_by_row] [32/48] Processing 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "Fetching keys for 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "LLM call failed: Expecting value: line 927 column 1 (char 5093)\n",
      "Failed to fetch values for 17_scottish_football_transfers_summer_2011\n",
      "[attribute_based]   ⚠ Attempt 1/3 failed (returned None), retrying...\n",
      "Asking LLM for partition column for 17_scottish_football_transfers_summer_2011...\n",
      "LLM call failed: Expecting value: line 927 column 1 (char 5093)\n",
      "Failed to fetch values for 17_scottish_football_transfers_summer_2011\n",
      "[attribute_based]   ⚠ Attempt 1/3 failed (returned None), retrying...\n",
      "Asking LLM for partition column for 17_scottish_football_transfers_summer_2011...\n",
      "Fetching distinct values for column 'Moving from'...\n",
      "Fetching distinct values for column 'Moving from'...\n",
      "[row_by_row]   ✓ Success - Saved to 48_royal_dulton_figurines_HN4100_HN4199.json\n",
      "[row_by_row] [33/48] Processing 49_adaalat_episodes_2012...\n",
      "Fetching keys for 49_adaalat_episodes_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 48_royal_dulton_figurines_HN4100_HN4199.json\n",
      "[row_by_row] [33/48] Processing 49_adaalat_episodes_2012...\n",
      "Fetching keys for 49_adaalat_episodes_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 49_adaalat_episodes_2012.json\n",
      "[row_by_row] [34/48] Processing 4_new_brunswick_parishes_2006_2011...\n",
      "Fetching keys for 4_new_brunswick_parishes_2006_2011...\n",
      "[row_by_row]   ✓ Success - Saved to 49_adaalat_episodes_2012.json\n",
      "[row_by_row] [34/48] Processing 4_new_brunswick_parishes_2006_2011...\n",
      "Fetching keys for 4_new_brunswick_parishes_2006_2011...\n",
      "[row_by_row]   ✓ Success - Saved to 4_new_brunswick_parishes_2006_2011.json\n",
      "[row_by_row] [35/48] Processing 51_just_dance_kids_2_tracks...\n",
      "Fetching keys for 51_just_dance_kids_2_tracks...\n",
      "[row_by_row]   ✓ Success - Saved to 4_new_brunswick_parishes_2006_2011.json\n",
      "[row_by_row] [35/48] Processing 51_just_dance_kids_2_tracks...\n",
      "Fetching keys for 51_just_dance_kids_2_tracks...\n",
      "[row_by_row]   ✓ Success - Saved to 51_just_dance_kids_2_tracks.json\n",
      "[row_by_row] [36/48] Processing 52_cross_country_junior_women_1996...\n",
      "Fetching keys for 52_cross_country_junior_women_1996...\n",
      "[row_by_row]   ✓ Success - Saved to 51_just_dance_kids_2_tracks.json\n",
      "[row_by_row] [36/48] Processing 52_cross_country_junior_women_1996...\n",
      "Fetching keys for 52_cross_country_junior_women_1996...\n",
      "[row_by_row]   ✓ Success - Saved to 52_cross_country_junior_women_1996.json\n",
      "[row_by_row] [37/48] Processing 53_metropolitan_opera_us_premieres...\n",
      "Fetching keys for 53_metropolitan_opera_us_premieres...\n",
      "[row_by_row]   ✓ Success - Saved to 52_cross_country_junior_women_1996.json\n",
      "[row_by_row] [37/48] Processing 53_metropolitan_opera_us_premieres...\n",
      "Fetching keys for 53_metropolitan_opera_us_premieres...\n",
      "[row_by_row]   ✓ Success - Saved to 53_metropolitan_opera_us_premieres.json\n",
      "[row_by_row] [38/48] Processing 54_kasparov_kramnik_1993_2004...\n",
      "Fetching keys for 54_kasparov_kramnik_1993_2004...\n",
      "[row_by_row]   ✓ Success - Saved to 53_metropolitan_opera_us_premieres.json\n",
      "[row_by_row] [38/48] Processing 54_kasparov_kramnik_1993_2004...\n",
      "Fetching keys for 54_kasparov_kramnik_1993_2004...\n",
      "[row_by_row]   ✓ Success - Saved to 54_kasparov_kramnik_1993_2004.json\n",
      "[row_by_row] [39/48] Processing 55_decathlon_top50_1999...\n",
      "Fetching keys for 55_decathlon_top50_1999...\n",
      "[row_by_row]   ✓ Success - Saved to 54_kasparov_kramnik_1993_2004.json\n",
      "[row_by_row] [39/48] Processing 55_decathlon_top50_1999...\n",
      "Fetching keys for 55_decathlon_top50_1999...\n",
      "[row_by_row]   ✓ Success - Saved to 55_decathlon_top50_1999.json\n",
      "[row_by_row] [40/48] Processing 56_minor_planets_152601_152700...\n",
      "Fetching keys for 56_minor_planets_152601_152700...\n",
      "[row_by_row]   ✓ Success - Saved to 55_decathlon_top50_1999.json\n",
      "[row_by_row] [40/48] Processing 56_minor_planets_152601_152700...\n",
      "Fetching keys for 56_minor_planets_152601_152700...\n",
      "[row_by_row]   ✓ Success - Saved to 56_minor_planets_152601_152700.json\n",
      "[row_by_row] [41/48] Processing 59_miss_new_york_usa_delegates_2012...\n",
      "Fetching keys for 59_miss_new_york_usa_delegates_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 56_minor_planets_152601_152700.json\n",
      "[row_by_row] [41/48] Processing 59_miss_new_york_usa_delegates_2012...\n",
      "Fetching keys for 59_miss_new_york_usa_delegates_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 59_miss_new_york_usa_delegates_2012.json\n",
      "[row_by_row] [42/48] Processing 5_ice_hockey_2006...\n",
      "Fetching keys for 5_ice_hockey_2006...\n",
      "[row_by_row]   ✓ Success - Saved to 59_miss_new_york_usa_delegates_2012.json\n",
      "[row_by_row] [42/48] Processing 5_ice_hockey_2006...\n",
      "Fetching keys for 5_ice_hockey_2006...\n",
      "[row_by_row]   ✓ Success - Saved to 5_ice_hockey_2006.json\n",
      "[row_by_row] [43/48] Processing 67_london_heathrow_busiest_routes_2012...\n",
      "Fetching keys for 67_london_heathrow_busiest_routes_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 5_ice_hockey_2006.json\n",
      "[row_by_row] [43/48] Processing 67_london_heathrow_busiest_routes_2012...\n",
      "Fetching keys for 67_london_heathrow_busiest_routes_2012...\n",
      "[row_by_row]   ✓ Success - Saved to 67_london_heathrow_busiest_routes_2012.json\n",
      "[row_by_row] [44/48] Processing 71_us_president_elections_idaho_2008...\n",
      "Fetching keys for 71_us_president_elections_idaho_2008...\n",
      "[row_by_row]   ✓ Success - Saved to 67_london_heathrow_busiest_routes_2012.json\n",
      "[row_by_row] [44/48] Processing 71_us_president_elections_idaho_2008...\n",
      "Fetching keys for 71_us_president_elections_idaho_2008...\n",
      "[row_by_row]   ✓ Success - Saved to 71_us_president_elections_idaho_2008.json\n",
      "[row_by_row] [45/48] Processing 78_bafta_best_actor_leading_role_2000s...\n",
      "Fetching keys for 78_bafta_best_actor_leading_role_2000s...\n",
      "[row_by_row]   ✓ Success - Saved to 71_us_president_elections_idaho_2008.json\n",
      "[row_by_row] [45/48] Processing 78_bafta_best_actor_leading_role_2000s...\n",
      "Fetching keys for 78_bafta_best_actor_leading_role_2000s...\n",
      "[row_by_row]   ✓ Success - Saved to 78_bafta_best_actor_leading_role_2000s.json\n",
      "[row_by_row] [46/48] Processing 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "Fetching keys for 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "[row_by_row]   ✓ Success - Saved to 78_bafta_best_actor_leading_role_2000s.json\n",
      "[row_by_row] [46/48] Processing 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "Fetching keys for 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "[row_by_row]   ✓ Success - Saved to 7_anaheim_ducks_draft_picks_1998_2013.json\n",
      "[row_by_row] [47/48] Processing 8_south_african_class_15f_4_8_2...\n",
      "Fetching keys for 8_south_african_class_15f_4_8_2...\n",
      "[row_by_row]   ✓ Success - Saved to 7_anaheim_ducks_draft_picks_1998_2013.json\n",
      "[row_by_row] [47/48] Processing 8_south_african_class_15f_4_8_2...\n",
      "Fetching keys for 8_south_african_class_15f_4_8_2...\n",
      "[row_by_row]   ✓ Success - Saved to 8_south_african_class_15f_4_8_2.json\n",
      "[row_by_row] [48/48] Processing 9_tour_de_france_2009...\n",
      "Fetching keys for 9_tour_de_france_2009...\n",
      "[row_by_row]   ✓ Success - Saved to 8_south_african_class_15f_4_8_2.json\n",
      "[row_by_row] [48/48] Processing 9_tour_de_france_2009...\n",
      "Fetching keys for 9_tour_de_france_2009...\n",
      "[row_by_row]   ✓ Success - Saved to 9_tour_de_france_2009.json\n",
      "\n",
      "[row_by_row] Completed: 48 successes, 0 errors\n",
      "[row_by_row]   ✓ Success - Saved to 9_tour_de_france_2009.json\n",
      "\n",
      "[row_by_row] Completed: 48 successes, 0 errors\n",
      "LLM call failed: Expecting value: line 1045 column 1 (char 5742)\n",
      "Failed to fetch values for 17_scottish_football_transfers_summer_2011\n",
      "[attribute_based]   ⚠ Attempt 2/3 failed (returned None), retrying...\n",
      "Asking LLM for partition column for 17_scottish_football_transfers_summer_2011...\n",
      "LLM call failed: Expecting value: line 1045 column 1 (char 5742)\n",
      "Failed to fetch values for 17_scottish_football_transfers_summer_2011\n",
      "[attribute_based]   ⚠ Attempt 2/3 failed (returned None), retrying...\n",
      "Asking LLM for partition column for 17_scottish_football_transfers_summer_2011...\n",
      "Fetching distinct values for column 'Moving from'...\n",
      "Fetching distinct values for column 'Moving from'...\n",
      "LLM call failed: Expecting value: line 995 column 1 (char 5467)\n",
      "Failed to fetch values for 17_scottish_football_transfers_summer_2011\n",
      "[attribute_based]   ✗ Failed after 3 attempts: Function returned None\n",
      "[attribute_based] [8/48] Processing 19_living_proof_the_farewell_tour...\n",
      "Asking LLM for partition column for 19_living_proof_the_farewell_tour...\n",
      "LLM call failed: Expecting value: line 995 column 1 (char 5467)\n",
      "Failed to fetch values for 17_scottish_football_transfers_summer_2011\n",
      "[attribute_based]   ✗ Failed after 3 attempts: Function returned None\n",
      "[attribute_based] [8/48] Processing 19_living_proof_the_farewell_tour...\n",
      "Asking LLM for partition column for 19_living_proof_the_farewell_tour...\n",
      "Fetching distinct values for column 'Date'...\n",
      "Fetching distinct values for column 'Date'...\n",
      "[attribute_based]   ✓ Success - Saved to 19_living_proof_the_farewell_tour.json\n",
      "[attribute_based] [9/48] Processing 20_new_zealand_demographics_1921_2011...\n",
      "Asking LLM for partition column for 20_new_zealand_demographics_1921_2011...\n",
      "[attribute_based]   ✓ Success - Saved to 19_living_proof_the_farewell_tour.json\n",
      "[attribute_based] [9/48] Processing 20_new_zealand_demographics_1921_2011...\n",
      "Asking LLM for partition column for 20_new_zealand_demographics_1921_2011...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 20_new_zealand_demographics_1921_2011.json\n",
      "[attribute_based] [10/48] Processing 21_liechtenstein_demographics_1901_2011...\n",
      "Asking LLM for partition column for 21_liechtenstein_demographics_1901_2011...\n",
      "[attribute_based]   ✓ Success - Saved to 20_new_zealand_demographics_1921_2011.json\n",
      "[attribute_based] [10/48] Processing 21_liechtenstein_demographics_1901_2011...\n",
      "Asking LLM for partition column for 21_liechtenstein_demographics_1901_2011...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 21_liechtenstein_demographics_1901_2011.json\n",
      "[attribute_based] [11/48] Processing 22_usa_demographics_1935_2010...\n",
      "Asking LLM for partition column for 22_usa_demographics_1935_2010...\n",
      "[attribute_based]   ✓ Success - Saved to 21_liechtenstein_demographics_1901_2011.json\n",
      "[attribute_based] [11/48] Processing 22_usa_demographics_1935_2010...\n",
      "Asking LLM for partition column for 22_usa_demographics_1935_2010...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 22_usa_demographics_1935_2010.json\n",
      "[attribute_based] [12/48] Processing 23_andorra_demographics_1948_2012...\n",
      "Asking LLM for partition column for 23_andorra_demographics_1948_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 22_usa_demographics_1935_2010.json\n",
      "[attribute_based] [12/48] Processing 23_andorra_demographics_1948_2012...\n",
      "Asking LLM for partition column for 23_andorra_demographics_1948_2012...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 23_andorra_demographics_1948_2012.json\n",
      "[attribute_based] [13/48] Processing 25_english_latin_rivalry_1887_2012...\n",
      "Asking LLM for partition column for 25_english_latin_rivalry_1887_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 23_andorra_demographics_1948_2012.json\n",
      "[attribute_based] [13/48] Processing 25_english_latin_rivalry_1887_2012...\n",
      "Asking LLM for partition column for 25_english_latin_rivalry_1887_2012...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 25_english_latin_rivalry_1887_2012.json\n",
      "[attribute_based] [14/48] Processing 28_equestrian_2012...\n",
      "Asking LLM for partition column for 28_equestrian_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 25_english_latin_rivalry_1887_2012.json\n",
      "[attribute_based] [14/48] Processing 28_equestrian_2012...\n",
      "Asking LLM for partition column for 28_equestrian_2012...\n",
      "Fetching distinct values for column 'Nation'...\n",
      "Fetching distinct values for column 'Nation'...\n",
      "[attribute_based]   ✓ Success - Saved to 28_equestrian_2012.json\n",
      "[attribute_based] [15/48] Processing 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "Asking LLM for partition column for 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 28_equestrian_2012.json\n",
      "[attribute_based] [15/48] Processing 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "Asking LLM for partition column for 29_tennessee_vanderbilt_rivalry_1900_2012...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 29_tennessee_vanderbilt_rivalry_1900_2012.json\n",
      "[attribute_based] [16/48] Processing 2_belgium_demographics_1900_2011...\n",
      "Asking LLM for partition column for 2_belgium_demographics_1900_2011...\n",
      "[attribute_based]   ✓ Success - Saved to 29_tennessee_vanderbilt_rivalry_1900_2012.json\n",
      "[attribute_based] [16/48] Processing 2_belgium_demographics_1900_2011...\n",
      "Asking LLM for partition column for 2_belgium_demographics_1900_2011...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 2_belgium_demographics_1900_2011.json\n",
      "[attribute_based] [17/48] Processing 30_classic_100_ten_years_on...\n",
      "Asking LLM for partition column for 30_classic_100_ten_years_on...\n",
      "[attribute_based]   ✓ Success - Saved to 2_belgium_demographics_1900_2011.json\n",
      "[attribute_based] [17/48] Processing 30_classic_100_ten_years_on...\n",
      "Asking LLM for partition column for 30_classic_100_ten_years_on...\n",
      "Fetching distinct values for column 'Composer'...\n",
      "Fetching distinct values for column 'Composer'...\n",
      "[attribute_based]   ✓ Success - Saved to 30_classic_100_ten_years_on.json\n",
      "[attribute_based] [18/48] Processing 31_udaykumar_films...\n",
      "Asking LLM for partition column for 31_udaykumar_films...\n",
      "[attribute_based]   ✓ Success - Saved to 30_classic_100_ten_years_on.json\n",
      "[attribute_based] [18/48] Processing 31_udaykumar_films...\n",
      "Asking LLM for partition column for 31_udaykumar_films...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 31_udaykumar_films.json\n",
      "[attribute_based] [19/48] Processing 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "Asking LLM for partition column for 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "[attribute_based]   ✓ Success - Saved to 31_udaykumar_films.json\n",
      "[attribute_based] [19/48] Processing 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "Asking LLM for partition column for 32_fa_cup_qualifying_rounds_1999_2000...\n",
      "Fetching distinct values for column 'Tie'...\n",
      "Fetching distinct values for column 'Tie'...\n",
      "[attribute_based]   ✓ Success - Saved to 32_fa_cup_qualifying_rounds_1999_2000.json\n",
      "[attribute_based] [20/48] Processing 33_portuguese_grape_varieties...\n",
      "Asking LLM for partition column for 33_portuguese_grape_varieties...\n",
      "[attribute_based]   ✓ Success - Saved to 32_fa_cup_qualifying_rounds_1999_2000.json\n",
      "[attribute_based] [20/48] Processing 33_portuguese_grape_varieties...\n",
      "Asking LLM for partition column for 33_portuguese_grape_varieties...\n",
      "Fetching distinct values for column 'Color'...\n",
      "Fetching distinct values for column 'Color'...\n",
      "[attribute_based]   ✓ Success - Saved to 33_portuguese_grape_varieties.json\n",
      "[attribute_based] [21/48] Processing 34_ramsar_convention_parties...\n",
      "Asking LLM for partition column for 34_ramsar_convention_parties...\n",
      "[attribute_based]   ✓ Success - Saved to 33_portuguese_grape_varieties.json\n",
      "[attribute_based] [21/48] Processing 34_ramsar_convention_parties...\n",
      "Asking LLM for partition column for 34_ramsar_convention_parties...\n",
      "Fetching distinct values for column 'Entry date'...\n",
      "Fetching distinct values for column 'Entry date'...\n",
      "[attribute_based]   ✓ Success - Saved to 34_ramsar_convention_parties.json\n",
      "[attribute_based] [22/48] Processing 35_guitar_hero_5_songs...\n",
      "Asking LLM for partition column for 35_guitar_hero_5_songs...\n",
      "[attribute_based]   ✓ Success - Saved to 34_ramsar_convention_parties.json\n",
      "[attribute_based] [22/48] Processing 35_guitar_hero_5_songs...\n",
      "Asking LLM for partition column for 35_guitar_hero_5_songs...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 35_guitar_hero_5_songs.json\n",
      "[attribute_based] [23/48] Processing 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Asking LLM for partition column for 36_south_cambridgeshire_district_council_1973_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 35_guitar_hero_5_songs.json\n",
      "[attribute_based] [23/48] Processing 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Asking LLM for partition column for 36_south_cambridgeshire_district_council_1973_2012...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 36_south_cambridgeshire_district_council_1973_2012.json\n",
      "[attribute_based] [24/48] Processing 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "Asking LLM for partition column for 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "[attribute_based]   ✓ Success - Saved to 36_south_cambridgeshire_district_council_1973_2012.json\n",
      "[attribute_based] [24/48] Processing 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "Asking LLM for partition column for 37_dublin_maternity_hospital_mortality_rates_1784_1849...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 37_dublin_maternity_hospital_mortality_rates_1784_1849.json\n",
      "[attribute_based] [25/48] Processing 38_ship_launches_january_1944...\n",
      "Asking LLM for partition column for 38_ship_launches_january_1944...\n",
      "[attribute_based]   ✓ Success - Saved to 37_dublin_maternity_hospital_mortality_rates_1784_1849.json\n",
      "[attribute_based] [25/48] Processing 38_ship_launches_january_1944...\n",
      "Asking LLM for partition column for 38_ship_launches_january_1944...\n",
      "Fetching distinct values for column 'Date'...\n",
      "Fetching distinct values for column 'Date'...\n",
      "[attribute_based]   ✓ Success - Saved to 38_ship_launches_january_1944.json\n",
      "[attribute_based] [26/48] Processing 39_uk_demographics_1960_2012...\n",
      "Asking LLM for partition column for 39_uk_demographics_1960_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 38_ship_launches_january_1944.json\n",
      "[attribute_based] [26/48] Processing 39_uk_demographics_1960_2012...\n",
      "Asking LLM for partition column for 39_uk_demographics_1960_2012...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 39_uk_demographics_1960_2012.json\n",
      "[attribute_based] [27/48] Processing 3_australia_demographics_1900_2010...\n",
      "Asking LLM for partition column for 3_australia_demographics_1900_2010...\n",
      "[attribute_based]   ✓ Success - Saved to 39_uk_demographics_1960_2012.json\n",
      "[attribute_based] [27/48] Processing 3_australia_demographics_1900_2010...\n",
      "Asking LLM for partition column for 3_australia_demographics_1900_2010...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 3_australia_demographics_1900_2010.json\n",
      "[attribute_based] [28/48] Processing 40_fukushima_plant_operating_history_1970_2009...\n",
      "Asking LLM for partition column for 40_fukushima_plant_operating_history_1970_2009...\n",
      "[attribute_based]   ✓ Success - Saved to 3_australia_demographics_1900_2010.json\n",
      "[attribute_based] [28/48] Processing 40_fukushima_plant_operating_history_1970_2009...\n",
      "Asking LLM for partition column for 40_fukushima_plant_operating_history_1970_2009...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 40_fukushima_plant_operating_history_1970_2009.json\n",
      "[attribute_based] [29/48] Processing 41_new_zealand_football_results_1922_2012...\n",
      "Asking LLM for partition column for 41_new_zealand_football_results_1922_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 40_fukushima_plant_operating_history_1970_2009.json\n",
      "[attribute_based] [29/48] Processing 41_new_zealand_football_results_1922_2012...\n",
      "Asking LLM for partition column for 41_new_zealand_football_results_1922_2012...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 41_new_zealand_football_results_1922_2012.json\n",
      "[attribute_based] [30/48] Processing 42_jack_nicklaus_achievements_1962_2005...\n",
      "Asking LLM for partition column for 42_jack_nicklaus_achievements_1962_2005...\n",
      "[attribute_based]   ✓ Success - Saved to 41_new_zealand_football_results_1922_2012.json\n",
      "[attribute_based] [30/48] Processing 42_jack_nicklaus_achievements_1962_2005...\n",
      "Asking LLM for partition column for 42_jack_nicklaus_achievements_1962_2005...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 42_jack_nicklaus_achievements_1962_2005.json\n",
      "[attribute_based] [31/48] Processing 47_european_countries_gdp_2007_2012...\n",
      "Asking LLM for partition column for 47_european_countries_gdp_2007_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 42_jack_nicklaus_achievements_1962_2005.json\n",
      "[attribute_based] [31/48] Processing 47_european_countries_gdp_2007_2012...\n",
      "Asking LLM for partition column for 47_european_countries_gdp_2007_2012...\n",
      "Fetching distinct values for column '2012 Rank'...\n",
      "Fetching distinct values for column '2012 Rank'...\n",
      "[attribute_based]   ✓ Success - Saved to 47_european_countries_gdp_2007_2012.json\n",
      "[attribute_based] [32/48] Processing 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "Asking LLM for partition column for 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "[attribute_based]   ✓ Success - Saved to 47_european_countries_gdp_2007_2012.json\n",
      "[attribute_based] [32/48] Processing 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "Asking LLM for partition column for 48_royal_dulton_figurines_HN4100_HN4199...\n",
      "Fetching distinct values for column 'HN Number'...\n",
      "Fetching distinct values for column 'HN Number'...\n",
      "[attribute_based]   ✓ Success - Saved to 48_royal_dulton_figurines_HN4100_HN4199.json\n",
      "[attribute_based] [33/48] Processing 49_adaalat_episodes_2012...\n",
      "Asking LLM for partition column for 49_adaalat_episodes_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 48_royal_dulton_figurines_HN4100_HN4199.json\n",
      "[attribute_based] [33/48] Processing 49_adaalat_episodes_2012...\n",
      "Asking LLM for partition column for 49_adaalat_episodes_2012...\n",
      "Fetching distinct values for column 'Year Case No'...\n",
      "Fetching distinct values for column 'Year Case No'...\n",
      "[attribute_based]   ✓ Success - Saved to 49_adaalat_episodes_2012.json\n",
      "[attribute_based] [34/48] Processing 4_new_brunswick_parishes_2006_2011...\n",
      "Asking LLM for partition column for 4_new_brunswick_parishes_2006_2011...\n",
      "[attribute_based]   ✓ Success - Saved to 49_adaalat_episodes_2012.json\n",
      "[attribute_based] [34/48] Processing 4_new_brunswick_parishes_2006_2011...\n",
      "Asking LLM for partition column for 4_new_brunswick_parishes_2006_2011...\n",
      "Fetching distinct values for column 'County'...\n",
      "Fetching distinct values for column 'County'...\n",
      "[attribute_based]   ✓ Success - Saved to 4_new_brunswick_parishes_2006_2011.json\n",
      "[attribute_based] [35/48] Processing 51_just_dance_kids_2_tracks...\n",
      "Asking LLM for partition column for 51_just_dance_kids_2_tracks...\n",
      "[attribute_based]   ✓ Success - Saved to 4_new_brunswick_parishes_2006_2011.json\n",
      "[attribute_based] [35/48] Processing 51_just_dance_kids_2_tracks...\n",
      "Asking LLM for partition column for 51_just_dance_kids_2_tracks...\n",
      "Fetching distinct values for column 'Difficulty'...\n",
      "Fetching distinct values for column 'Difficulty'...\n",
      "[attribute_based]   ✓ Success - Saved to 51_just_dance_kids_2_tracks.json\n",
      "[attribute_based] [36/48] Processing 52_cross_country_junior_women_1996...\n",
      "Asking LLM for partition column for 52_cross_country_junior_women_1996...\n",
      "[attribute_based]   ✓ Success - Saved to 51_just_dance_kids_2_tracks.json\n",
      "[attribute_based] [36/48] Processing 52_cross_country_junior_women_1996...\n",
      "Asking LLM for partition column for 52_cross_country_junior_women_1996...\n",
      "Fetching distinct values for column 'Country'...\n",
      "Fetching distinct values for column 'Country'...\n",
      "[attribute_based]   ✓ Success - Saved to 52_cross_country_junior_women_1996.json\n",
      "[attribute_based] [37/48] Processing 53_metropolitan_opera_us_premieres...\n",
      "Asking LLM for partition column for 53_metropolitan_opera_us_premieres...\n",
      "[attribute_based]   ✓ Success - Saved to 52_cross_country_junior_women_1996.json\n",
      "[attribute_based] [37/48] Processing 53_metropolitan_opera_us_premieres...\n",
      "Asking LLM for partition column for 53_metropolitan_opera_us_premieres...\n",
      "Fetching distinct values for column 'Date of premiere'...\n",
      "Fetching distinct values for column 'Date of premiere'...\n",
      "[attribute_based]   ✓ Success - Saved to 53_metropolitan_opera_us_premieres.json\n",
      "[attribute_based] [38/48] Processing 54_kasparov_kramnik_1993_2004...\n",
      "Asking LLM for partition column for 54_kasparov_kramnik_1993_2004...\n",
      "[attribute_based]   ✓ Success - Saved to 53_metropolitan_opera_us_premieres.json\n",
      "[attribute_based] [38/48] Processing 54_kasparov_kramnik_1993_2004...\n",
      "Asking LLM for partition column for 54_kasparov_kramnik_1993_2004...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 54_kasparov_kramnik_1993_2004.json\n",
      "[attribute_based] [39/48] Processing 55_decathlon_top50_1999...\n",
      "Asking LLM for partition column for 55_decathlon_top50_1999...\n",
      "[attribute_based]   ✓ Success - Saved to 54_kasparov_kramnik_1993_2004.json\n",
      "[attribute_based] [39/48] Processing 55_decathlon_top50_1999...\n",
      "Asking LLM for partition column for 55_decathlon_top50_1999...\n",
      "Fetching distinct values for column 'Venue'...\n",
      "Fetching distinct values for column 'Venue'...\n",
      "[attribute_based]   ✓ Success - Saved to 55_decathlon_top50_1999.json\n",
      "[attribute_based] [40/48] Processing 56_minor_planets_152601_152700...\n",
      "Asking LLM for partition column for 56_minor_planets_152601_152700...\n",
      "[attribute_based]   ✓ Success - Saved to 55_decathlon_top50_1999.json\n",
      "[attribute_based] [40/48] Processing 56_minor_planets_152601_152700...\n",
      "Asking LLM for partition column for 56_minor_planets_152601_152700...\n",
      "Fetching distinct values for column 'Discovery date'...\n",
      "Fetching distinct values for column 'Discovery date'...\n",
      "[attribute_based]   ✓ Success - Saved to 56_minor_planets_152601_152700.json\n",
      "[attribute_based] [41/48] Processing 59_miss_new_york_usa_delegates_2012...\n",
      "Asking LLM for partition column for 59_miss_new_york_usa_delegates_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 56_minor_planets_152601_152700.json\n",
      "[attribute_based] [41/48] Processing 59_miss_new_york_usa_delegates_2012...\n",
      "Asking LLM for partition column for 59_miss_new_york_usa_delegates_2012...\n",
      "Fetching distinct values for column 'Represents'...\n",
      "Fetching distinct values for column 'Represents'...\n",
      "[attribute_based]   ✓ Success - Saved to 59_miss_new_york_usa_delegates_2012.json\n",
      "[attribute_based] [42/48] Processing 5_ice_hockey_2006...\n",
      "Asking LLM for partition column for 5_ice_hockey_2006...\n",
      "[attribute_based]   ✓ Success - Saved to 59_miss_new_york_usa_delegates_2012.json\n",
      "[attribute_based] [42/48] Processing 5_ice_hockey_2006...\n",
      "Asking LLM for partition column for 5_ice_hockey_2006...\n",
      "Fetching distinct values for column 'Country'...\n",
      "Fetching distinct values for column 'Country'...\n",
      "[attribute_based]   ✓ Success - Saved to 5_ice_hockey_2006.json\n",
      "[attribute_based] [43/48] Processing 67_london_heathrow_busiest_routes_2012...\n",
      "Asking LLM for partition column for 67_london_heathrow_busiest_routes_2012...\n",
      "[attribute_based]   ✓ Success - Saved to 5_ice_hockey_2006.json\n",
      "[attribute_based] [43/48] Processing 67_london_heathrow_busiest_routes_2012...\n",
      "Asking LLM for partition column for 67_london_heathrow_busiest_routes_2012...\n",
      "Fetching distinct values for column 'Airport'...\n",
      "Fetching distinct values for column 'Airport'...\n",
      "[attribute_based]   ✓ Success - Saved to 67_london_heathrow_busiest_routes_2012.json\n",
      "[attribute_based] [44/48] Processing 71_us_president_elections_idaho_2008...\n",
      "Asking LLM for partition column for 71_us_president_elections_idaho_2008...\n",
      "[attribute_based]   ✓ Success - Saved to 67_london_heathrow_busiest_routes_2012.json\n",
      "[attribute_based] [44/48] Processing 71_us_president_elections_idaho_2008...\n",
      "Asking LLM for partition column for 71_us_president_elections_idaho_2008...\n",
      "Fetching distinct values for column 'County'...\n",
      "Fetching distinct values for column 'County'...\n",
      "[attribute_based]   ✓ Success - Saved to 71_us_president_elections_idaho_2008.json\n",
      "[attribute_based] [45/48] Processing 78_bafta_best_actor_leading_role_2000s...\n",
      "Asking LLM for partition column for 78_bafta_best_actor_leading_role_2000s...\n",
      "[attribute_based]   ✓ Success - Saved to 71_us_president_elections_idaho_2008.json\n",
      "[attribute_based] [45/48] Processing 78_bafta_best_actor_leading_role_2000s...\n",
      "Asking LLM for partition column for 78_bafta_best_actor_leading_role_2000s...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 78_bafta_best_actor_leading_role_2000s.json\n",
      "[attribute_based] [46/48] Processing 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "Asking LLM for partition column for 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "[attribute_based]   ✓ Success - Saved to 78_bafta_best_actor_leading_role_2000s.json\n",
      "[attribute_based] [46/48] Processing 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "Asking LLM for partition column for 7_anaheim_ducks_draft_picks_1998_2013...\n",
      "Fetching distinct values for column 'Draft'...\n",
      "Fetching distinct values for column 'Draft'...\n",
      "[attribute_based]   ✓ Success - Saved to 7_anaheim_ducks_draft_picks_1998_2013.json\n",
      "[attribute_based] [47/48] Processing 8_south_african_class_15f_4_8_2...\n",
      "Asking LLM for partition column for 8_south_african_class_15f_4_8_2...\n",
      "[attribute_based]   ✓ Success - Saved to 7_anaheim_ducks_draft_picks_1998_2013.json\n",
      "[attribute_based] [47/48] Processing 8_south_african_class_15f_4_8_2...\n",
      "Asking LLM for partition column for 8_south_african_class_15f_4_8_2...\n",
      "Fetching distinct values for column 'Year'...\n",
      "Fetching distinct values for column 'Year'...\n",
      "[attribute_based]   ✓ Success - Saved to 8_south_african_class_15f_4_8_2.json\n",
      "[attribute_based] [48/48] Processing 9_tour_de_france_2009...\n",
      "Asking LLM for partition column for 9_tour_de_france_2009...\n",
      "[attribute_based]   ✓ Success - Saved to 8_south_african_class_15f_4_8_2.json\n",
      "[attribute_based] [48/48] Processing 9_tour_de_france_2009...\n",
      "Asking LLM for partition column for 9_tour_de_france_2009...\n",
      "Fetching distinct values for column 'Team'...\n",
      "Fetching distinct values for column 'Team'...\n",
      "[attribute_based]   ✓ Success - Saved to 9_tour_de_france_2009.json\n",
      "\n",
      "[attribute_based] Completed: 47 successes, 1 errors\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "full_table: 48 tables processed\n",
      "row_by_row: 48 tables processed\n",
      "attribute_based: 47 tables processed\n",
      "classic_pagination: 48 tables processed\n",
      "range_based: 48 tables processed\n",
      "============================================================\n",
      "[attribute_based]   ✓ Success - Saved to 9_tour_de_france_2009.json\n",
      "\n",
      "[attribute_based] Completed: 47 successes, 1 errors\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "full_table: 48 tables processed\n",
      "row_by_row: 48 tables processed\n",
      "attribute_based: 47 tables processed\n",
      "classic_pagination: 48 tables processed\n",
      "range_based: 48 tables processed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = OUTPUT_ROOT / timestamp\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create subdirectories for each strategy upfront\n",
    "for strategy_name in ACTIVE_STRATEGIES:\n",
    "    strategy_dir = output_dir / strategy_name\n",
    "    strategy_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Process all tables for each strategy\n",
    "results = {strategy: [] for strategy in ACTIVE_STRATEGIES}\n",
    "errors = {strategy: [] for strategy in ACTIVE_STRATEGIES}\n",
    "\n",
    "def process_strategy(strategy_name: str):\n",
    "    \"\"\"Process all tables for a single strategy with retry logic.\"\"\"\n",
    "    strategy_results = []\n",
    "    strategy_errors = []\n",
    "    MAX_RETRIES = 3\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running strategy: {strategy_name.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    strategy_func = STRATEGIES_TO_RUN[strategy_name]\n",
    "    strategy_dir = output_dir / strategy_name\n",
    "    \n",
    "    for i, table in enumerate(tables_to_process):\n",
    "        print(f\"[{strategy_name}] [{i+1}/{len(tables_to_process)}] Processing {table['table_id']}...\")\n",
    "        \n",
    "        plan = None\n",
    "        last_error = None\n",
    "        \n",
    "        # Retry loop\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                plan = strategy_func(table)\n",
    "                \n",
    "                if plan:\n",
    "                    # Success!\n",
    "                    break\n",
    "                else:\n",
    "                    last_error = 'Function returned None'\n",
    "                    if attempt < MAX_RETRIES - 1:\n",
    "                        print(f\"[{strategy_name}]   ⚠ Attempt {attempt + 1}/{MAX_RETRIES} failed (returned None), retrying...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                last_error = str(e)\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    print(f\"[{strategy_name}]   ⚠ Attempt {attempt + 1}/{MAX_RETRIES} failed: {e}, retrying...\")\n",
    "        \n",
    "        # After all retries, check if we succeeded\n",
    "        if plan:\n",
    "            strategy_results.append(plan)\n",
    "            \n",
    "            # Save immediately after successful processing\n",
    "            table_id = plan['table_id']\n",
    "            output_file = strategy_dir / f\"{table_id}.json\"\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(plan, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            retry_msg = f\" (after {attempt + 1} attempt{'s' if attempt > 0 else ''})\" if attempt > 0 else \"\"\n",
    "            print(f\"[{strategy_name}]   ✓ Success{retry_msg} - Saved to {output_file.name}\")\n",
    "        else:\n",
    "            # All retries failed\n",
    "            strategy_errors.append({\n",
    "                'table_id': table['table_id'],\n",
    "                'error': last_error,\n",
    "                'attempts': MAX_RETRIES\n",
    "            })\n",
    "            print(f\"[{strategy_name}]   ✗ Failed after {MAX_RETRIES} attempts: {last_error}\")\n",
    "        \n",
    "        # Save errors incrementally too\n",
    "        if strategy_errors:\n",
    "            errors_file = strategy_dir / '_errors.json'\n",
    "            with open(errors_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(strategy_errors, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n[{strategy_name}] Completed: {len(strategy_results)} successes, {len(strategy_errors)} errors\")\n",
    "    return strategy_name, strategy_results, strategy_errors\n",
    "\n",
    "# Run strategies in parallel or sequentially\n",
    "if PARALLEL_STRATEGIES:\n",
    "    print(f\"\\n⚡ Running {len(ACTIVE_STRATEGIES)} strategies in PARALLEL with {MAX_WORKERS} workers\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all strategy tasks\n",
    "        future_to_strategy = {\n",
    "            executor.submit(process_strategy, strategy_name): strategy_name \n",
    "            for strategy_name in ACTIVE_STRATEGIES\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_strategy):\n",
    "            strategy_name, strategy_results, strategy_errors = future.result()\n",
    "            results[strategy_name] = strategy_results\n",
    "            errors[strategy_name] = strategy_errors\n",
    "else:\n",
    "    print(f\"\\n🔄 Running {len(ACTIVE_STRATEGIES)} strategies SEQUENTIALLY\")\n",
    "    \n",
    "    for strategy_name in ACTIVE_STRATEGIES:\n",
    "        strategy_name, strategy_results, strategy_errors = process_strategy(strategy_name)\n",
    "        results[strategy_name] = strategy_results\n",
    "        errors[strategy_name] = strategy_errors\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for strategy_name in ACTIVE_STRATEGIES:\n",
    "    print(f\"{strategy_name}: {len(results[strategy_name])} tables processed\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e0841",
   "metadata": {},
   "source": [
    "## Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f9110bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final summary saved to processing/1_strategy/20251005_004530/_summary.json\n",
      "\n",
      "All done! Results saved to processing/1_strategy/20251005_004530\n"
     ]
    }
   ],
   "source": [
    "# All individual files have been saved incrementally during processing\n",
    "# Now just save the final summary\n",
    "\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'max_tables_limit': MAX_TABLES,\n",
    "    'total_tables': len(tables),\n",
    "    'processed_tables': len(tables_to_process),\n",
    "    'strategies': {\n",
    "        strategy_name: {\n",
    "            'success_count': len(results[strategy_name]),\n",
    "            'error_count': len(errors[strategy_name])\n",
    "        }\n",
    "        for strategy_name in ACTIVE_STRATEGIES\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_file = output_dir / '_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nFinal summary saved to {summary_file}\")\n",
    "print(f\"\\nAll done! Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10395346",
   "metadata": {},
   "source": [
    "## Sample Output Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e133d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample output for FULL_TABLE\n",
      "============================================================\n",
      "{\n",
      "  \"table_id\": \"10_men_butterfly_100m_2009\",\n",
      "  \"table_name\": \"men_butterfly_100m_2009\",\n",
      "  \"strategy\": \"full_table\",\n",
      "  \"metadata\": {\n",
      "    \"table_title\": \"men's 100 metre butterfly results in heats at the 2009 World Aquatics Championships\",\n",
      "    \"columns\": [\n",
      "      \"Name\",\n",
      "      \"Nationality\",\n",
      "      \"Time\",\n",
      "      \"Heat\",\n",
      "      \"Lane\"\n",
      "    ],\n",
      "    \"key_columns\": [\n",
      "      \"Name\"\n",
      "    ]\n",
      "  },\n",
      "  \"pagination_config\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect a sample output from each strategy\n",
    "for strategy_name in ACTIVE_STRATEGIES:\n",
    "    if results[strategy_name]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Sample output for {strategy_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        sample = results[strategy_name][0]\n",
    "        print(json.dumps(sample, indent=2))\n",
    "        break  # Show just one example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c20d2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The pagination plans have been generated and saved to `processing/1_strategy/<timestamp>/`.\n",
    "\n",
    "Each strategy creates a subdirectory with JSON files for each table containing:\n",
    "- **table_id**: Unique identifier\n",
    "- **strategy**: The pagination approach used\n",
    "- **metadata**: Table information (title, columns, keys)\n",
    "- **pagination_config**: Strategy-specific configuration for the fetch notebook\n",
    "\n",
    "### For the next notebook (2_FetchPages.ipynb):\n",
    "1. Load these JSON files\n",
    "2. For each pagination plan, execute the appropriate fetching logic:\n",
    "   - **full_table**: One query for entire table\n",
    "   - **row_by_row**: Use key_values to fetch each row\n",
    "   - **attribute_based**: Use partition_column and partition_values to fetch filtered pages\n",
    "   - **classic_pagination**: Iteratively fetch pages using page_size and primary_keys\n",
    "   - **range_based**: Use ranges to fetch data in buckets\n",
    "\n",
    "### To enable more strategies:\n",
    "Uncomment the strategies you want in the `ACTIVE_STRATEGIES` list above and re-run the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
