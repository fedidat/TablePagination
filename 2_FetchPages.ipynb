{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07c4455",
   "metadata": {},
   "source": [
    "# Stage 3: Fetch Pages\n",
    "\n",
    "\n",
    "This notebook implements the third stage of the pipeline: fetching pages using LLMs, recording metrics, and saving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74912bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f70d826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pagination folder: processing/1_pagination/20250831_110417\n"
     ]
    }
   ],
   "source": [
    "# Identify Latest Folder Under Pagination\n",
    "pagination_root = 'processing/1_pagination/'\n",
    "folders = [f for f in os.listdir(pagination_root) if os.path.isdir(os.path.join(pagination_root, f))]\n",
    "latest_folder = sorted(folders)[-1] if folders else None\n",
    "pagination_path = os.path.join(pagination_root, latest_folder) if latest_folder else None\n",
    "assert pagination_path and os.path.exists(pagination_path), 'No pagination folder found.'\n",
    "print('Using pagination folder:', pagination_path)\n",
    "\n",
    "# INCREMENT MODE: Build a set of already processed table IDs/names\n",
    "INCREMENT_MODE = True  # Set to False for normal full run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed3ce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 tables.\n"
     ]
    }
   ],
   "source": [
    "# Load Pagination Criteria\n",
    "json_files = glob.glob(os.path.join(pagination_path, '*.json'))\n",
    "tables = []\n",
    "for jf in json_files:\n",
    "    with open(jf, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    tables.append({'path': jf, 'meta': obj['meta'], 'criteria': obj['pagination_criteria']})\n",
    "print(f'Loaded {len(tables)} tables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fcb997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TableGenerator_JSON():\n",
    "    TEMPLATE = \"\"\"\n",
    "    List %s - as many as possible to fit into response.\n",
    "    The response will be formatted as JSON shown below.\n",
    "    Each element of the response will contain %d fields: %s.\n",
    "    Do not output any additional text that is not in JSON format.\n",
    "    %s\n",
    "    \n",
    "    \"\"\"   \n",
    "\n",
    "    def _norm_field(self, s):\n",
    "        s = s.lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\".\", \"\").replace(\",\",\"_\")\\\n",
    "                .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"','').replace(\"'\",\"\")\\\n",
    "                .replace(\"/\", \"\")\n",
    "        return re.sub('_+', '_', s)\n",
    "        \n",
    "    def generate_prompts(self, query, fields, paging: dict | None):\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "\n",
    "        num_fields = len(fields)\n",
    "        fields_json = []\n",
    "        fields = [f for f in fields]\n",
    "        for field in fields:\n",
    "            fields_json.append('\"%s\": \"%s\"' % ('_'.join(field.replace(\"-\", \" \").split()), field))\n",
    "        response_format = ', '.join(fields)\n",
    "        if paging:\n",
    "            paging_criteria = ('Only fetch the results where values for %s match: %s.' % (paging['field'], paging['value']))\n",
    "        else:\n",
    "            paging_criteria = ''\n",
    "        user_msg = self.TEMPLATE % (query, num_fields, response_format, paging_criteria)\n",
    "        return system_msg, user_msg\n",
    "\n",
    "    def parse_llm_response(self, response): \n",
    "        res = []\n",
    "        try:\n",
    "            if not response.startswith(\"[\") and \"[\" in response:\n",
    "                response = response[response.find(\"[\"):]\n",
    "\n",
    "            if not response.endswith(\"]\") and \"]\" in response:\n",
    "                response = response[:response.rfind(\"]\")+1]\n",
    "\n",
    "            if '[' not in response and ']' not in response and '{' in response and '}' in response:\n",
    "                response = '[' + response + ']'    \n",
    "\n",
    "            response_json = json.loads(response)\n",
    "\n",
    "            if isinstance(response_json, dict) and len(response_json.keys()) == 1:\n",
    "                response_json = list(response_json.values())[0]    \n",
    "        except:  \n",
    "            split_response = response.split(\"{\")\n",
    "            response_json = []\n",
    "            for s in split_response[1:]:\n",
    "                split_s = s.split(\"}\")\n",
    "                if len(split_s) > 1:\n",
    "                    content = split_s[0]\n",
    "                    attributes = content.split(\",\")\n",
    "                    elements = {}\n",
    "                    for attr in attributes:\n",
    "                        knv = attr.split(\":\")   \n",
    "                        if len(knv) > 1:\n",
    "                            parsed_k = \"%s\" % knv[0].replace('\"','').strip()\n",
    "                            parsed_v = \"%s\" % knv[1].replace('\"','').strip()\n",
    "                            elements[parsed_k] = parsed_v\n",
    "\n",
    "                    response_json.append(elements)  \n",
    "\n",
    "        df = pd.DataFrame.from_records(response_json) \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec1f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_llm(prompt, model, api_key, system_msg: str = ''):\n",
    "    url = 'https://openrouter.ai/api/v1/chat/completions'\n",
    "    headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}\n",
    "    payload = {\n",
    "        'model': model,\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': system_msg},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'max_tokens': 20000\n",
    "    }\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=LLM_TIMEOUT)\n",
    "        latency = time.time() - start\n",
    "        resp.raise_for_status()\n",
    "        result = resp.json()\n",
    "        content = result['choices'][0]['message']['content'] if 'choices' in result else ''\n",
    "        content = table_generator.parse_llm_response(content)\n",
    "        usage = result.get('usage', {})\n",
    "        return content, latency, usage, None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to fetch page: {e}\")\n",
    "        return None, None, None, str(e)\n",
    "\n",
    "def fetch_page_task(args):\n",
    "    # args: (meta, crit_obj, page_key, model_name, source_columns, source_csv_str)\n",
    "    meta, crit_obj, page_key, model_name, source_columns, source_csv_str = args\n",
    "    page_content = {'field': crit_obj.get('criteria',''), 'value': page_key} if page_key != 'ALL' else None\n",
    "    system_msg, user_msg = table_generator.generate_prompts(meta.get('query_without_cutoff'), source_columns, page_content)\n",
    "    if PROVIDE_SOURCE_TABLE and source_csv_str:\n",
    "        user_msg += f\"\\n\\nSource table as CSV:\\n{source_csv_str}\"\n",
    "    print(f\"[FETCH] Table: {meta.get('name')}, Page: {page_content}, Model: {model_name}\")\n",
    "    content, latency, usage, error = fetch_page_llm(user_msg, LLM_MODEL, OPENROUTER_API_KEY, system_msg)\n",
    "    return {\n",
    "        'content': content,\n",
    "        'latency': latency,\n",
    "        'usage': usage,\n",
    "        'error': error,\n",
    "        'page_key': page_key\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604a0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(merged_df, source_columns, metrics, source_df):\n",
    "    row_count = len(merged_df) if merged_df is not None else 0\n",
    "    col_consistency = (set(merged_df.columns) == set(source_columns)) if source_columns and not merged_df.empty else None\n",
    "    error_count = sum(1 for m in metrics if m['error'])\n",
    "    total_pages = len(metrics)\n",
    "    error_rate = error_count / total_pages if total_pages > 0 else None\n",
    "    latencies = [m['latency'] for m in metrics if m['latency'] is not None]\n",
    "    avg_latency = sum(latencies) / len(latencies) if latencies else None\n",
    "    token_counts = [m['usage'].get('total_tokens', 0) for m in metrics if m['usage'] and 'total_tokens' in m['usage']]\n",
    "    sum_tokens = sum(token_counts) if token_counts else None\n",
    "    acc_metrics = None\n",
    "    if source_df is not None and not merged_df.empty:\n",
    "        acc_metrics = accuracy_metrics(merged_df, source_df)\n",
    "    return {\n",
    "        'row_count': row_count,\n",
    "        'column_consistency': col_consistency,\n",
    "        'error_rate': error_rate,\n",
    "        'avg_latency': avg_latency,\n",
    "        'sum_tokens': sum_tokens,\n",
    "        'accuracy': acc_metrics\n",
    "    }\n",
    "\n",
    "# Accuracy metric functions (from old/2_Metrics_calculation.ipynb)\n",
    "def accuracy_metrics(merged_df, source_df):\n",
    "    # Only compare columns present in both\n",
    "    common_cols = [col for col in source_df.columns if col in merged_df.columns]\n",
    "    # Cast all columns to string for comparison\n",
    "    src = source_df[common_cols].drop_duplicates().reset_index(drop=True).astype(str)\n",
    "    pred = merged_df[common_cols].drop_duplicates().reset_index(drop=True).astype(str)\n",
    "    # Row-level accuracy: fraction of source rows present in merged\n",
    "    correct_rows = src.merge(pred, how='inner').shape[0]\n",
    "    total_rows = src.shape[0]\n",
    "    row_recall = correct_rows / total_rows if total_rows > 0 else None\n",
    "    # Precision: fraction of merged rows that are correct\n",
    "    correct_pred_rows = pred.merge(src, how='inner').shape[0]\n",
    "    total_pred_rows = pred.shape[0]\n",
    "    row_precision = correct_pred_rows / total_pred_rows if total_pred_rows > 0 else None\n",
    "    # F1 score\n",
    "    if row_precision is not None and row_recall is not None and (row_precision + row_recall) > 0:\n",
    "        row_f1 = 2 * row_precision * row_recall / (row_precision + row_recall)\n",
    "    else:\n",
    "        row_f1 = None\n",
    "    return {\n",
    "        'row_recall': row_recall,\n",
    "        'row_precision': row_precision,\n",
    "        'row_f1': row_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad1417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INCREMENT] Using latest output folder: processing/2_fetched_pages/20250831_111058\n",
      "[INCREMENT] Existing metrics found for tables: {'new_brunswick_parishes_2006_2011', 'figure_skating_ladies_2009_2010', 'portuguese_grape_varieties', 'south_african_class_15f_4_8_2', 'fa_cup_qualifying_rounds_1999_2000', 'english_latin_rivalry_1887_2012', 'cross_country_junior_women_1996', 'liechtenstein_demographics_1901_2011', 'ramsar_convention_parties', 'ice_hockey_2006', 'udaykumar_films', 'miss_new_york_usa_delegates_2012', 'australia_demographics_1900_2010', 'living_proof_the_farewell_tour', 'men_butterfly_100m_2009', 'rock_band_downloadable_2011', 'anaheim_ducks_draft_picks_1998_2013', 'tennessee_vanderbilt_rivalry_1900_2012', 'playstation_3_cooperative_games', 'tour_de_france_2009', 'belgium_demographics_1900_2011', 'republican_straw_polls_2012', 'scottish_football_transfers_summer_2011'}\n",
      "[INCREMENT] Skipping table english_latin_rivalry_1887_2012 (metrics.json exists)\n",
      "[INCREMENT] Skipping table australia_demographics_1900_2010 (metrics.json exists)\n",
      "[PROCESSING] Table: elements, Source Columns: ['Z', 'Sym', 'Element', 'Group', 'Period', 'Atomic weight u', 'Density g / cm 3', 'Melt K', 'Boil K', 'Heat J / g * K', 'Neg']\n",
      "[FETCH] Table: elements, Page: None, Model: x-ai_grok-3-mini\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/15_elements_naive_x-ai_grok-3-mini.csv\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': 1}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': 2}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': 3}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': 4}, Model: x-ai_grok-3-mini\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/15_elements_statistical_x-ai_grok-3-mini.csv\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': '1'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': '2'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': '3'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': '4'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': '5'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': '6'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: elements, Page: {'field': 'Period', 'value': '7'}, Model: google_gemini-2.5-flash-lite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/dzjx_6bx24960zc7sd87xsq400xh_z/T/ipykernel_86550/2873140760.py:103: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([merged_df, df_page], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/15_elements_llm_google_gemini-2.5-flash-lite.csv\n",
      "[FETCH] Table: elements, Page: {'field': 'Period,Group', 'value': '1,1-18'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: elements, Page: {'field': 'Period,Group', 'value': '2,1-18'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: elements, Page: {'field': 'Period,Group', 'value': '3,1-18'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: elements, Page: {'field': 'Period,Group', 'value': '4,1-18'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: elements, Page: {'field': 'Period,Group', 'value': '5-7,1-18'}, Model: deepseek_deepseek-chat-v3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/dzjx_6bx24960zc7sd87xsq400xh_z/T/ipykernel_86550/2873140760.py:103: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([merged_df, df_page], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/15_elements_llm_deepseek_deepseek-chat-v3.1.csv\n",
      "[FETCH] Table: elements, Page: {'field': 'Group', 'value': '1'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: elements, Page: {'field': 'Group', 'value': '2'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: elements, Page: {'field': 'Group', 'value': '13'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: elements, Page: {'field': 'Group', 'value': '14'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: elements, Page: {'field': 'Group', 'value': '18'}, Model: openai_gpt-4o-mini\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/15_elements_llm_openai_gpt-4o-mini.csv\n",
      "Saved JSON metadata: processing/2_fetched_pages/20250831_111058/15_elements_metrics.json\n",
      "[INCREMENT] Skipping table rock_band_downloadable_2011 (metrics.json exists)\n",
      "[INCREMENT] Skipping table living_proof_the_farewell_tour (metrics.json exists)\n",
      "[INCREMENT] Skipping table portuguese_grape_varieties (metrics.json exists)\n",
      "[INCREMENT] Skipping table republican_straw_polls_2012 (metrics.json exists)\n",
      "[INCREMENT] Skipping table miss_new_york_usa_delegates_2012 (metrics.json exists)\n",
      "[INCREMENT] Skipping table cross_country_junior_women_1996 (metrics.json exists)\n",
      "[INCREMENT] Skipping table tour_de_france_2009 (metrics.json exists)\n",
      "[INCREMENT] Skipping table anaheim_ducks_draft_picks_1998_2013 (metrics.json exists)\n",
      "[INCREMENT] Skipping table belgium_demographics_1900_2011 (metrics.json exists)\n",
      "[INCREMENT] Skipping table men_butterfly_100m_2009 (metrics.json exists)\n",
      "[INCREMENT] Skipping table tennessee_vanderbilt_rivalry_1900_2012 (metrics.json exists)\n",
      "[INCREMENT] Skipping table ice_hockey_2006 (metrics.json exists)\n",
      "[INCREMENT] Skipping table scottish_football_transfers_summer_2011 (metrics.json exists)\n",
      "[INCREMENT] Skipping table udaykumar_films (metrics.json exists)\n",
      "[INCREMENT] Skipping table ramsar_convention_parties (metrics.json exists)\n",
      "[INCREMENT] Skipping table figure_skating_ladies_2009_2010 (metrics.json exists)\n",
      "[INCREMENT] Skipping table liechtenstein_demographics_1901_2011 (metrics.json exists)\n",
      "[INCREMENT] Skipping table new_brunswick_parishes_2006_2011 (metrics.json exists)\n",
      "[PROCESSING] Table: curling_teams_women_2013_2014, Source Columns: ['Skip', 'Third', 'Second', 'Lead', 'Alternate', 'Locale']\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: None, Model: x-ai_grok-3-mini\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/16_curling_teams_women_2013_2014_naive_x-ai_grok-3-mini.csv\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Alberta'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'British Columbia'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Manitoba'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Minnesota'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Nova Scotia'}, Model: google_gemini-2.5-flash-lite\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/16_curling_teams_women_2013_2014_llm_google_gemini-2.5-flash-lite.csv\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Alberta'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'British Columbia'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Manitoba'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Ontario'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Other Regions'}, Model: deepseek_deepseek-chat-v3.1\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/16_curling_teams_women_2013_2014_llm_deepseek_deepseek-chat-v3.1.csv\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Alberta'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Minnesota'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Pennsylvania'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Nova Scotia'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: curling_teams_women_2013_2014, Page: {'field': 'Locale', 'value': 'Sweden'}, Model: openai_gpt-4o-mini\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/16_curling_teams_women_2013_2014_llm_openai_gpt-4o-mini.csv\n",
      "Saved JSON metadata: processing/2_fetched_pages/20250831_111058/16_curling_teams_women_2013_2014_metrics.json\n",
      "[INCREMENT] Skipping table south_african_class_15f_4_8_2 (metrics.json exists)\n",
      "[INCREMENT] Skipping table playstation_3_cooperative_games (metrics.json exists)\n",
      "[INCREMENT] Skipping table fa_cup_qualifying_rounds_1999_2000 (metrics.json exists)\n",
      "[PROCESSING] Table: minor_planets_discovered_by_nikolai_chernykh, Source Columns: ['Name', 'Discovery Date']\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: None, Model: x-ai_grok-3-mini\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/14_minor_planets_discovered_by_nikolai_chernykh_naive_x-ai_grok-3-mini.csv\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1966-1977'}, Model: google_gemini-2.5-flash-lite\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/14_minor_planets_discovered_by_nikolai_chernykh_llm_google_gemini-2.5-flash-lite.csv\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1966'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1971'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1972'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1973'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1976-1977'}, Model: deepseek_deepseek-chat-v3.1\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_111058/14_minor_planets_discovered_by_nikolai_chernykh_llm_deepseek_deepseek-chat-v3.1.csv\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1966-1970'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1971-1975'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: minor_planets_discovered_by_nikolai_chernykh, Page: {'field': 'Discovery Date', 'value': '1976-1980'}, Model: openai_gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Fetch Pages Using LLM and Record Metrics (Parallelized)\n",
    "from io import StringIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "PROVIDE_SOURCE_TABLE = True  # If True, include source table as CSV in the prompt\n",
    "LLM_TIMEOUT = 30  # seconds\n",
    "LLM_MODEL = 'x-ai/grok-3-mini'  # Use this model for all criteria\n",
    "\n",
    "OPENROUTER_API_KEY = os.environ.get('OPENROUTER_API_KEY', '')\n",
    "\n",
    "output_root = 'processing/2_fetched_pages/'\n",
    "\n",
    "table_generator = TableGenerator_JSON()\n",
    "\n",
    "def get_latest_timestamped_dir(root):\n",
    "    folders = [f for f in os.listdir(root) if os.path.isdir(os.path.join(root, f))]\n",
    "    if not folders:\n",
    "        return None\n",
    "    # Sort by timestamp in folder name (assuming YYYYMMDD_HHMMSS)\n",
    "    return sorted(folders)[-1]\n",
    "\n",
    "if INCREMENT_MODE:\n",
    "    output_root = 'processing/2_fetched_pages/'\n",
    "    latest_dir = get_latest_timestamped_dir(output_root)\n",
    "    if latest_dir:\n",
    "        output_folder = os.path.join(output_root, latest_dir)\n",
    "        print(f\"[INCREMENT] Using latest output folder: {output_folder}\")\n",
    "    else:\n",
    "        output_folder = os.path.join(output_root, timestamp)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        print(f\"[INCREMENT] No previous folder found, using new: {output_folder}\")\n",
    "    # Build set of already processed table names\n",
    "    existing_metrics = set()\n",
    "    for fname in os.listdir(output_folder):\n",
    "        if fname.endswith('_metrics.json'):\n",
    "            parts = fname.split('_metrics.json')[0].split('_', 1)\n",
    "            if len(parts) == 2:\n",
    "                existing_metrics.add(parts[1])  # table name\n",
    "            else:\n",
    "                existing_metrics.add(parts[0])  # fallback\n",
    "    print(f\"[INCREMENT] Existing metrics found for tables: {existing_metrics}\")\n",
    "else:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_folder = os.path.join(output_root, timestamp)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"[FULL RUN] Using new output folder: {output_folder}\")\n",
    "\n",
    "for table in tables:\n",
    "    meta = table['meta']\n",
    "    criteria = table['criteria']\n",
    "    if INCREMENT_MODE and meta.get('name') in existing_metrics:\n",
    "        print(f\"[INCREMENT] Skipping table {meta.get('name')} (metrics.json exists)\")\n",
    "        continue\n",
    "    source_csv_path = meta.get('source_file')\n",
    "    source_csv_str = ''\n",
    "    source_columns = None\n",
    "    source_df = None\n",
    "    if PROVIDE_SOURCE_TABLE and source_csv_path and os.path.exists(source_csv_path):\n",
    "        try:\n",
    "            source_df = pd.read_csv(source_csv_path)\n",
    "            source_csv_str = source_df.to_csv(index=False)\n",
    "            source_columns = list(source_df.columns)\n",
    "            csv_path = os.path.join(output_folder, f\"{meta.get('id','')}_{meta['file']}\")\n",
    "            source_df.to_csv(csv_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f'[WARNING] Could not load source CSV for {meta.get(\"name\")}: {e}')\n",
    "            source_csv_str = ''\n",
    "            source_columns = None\n",
    "    print(f\"[PROCESSING] Table: {meta.get('name')}, Source Columns: {source_columns}\")\n",
    "    table_results = {}\n",
    "    for method, crit in criteria.items():\n",
    "        # For llm, collect top recommendation from all listed models\n",
    "        if method == 'llm':\n",
    "            criteria_list = []\n",
    "            for model_name, model_criteria in crit.items():\n",
    "                if not model_criteria:\n",
    "                    continue\n",
    "                top_crit = model_criteria[0] if isinstance(model_criteria, list) else model_criteria\n",
    "                criteria_list.append((model_name, top_crit))\n",
    "        else:\n",
    "            criteria_list = [(LLM_MODEL, crit)]\n",
    "        for model_name, crit_obj in criteria_list:\n",
    "            model_name = model_name.replace('/', '_')\n",
    "            pages = crit_obj.get('pages', [])\n",
    "            if source_columns:\n",
    "                merged_df = pd.DataFrame(columns=source_columns)\n",
    "            else:\n",
    "                merged_df = pd.DataFrame()\n",
    "            metrics = []\n",
    "            # Prepare tasks for all pages\n",
    "            tasks = [(meta, crit_obj, page_key, model_name, source_columns, source_csv_str) for page_key in pages]\n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                future_to_page = {executor.submit(fetch_page_task, t): t[2] for t in tasks}\n",
    "                for future in as_completed(future_to_page):\n",
    "                    page_key = future_to_page[future]\n",
    "                    try:\n",
    "                        res = future.result()\n",
    "                        metrics.append({'latency': res['latency'], 'usage': res['usage'], 'error': res['error']})\n",
    "                        if res['content'] is not None:\n",
    "                            try:\n",
    "                                df_page = res['content']\n",
    "                                merged_df = pd.concat([merged_df, df_page], ignore_index=True)\n",
    "                            except Exception:\n",
    "                                print(f\"[WARNING] Skipping non-CSV response for table {meta.get('name')}, page {page_key}, model {model_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Exception in future for page {page_key}: {e}\")\n",
    "            if merged_df is not None and not merged_df.empty:\n",
    "                csv_name = f\"{meta.get('id','')}_{meta.get('name','')}_{method}_{model_name}.csv\"\n",
    "                csv_path = os.path.join(output_folder, csv_name)\n",
    "                merged_df.to_csv(csv_path, index=False)\n",
    "                print(f'Saved merged CSV: {csv_path}')\n",
    "            # else:\n",
    "                metrics.append({'latency': None, 'usage': None, 'error': 'Merged DataFrame is empty'})\n",
    "            # After building merged_df and metrics:\n",
    "            metric_result = compute_metrics(merged_df, source_columns, metrics, source_df)\n",
    "            table_results[f'{method}_{model_name}'] = {\n",
    "                'merged_df': merged_df,\n",
    "                'metrics': metrics,\n",
    "                'criteria': crit_obj,\n",
    "                **metric_result\n",
    "            }\n",
    "    out_json = {\n",
    "        'meta': meta,\n",
    "        'results': {}\n",
    "    }\n",
    "    for key, res in table_results.items():\n",
    "        out_json['results'][key] = {\n",
    "            'criteria': res.get('criteria'),\n",
    "            'metrics': res.get('metrics'),\n",
    "            'row_count': res.get('row_count'),\n",
    "            'column_consistency': res.get('column_consistency'),\n",
    "            'error_rate': res.get('error_rate'),\n",
    "            'avg_latency': res.get('avg_latency'),\n",
    "            'sum_tokens': res.get('sum_tokens'),\n",
    "            'accuracy': res.get('accuracy')\n",
    "        }\n",
    "    json_name = f\"{meta.get('id','')}_{meta.get('name','')}_metrics.json\"\n",
    "    json_path = os.path.join(output_folder, json_name)\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(out_json, f, indent=2)\n",
    "    print(f'Saved JSON metadata: {json_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
