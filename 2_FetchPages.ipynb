{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07c4455",
   "metadata": {},
   "source": [
    "# Stage 3: Fetch Pages\n",
    "\n",
    "\n",
    "This notebook implements the third stage of the pipeline: fetching pages using LLMs, recording metrics, and saving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74912bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f70d826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pagination folder: processing/1_pagination/20250831_091630\n"
     ]
    }
   ],
   "source": [
    "# Identify Latest Folder Under Pagination\n",
    "pagination_root = 'processing/1_pagination/'\n",
    "folders = [f for f in os.listdir(pagination_root) if os.path.isdir(os.path.join(pagination_root, f))]\n",
    "latest_folder = sorted(folders)[-1] if folders else None\n",
    "pagination_path = os.path.join(pagination_root, latest_folder) if latest_folder else None\n",
    "assert pagination_path and os.path.exists(pagination_path), 'No pagination folder found.'\n",
    "print('Using pagination folder:', pagination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed3ce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 tables.\n"
     ]
    }
   ],
   "source": [
    "# Load Pagination Criteria\n",
    "json_files = glob.glob(os.path.join(pagination_path, '*.json'))\n",
    "tables = []\n",
    "for jf in json_files:\n",
    "    with open(jf, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    tables.append({'path': jf, 'meta': obj['meta'], 'criteria': obj['pagination_criteria']})\n",
    "print(f'Loaded {len(tables)} tables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcb997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TableGenerator_JSON():\n",
    "    TEMPLATE = \"\"\"\n",
    "    List %s - as many as possible to fit into response.\n",
    "    The response will be formatted as JSON shown below.\n",
    "    Each element of the response will contain %d fields: %s.\n",
    "    Do not output any additional text that is not in JSON format.\n",
    "    %s\n",
    "    \n",
    "    \"\"\"   \n",
    "\n",
    "    def _norm_field(self, s):\n",
    "        s = s.lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\".\", \"\").replace(\",\",\"_\")\\\n",
    "                .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"','').replace(\"'\",\"\")\\\n",
    "                .replace(\"/\", \"\")\n",
    "        return re.sub('_+', '_', s)\n",
    "        \n",
    "    def generate_prompts(self, query, fields, paging: dict | None):\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "\n",
    "        num_fields = len(fields)\n",
    "        fields_json = []\n",
    "        fields = [f for f in fields]\n",
    "        for field in fields:\n",
    "            fields_json.append('\"%s\": \"%s\"' % ('_'.join(field.replace(\"-\", \" \").split()), field))\n",
    "        response_format = ', '.join(fields)\n",
    "        if paging:\n",
    "            paging_criteria = ('Only fetch the results where values for %s match: %s.' % (paging['field'], paging['value']))\n",
    "        else:\n",
    "            paging_criteria = ''\n",
    "        user_msg = self.TEMPLATE % (query, num_fields, response_format, paging_criteria)\n",
    "        return system_msg, user_msg\n",
    "\n",
    "    def parse_llm_response(self, response): \n",
    "        res = []\n",
    "        try:\n",
    "            if not response.startswith(\"[\") and \"[\" in response:\n",
    "                response = response[response.find(\"[\"):]\n",
    "\n",
    "            if not response.endswith(\"]\") and \"]\" in response:\n",
    "                response = response[:response.rfind(\"]\")+1]\n",
    "\n",
    "            if '[' not in response and ']' not in response and '{' in response and '}' in response:\n",
    "                response = '[' + response + ']'    \n",
    "\n",
    "            response_json = json.loads(response)\n",
    "\n",
    "            if isinstance(response_json, dict) and len(response_json.keys()) == 1:\n",
    "                response_json = list(response_json.values())[0]    \n",
    "        except:  \n",
    "            split_response = response.split(\"{\")\n",
    "            response_json = []\n",
    "            for s in split_response[1:]:\n",
    "                split_s = s.split(\"}\")\n",
    "                if len(split_s) > 1:\n",
    "                    content = split_s[0]\n",
    "                    attributes = content.split(\",\")\n",
    "                    elements = {}\n",
    "                    for attr in attributes:\n",
    "                        knv = attr.split(\":\")   \n",
    "                        if len(knv) > 1:\n",
    "                            parsed_k = \"%s\" % knv[0].replace('\"','').strip()\n",
    "                            parsed_v = \"%s\" % knv[1].replace('\"','').strip()\n",
    "                            elements[parsed_k] = parsed_v\n",
    "\n",
    "                    response_json.append(elements)  \n",
    "\n",
    "        df = pd.DataFrame.from_records(response_json) \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad1417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: None, Model: x-ai_grok-3-mini\n",
      "     Year  Latin  English   Winner\n",
      "0    1887     16        0    Latin\n",
      "1    1888     38        0    Latin\n",
      "2    1889      4       10  English\n",
      "3    1890      0       22  English\n",
      "4    1891     14       10    Latin\n",
      "..    ...    ...      ...      ...\n",
      "121  2008     36        0    Latin\n",
      "122  2009     27       16    Latin\n",
      "123  2010     54       12    Latin\n",
      "124  2011     50        0    Latin\n",
      "125  2012     44       15    Latin\n",
      "\n",
      "[126 rows x 4 columns]\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_104628/25_english_latin_rivalry_1887_2012_naive_x-ai_grok-3-mini.csv\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 0}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 4}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 5}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 6}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 10}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 12}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 22}, Model: x-ai_grok-3-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 23}, Model: x-ai_grok-3-mini\n",
      "   Year  Latin  English   Winner\n",
      "0  1899      0       23  English\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'English', 'value': 44}, Model: x-ai_grok-3-mini\n",
      "   Year  Latin  English   Winner\n",
      "0  1890      0       22  English\n",
      "1  1964     24       22    Latin\n",
      "   Year  Latin  English   Winner\n",
      "0  1901      6        5    Latin\n",
      "1  1904      5        5      Tie\n",
      "2  1907      0        5  English\n",
      "   Year  Latin  English   Winner\n",
      "0  1895      0        4  English\n",
      "   Year  Latin  English   Winner\n",
      "0  1889      4       10  English\n",
      "1  1891     14       10    Latin\n",
      "2  1906      0       10  English\n",
      "   Year  Latin  English   Winner\n",
      "0  1897      6       44  English\n",
      "   Year  Latin  English   Winner\n",
      "0  1892     10       12  English\n",
      "1  1934     13       12    Latin\n",
      "2  1940     19       12    Latin\n",
      "3  1952     30       12    Latin\n",
      "4  1956     19       12    Latin\n",
      "5  1965     24       12    Latin\n",
      "6  1968     33       12    Latin\n",
      "7  1995     36       12    Latin\n",
      "8  2005     36       12    Latin\n",
      "9  2010     54       12    Latin\n",
      "    Year  Latin  English Winner\n",
      "0   1887     16        0  Latin\n",
      "1   1888     38        0  Latin\n",
      "2   1894      4        0  Latin\n",
      "3   1898      5        0  Latin\n",
      "4   1900     12        0  Latin\n",
      "5   1902     25        0  Latin\n",
      "6   1903      5        0  Latin\n",
      "7   1910      9        0  Latin\n",
      "8   1918     28        0  Latin\n",
      "9   1924      7        0  Latin\n",
      "10  1931      6        0  Latin\n",
      "11  1936     13        0  Latin\n",
      "12  1941     19        0  Latin\n",
      "13  1948     19        0  Latin\n",
      "14  1949     19        0  Latin\n",
      "15  1966     40        0  Latin\n",
      "16  1967     14        0  Latin\n",
      "17  1969     40        0  Latin\n",
      "18  1971      6        0  Latin\n",
      "19  1972     40        0  Latin\n",
      "20  1974     42        0  Latin\n",
      "21  1977     23        0  Latin\n",
      "22  1978     34        0  Latin\n",
      "23  1979     22        0  Latin\n",
      "24  1980     20        0  Latin\n",
      "25  1984     43        0  Latin\n",
      "26  1991     19        0  Latin\n",
      "27  1992     41        0  Latin\n",
      "28  1994     41        0  Latin\n",
      "29  2000     14        0  Latin\n",
      "30  2004     44        0  Latin\n",
      "31  2006     14        0  Latin\n",
      "32  2008     36        0  Latin\n",
      "33  2011     50        0  Latin\n",
      "    Year  Latin  English   Winner\n",
      "0   1893      0        6  English\n",
      "1   1896      0        6  English\n",
      "2   1908      6        6      Tie\n",
      "3   1912      7        6    Latin\n",
      "4   1922     20        6    Latin\n",
      "5   1926      0        6  English\n",
      "6   1929     13        6    Latin\n",
      "7   1938      0        6  English\n",
      "8   1954     20        6    Latin\n",
      "9   1959     22        6    Latin\n",
      "10  1975     24        6    Latin\n",
      "11  1976     11        6    Latin\n",
      "12  1982     15        6    Latin\n",
      "13  1983     21        6    Latin\n",
      "14  1985     10        6    Latin\n",
      "15  1986     40        6    Latin\n",
      "16  1987     14        6    Latin\n",
      "17  1989     22        6    Latin\n",
      "18  1990     14        6    Latin\n",
      "19  1993      7        6    Latin\n",
      "20  1996     31        6    Latin\n",
      "21  1998     34        6    Latin\n",
      "22  2001     46        6    Latin\n",
      "23  2007     33        6    Latin\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_104628/25_english_latin_rivalry_1887_2012_statistical_x-ai_grok-3-mini.csv\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1901-1920'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1921-1940'}, Model: google_gemini-2.5-flash-lite\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1941-1960'}, Model: google_gemini-2.5-flash-lite\n",
      "    Year  Latin  English   Winner\n",
      "0   1901      6        5    Latin\n",
      "1   1902     25        0    Latin\n",
      "2   1903      5        0    Latin\n",
      "3   1904      5        5      Tie\n",
      "4   1905      0        0      Tie\n",
      "5   1906      0       10  English\n",
      "6   1907      0        5  English\n",
      "7   1908      6        6      Tie\n",
      "8   1909      0        0      Tie\n",
      "9   1910      9        0    Latin\n",
      "10  1911      0        0      Tie\n",
      "11  1912      7        6    Latin\n",
      "12  1913      0       21  English\n",
      "13  1914      3        3      Tie\n",
      "14  1915     14       13    Latin\n",
      "15  1916      0       13  English\n",
      "16  1917      0       13  English\n",
      "17  1918     28        0    Latin\n",
      "18  1919      0        0      Tie\n",
      "19  1920      6        7  English\n",
      "    Year  Latin  English   Winner\n",
      "0   1941     19        0    Latin\n",
      "1   1942      0       19  English\n",
      "2   1943      0        0      Tie\n",
      "3   1944      6       13  English\n",
      "4   1945      0        0      Tie\n",
      "5   1946      0       19  English\n",
      "6   1947     13       26  English\n",
      "7   1948     19        0    Latin\n",
      "8   1949     19        0    Latin\n",
      "9   1950     30       41  English\n",
      "10  1951      0       30  English\n",
      "11  1952     30       12    Latin\n",
      "12  1953     31       25    Latin\n",
      "13  1954     20        6    Latin\n",
      "14  1955     14       20  English\n",
      "15  1956     19       12    Latin\n",
      "16  1957     20       26  English\n",
      "17  1958     26       24    Latin\n",
      "18  1959     22        6    Latin\n",
      "19  1960     20       16    Latin\n",
      "    Year  Latin  English   Winner\n",
      "0   1921      0        0      Tie\n",
      "1   1922     20        6    Latin\n",
      "2   1923      0        0      Tie\n",
      "3   1924      7        0    Latin\n",
      "4   1925      0        7  English\n",
      "5   1926      0        6  English\n",
      "6   1927     13       20  English\n",
      "7   1928      0       18  English\n",
      "8   1929     13        6    Latin\n",
      "9   1930     13       14  English\n",
      "10  1931      6        0    Latin\n",
      "11  1932     18        7    Latin\n",
      "12  1933      7       20  English\n",
      "13  1934     13       12    Latin\n",
      "14  1935      0       14  English\n",
      "15  1936     13        0    Latin\n",
      "16  1937      0        0      Tie\n",
      "17  1938      0        6  English\n",
      "18  1939      0        0      Tie\n",
      "19  1940     19       12    Latin\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_104628/25_english_latin_rivalry_1887_2012_llm_google_gemini-2.5-flash-lite.csv\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1901-1920'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1921-1940'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1941-1960'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1961-1980'}, Model: deepseek_deepseek-chat-v3.1\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1981-2000'}, Model: deepseek_deepseek-chat-v3.1\n",
      "    Year  Latin  English   Winner\n",
      "0   1901      6        5    Latin\n",
      "1   1902     25        0    Latin\n",
      "2   1903      5        0    Latin\n",
      "3   1904      5        5      Tie\n",
      "4   1905      0        0      Tie\n",
      "5   1906      0       10  English\n",
      "6   1907      0        5  English\n",
      "7   1908      6        6      Tie\n",
      "8   1909      0        0      Tie\n",
      "9   1910      9        0    Latin\n",
      "10  1911      0        0      Tie\n",
      "11  1912      7        6    Latin\n",
      "12  1913      0       21  English\n",
      "13  1914      3        3      Tie\n",
      "14  1915     14       13    Latin\n",
      "15  1916      0       13  English\n",
      "16  1917      0       13  English\n",
      "17  1918     28        0    Latin\n",
      "18  1919      0        0      Tie\n",
      "19  1920      6        7  English\n",
      "    Year  Latin  English   Winner\n",
      "0   1981      2       14  English\n",
      "1   1982     15        6    Latin\n",
      "2   1983     21        6    Latin\n",
      "3   1984     43        0    Latin\n",
      "4   1985     10        6    Latin\n",
      "5   1986     40        6    Latin\n",
      "6   1987     14        6    Latin\n",
      "7   1988     20       13    Latin\n",
      "8   1989     22        6    Latin\n",
      "9   1990     14        6    Latin\n",
      "10  1991     19        0    Latin\n",
      "11  1992     41        0    Latin\n",
      "12  1993      7        6    Latin\n",
      "13  1994     41        0    Latin\n",
      "14  1995     36       12    Latin\n",
      "15  1996     31        6    Latin\n",
      "16  1997      6        8  English\n",
      "17  1998     34        6    Latin\n",
      "18  1999     42       20    Latin\n",
      "19  2000     14        0    Latin\n",
      "    Year  Latin  English   Winner\n",
      "0   1961      0       39  English\n",
      "1   1962      6       32  English\n",
      "2   1963     12       18  English\n",
      "3   1964     24       22    Latin\n",
      "4   1965     24       12    Latin\n",
      "5   1966     40        0    Latin\n",
      "6   1967     14        0    Latin\n",
      "7   1968     33       12    Latin\n",
      "8   1969     40        0    Latin\n",
      "9   1970     12        8    Latin\n",
      "10  1971      6        0    Latin\n",
      "11  1972     40        0    Latin\n",
      "12  1973     35        8    Latin\n",
      "13  1974     42        0    Latin\n",
      "14  1975     24        6    Latin\n",
      "15  1976     11        6    Latin\n",
      "16  1977     23        0    Latin\n",
      "17  1978     34        0    Latin\n",
      "18  1979     22        0    Latin\n",
      "19  1980     20        0    Latin\n",
      "    Year  Latin  English   Winner\n",
      "0   1941     19        0    Latin\n",
      "1   1942      0       19  English\n",
      "2   1943      0        0      Tie\n",
      "3   1944      6       13  English\n",
      "4   1945      0        0      Tie\n",
      "5   1946      0       19  English\n",
      "6   1947     13       26  English\n",
      "7   1948     19        0    Latin\n",
      "8   1949     19        0    Latin\n",
      "9   1950     30       41  English\n",
      "10  1951      0       30  English\n",
      "11  1952     30       12    Latin\n",
      "12  1953     31       25    Latin\n",
      "13  1954     20        6    Latin\n",
      "14  1955     14       20  English\n",
      "15  1956     19       12    Latin\n",
      "16  1957     20       26  English\n",
      "17  1958     26       24    Latin\n",
      "18  1959     22        6    Latin\n",
      "19  1960     20       16    Latin\n",
      "    Year  Latin  English   Winner\n",
      "0   1921      0        0      Tie\n",
      "1   1922     20        6    Latin\n",
      "2   1923      0        0      Tie\n",
      "3   1924      7        0    Latin\n",
      "4   1925      0        7  English\n",
      "5   1926      0        6  English\n",
      "6   1927     13       20  English\n",
      "7   1928      0       18  English\n",
      "8   1929     13        6    Latin\n",
      "9   1930     13       14  English\n",
      "10  1931      6        0    Latin\n",
      "11  1932     18        7    Latin\n",
      "12  1933      7       20  English\n",
      "13  1934     13       12    Latin\n",
      "14  1935      0       14  English\n",
      "15  1936     13        0    Latin\n",
      "16  1937      0        0      Tie\n",
      "17  1938      0        6  English\n",
      "18  1939      0        0      Tie\n",
      "19  1940     19       12    Latin\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_104628/25_english_latin_rivalry_1887_2012_llm_deepseek_deepseek-chat-v3.1.csv\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1901-1920'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1921-1940'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1941-1960'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1961-1980'}, Model: openai_gpt-4o-mini\n",
      "[FETCH] Table: english_latin_rivalry_1887_2012, Page: {'field': 'year', 'value': '1981-2000'}, Model: openai_gpt-4o-mini\n",
      "    Year  Latin  English   Winner\n",
      "0   1901      6        5    Latin\n",
      "1   1902     25        0    Latin\n",
      "2   1903      5        0    Latin\n",
      "3   1904      5        5      Tie\n",
      "4   1905      0        0      Tie\n",
      "5   1906      0       10  English\n",
      "6   1907      0        5  English\n",
      "7   1908      6        6      Tie\n",
      "8   1909      0        0      Tie\n",
      "9   1910      9        0    Latin\n",
      "10  1911      0        0      Tie\n",
      "11  1912      7        6    Latin\n",
      "12  1913      0       21  English\n",
      "13  1914      3        3      Tie\n",
      "14  1915     14       13    Latin\n",
      "15  1916      0       13  English\n",
      "16  1917      0       13  English\n",
      "17  1918     28        0    Latin\n",
      "18  1919      0        0      Tie\n",
      "19  1920      6        7  English\n",
      "    Year  Latin  English   Winner\n",
      "0   1941     19        0    Latin\n",
      "1   1942      0       19  English\n",
      "2   1943      0        0      Tie\n",
      "3   1944      6       13  English\n",
      "4   1945      0        0      Tie\n",
      "5   1946      0       19  English\n",
      "6   1947     13       26  English\n",
      "7   1948     19        0    Latin\n",
      "8   1949     19        0    Latin\n",
      "9   1950     30       41  English\n",
      "10  1951      0       30  English\n",
      "11  1952     30       12    Latin\n",
      "12  1953     31       25    Latin\n",
      "13  1954     20        6    Latin\n",
      "14  1955     14       20  English\n",
      "15  1956     19       12    Latin\n",
      "16  1957     20       26  English\n",
      "17  1958     26       24    Latin\n",
      "18  1959     22        6    Latin\n",
      "19  1960     20       16    Latin\n",
      "    Year  Latin  English   Winner\n",
      "0   1921      0        0      Tie\n",
      "1   1922     20        6    Latin\n",
      "2   1923      0        0      Tie\n",
      "3   1924      7        0    Latin\n",
      "4   1925      0        7  English\n",
      "5   1926      0        6  English\n",
      "6   1927     13       20  English\n",
      "7   1928      0       18  English\n",
      "8   1929     13        6    Latin\n",
      "9   1930     13       14  English\n",
      "10  1931      6        0    Latin\n",
      "11  1932     18        7    Latin\n",
      "12  1933      7       20  English\n",
      "13  1934     13       12    Latin\n",
      "14  1935      0       14  English\n",
      "15  1936     13        0    Latin\n",
      "16  1937      0        0      Tie\n",
      "17  1938      0        6  English\n",
      "18  1939      0        0      Tie\n",
      "19  1940     19       12    Latin\n",
      "    Year  Latin  English   Winner\n",
      "0   1961      0       39  English\n",
      "1   1962      6       32  English\n",
      "2   1963     12       18  English\n",
      "3   1964     24       22    Latin\n",
      "4   1965     24       12    Latin\n",
      "5   1966     40        0    Latin\n",
      "6   1967     14        0    Latin\n",
      "7   1968     33       12    Latin\n",
      "8   1969     40        0    Latin\n",
      "9   1970     12        8    Latin\n",
      "10  1971      6        0    Latin\n",
      "11  1972     40        0    Latin\n",
      "12  1973     35        8    Latin\n",
      "13  1974     42        0    Latin\n",
      "14  1975     24        6    Latin\n",
      "15  1976     11        6    Latin\n",
      "16  1977     23        0    Latin\n",
      "17  1978     34        0    Latin\n",
      "18  1979     22        0    Latin\n",
      "19  1980     20        0    Latin\n",
      "    Year  Latin  English   Winner\n",
      "0   1981      2       14  English\n",
      "1   1982     15        6    Latin\n",
      "2   1983     21        6    Latin\n",
      "3   1984     43        0    Latin\n",
      "4   1985     10        6    Latin\n",
      "5   1986     40        6    Latin\n",
      "6   1987     14        6    Latin\n",
      "7   1988     20       13    Latin\n",
      "8   1989     22        6    Latin\n",
      "9   1990     14        6    Latin\n",
      "10  1991     19        0    Latin\n",
      "11  1992     41        0    Latin\n",
      "12  1993      7        6    Latin\n",
      "13  1994     41        0    Latin\n",
      "14  1995     36       12    Latin\n",
      "15  1996     31        6    Latin\n",
      "16  1997      6        8  English\n",
      "17  1998     34        6    Latin\n",
      "18  1999     42       20    Latin\n",
      "19  2000     14        0    Latin\n",
      "Saved merged CSV: processing/2_fetched_pages/20250831_104628/25_english_latin_rivalry_1887_2012_llm_openai_gpt-4o-mini.csv\n",
      "Saved JSON metadata: processing/2_fetched_pages/20250831_104628/25_english_latin_rivalry_1887_2012_metrics.json\n",
      "[FETCH] Table: australia_demographics_1900_2010, Page: None, Model: x-ai_grok-3-mini\n"
     ]
    }
   ],
   "source": [
    "# Fetch Pages Using LLM and Record Metrics (Parallelized)\n",
    "from io import StringIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "PROVIDE_SOURCE_TABLE = True  # If True, include source table as CSV in the prompt\n",
    "LLM_TIMEOUT = 30  # seconds\n",
    "LLM_MODEL = 'x-ai/grok-3-mini'  # Use this model for all criteria\n",
    "\n",
    "OPENROUTER_API_KEY = os.environ.get('OPENROUTER_API_KEY', '')\n",
    "\n",
    "output_root = 'processing/2_fetched_pages/'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_folder = os.path.join(output_root, timestamp)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def compute_metrics(merged_df, source_columns, metrics, source_df):\n",
    "    row_count = len(merged_df) if merged_df is not None else 0\n",
    "    col_consistency = (set(merged_df.columns) == set(source_columns)) if source_columns and not merged_df.empty else None\n",
    "    error_count = sum(1 for m in metrics if m['error'])\n",
    "    total_pages = len(metrics)\n",
    "    error_rate = error_count / total_pages if total_pages > 0 else None\n",
    "    latencies = [m['latency'] for m in metrics if m['latency'] is not None]\n",
    "    avg_latency = sum(latencies) / len(latencies) if latencies else None\n",
    "    token_counts = [m['usage'].get('total_tokens', 0) for m in metrics if m['usage'] and 'total_tokens' in m['usage']]\n",
    "    sum_tokens = sum(token_counts) if token_counts else None\n",
    "    acc_metrics = None\n",
    "    if source_df is not None and not merged_df.empty:\n",
    "        acc_metrics = accuracy_metrics(merged_df, source_df)\n",
    "    return {\n",
    "        'row_count': row_count,\n",
    "        'column_consistency': col_consistency,\n",
    "        'error_rate': error_rate,\n",
    "        'avg_latency': avg_latency,\n",
    "        'sum_tokens': sum_tokens,\n",
    "        'accuracy': acc_metrics\n",
    "    }\n",
    "\n",
    "table_generator = TableGenerator_JSON()\n",
    "\n",
    "def fetch_page_llm(prompt, model, api_key, system_msg: str = ''):\n",
    "    url = 'https://openrouter.ai/api/v1/chat/completions'\n",
    "    headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}\n",
    "    payload = {\n",
    "        'model': model,\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': system_msg},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'max_tokens': 100000\n",
    "    }\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=LLM_TIMEOUT)\n",
    "        latency = time.time() - start\n",
    "        resp.raise_for_status()\n",
    "        result = resp.json()\n",
    "        content = result['choices'][0]['message']['content'] if 'choices' in result else ''\n",
    "        content = table_generator.parse_llm_response(content)\n",
    "        usage = result.get('usage', {})\n",
    "        return content, latency, usage, None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to fetch page: {e}\")\n",
    "        return None, None, None, str(e)\n",
    "\n",
    "# Accuracy metric functions (from old/2_Metrics_calculation.ipynb)\n",
    "def accuracy_metrics(merged_df, source_df):\n",
    "    # Only compare columns present in both\n",
    "    common_cols = [col for col in source_df.columns if col in merged_df.columns]\n",
    "    # Cast all columns to string for comparison\n",
    "    src = source_df[common_cols].drop_duplicates().reset_index(drop=True).astype(str)\n",
    "    pred = merged_df[common_cols].drop_duplicates().reset_index(drop=True).astype(str)\n",
    "    # Row-level accuracy: fraction of source rows present in merged\n",
    "    correct_rows = src.merge(pred, how='inner').shape[0]\n",
    "    total_rows = src.shape[0]\n",
    "    row_recall = correct_rows / total_rows if total_rows > 0 else None\n",
    "    # Precision: fraction of merged rows that are correct\n",
    "    correct_pred_rows = pred.merge(src, how='inner').shape[0]\n",
    "    total_pred_rows = pred.shape[0]\n",
    "    row_precision = correct_pred_rows / total_pred_rows if total_pred_rows > 0 else None\n",
    "    # F1 score\n",
    "    if row_precision is not None and row_recall is not None and (row_precision + row_recall) > 0:\n",
    "        row_f1 = 2 * row_precision * row_recall / (row_precision + row_recall)\n",
    "    else:\n",
    "        row_f1 = None\n",
    "    return {\n",
    "        'row_recall': row_recall,\n",
    "        'row_precision': row_precision,\n",
    "        'row_f1': row_f1\n",
    "    }\n",
    "\n",
    "def fetch_page_task(args):\n",
    "    # args: (meta, crit_obj, page_key, model_name, source_columns, source_csv_str)\n",
    "    meta, crit_obj, page_key, model_name, source_columns, source_csv_str = args\n",
    "    page_content = {'field': crit_obj.get('criteria',''), 'value': page_key} if page_key != 'ALL' else None\n",
    "    system_msg, user_msg = table_generator.generate_prompts(meta.get('query_without_cutoff'), source_columns, page_content)\n",
    "    if PROVIDE_SOURCE_TABLE and source_csv_str:\n",
    "        user_msg += f\"\\n\\nSource table as CSV:\\n{source_csv_str}\"\n",
    "    print(f\"[FETCH] Table: {meta.get('name')}, Page: {page_content}, Model: {model_name}\")\n",
    "    content, latency, usage, error = fetch_page_llm(user_msg, LLM_MODEL, OPENROUTER_API_KEY, system_msg)\n",
    "    return {\n",
    "        'content': content,\n",
    "        'latency': latency,\n",
    "        'usage': usage,\n",
    "        'error': error,\n",
    "        'page_key': page_key\n",
    "    }\n",
    "\n",
    "for table in tables:\n",
    "    meta = table['meta']\n",
    "    criteria = table['criteria']\n",
    "    source_csv_path = meta.get('source_file')\n",
    "    source_csv_str = ''\n",
    "    source_columns = None\n",
    "    source_df = None\n",
    "    if PROVIDE_SOURCE_TABLE and source_csv_path and os.path.exists(source_csv_path):\n",
    "        try:\n",
    "            source_df = pd.read_csv(source_csv_path)\n",
    "            source_csv_str = source_df.to_csv(index=False)\n",
    "            source_columns = list(source_df.columns)\n",
    "            csv_path = os.path.join(output_folder, meta['file'])\n",
    "            source_df.to_csv(csv_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f'[WARNING] Could not load source CSV for {meta.get(\"name\")}: {e}')\n",
    "            source_csv_str = ''\n",
    "            source_columns = None\n",
    "    table_results = {}\n",
    "    for method, crit in criteria.items():\n",
    "        # For llm, collect top recommendation from all listed models\n",
    "        if method == 'llm':\n",
    "            criteria_list = []\n",
    "            for model_name, model_criteria in crit.items():\n",
    "                if not model_criteria:\n",
    "                    continue\n",
    "                top_crit = model_criteria[0] if isinstance(model_criteria, list) else model_criteria\n",
    "                criteria_list.append((model_name, top_crit))\n",
    "        else:\n",
    "            criteria_list = [(LLM_MODEL, crit)]\n",
    "        for model_name, crit_obj in criteria_list:\n",
    "            model_name = model_name.replace('/', '_')\n",
    "            pages = crit_obj.get('pages', [])\n",
    "            if source_columns:\n",
    "                merged_df = pd.DataFrame(columns=source_columns)\n",
    "            else:\n",
    "                merged_df = pd.DataFrame()\n",
    "            metrics = []\n",
    "            # Prepare tasks for all pages\n",
    "            tasks = [(meta, crit_obj, page_key, model_name, source_columns, source_csv_str) for page_key in pages]\n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                future_to_page = {executor.submit(fetch_page_task, t): t[2] for t in tasks}\n",
    "                for future in as_completed(future_to_page):\n",
    "                    page_key = future_to_page[future]\n",
    "                    try:\n",
    "                        res = future.result()\n",
    "                        metrics.append({'latency': res['latency'], 'usage': res['usage'], 'error': res['error']})\n",
    "                        if res['content'] is not None:\n",
    "                            try:\n",
    "                                df_page = res['content']\n",
    "                                merged_df = pd.concat([merged_df, df_page], ignore_index=True)\n",
    "                            except Exception:\n",
    "                                print(f\"[WARNING] Skipping non-CSV response for table {meta.get('name')}, page {page_key}, model {model_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Exception in future for page {page_key}: {e}\")\n",
    "            if merged_df is not None and not merged_df.empty:\n",
    "                csv_name = f\"{meta.get('id','')}_{meta.get('name','')}_{method}_{model_name}.csv\"\n",
    "                csv_path = os.path.join(output_folder, csv_name)\n",
    "                merged_df.to_csv(csv_path, index=False)\n",
    "                print(f'Saved merged CSV: {csv_path}')\n",
    "            else:\n",
    "                raise ValueError(f'Merged DataFrame is empty for table {meta.get(\"name\")}, method {method}, model {model_name}')\n",
    "            # After building merged_df and metrics:\n",
    "            metric_result = compute_metrics(merged_df, source_columns, metrics, source_df)\n",
    "            table_results[f'{method}_{model_name}'] = {\n",
    "                'merged_df': merged_df,\n",
    "                'metrics': metrics,\n",
    "                'criteria': crit_obj,\n",
    "                **metric_result\n",
    "            }\n",
    "    out_json = {\n",
    "        'meta': meta,\n",
    "        'results': {}\n",
    "    }\n",
    "    for key, res in table_results.items():\n",
    "        out_json['results'][key] = {\n",
    "            'criteria': res.get('criteria'),\n",
    "            'metrics': res.get('metrics'),\n",
    "            'row_count': res.get('row_count'),\n",
    "            'column_consistency': res.get('column_consistency'),\n",
    "            'error_rate': res.get('error_rate'),\n",
    "            'avg_latency': res.get('avg_latency'),\n",
    "            'sum_tokens': res.get('sum_tokens'),\n",
    "            'accuracy': res.get('accuracy')\n",
    "        }\n",
    "    json_name = f\"{meta.get('id','')}_{meta.get('name','')}_metrics.json\"\n",
    "    json_path = os.path.join(output_folder, json_name)\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(out_json, f, indent=2)\n",
    "    print(f'Saved JSON metadata: {json_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
