{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6167645",
   "metadata": {},
   "source": [
    "# Page Fetching for Pagination Strategies\n",
    "\n",
    "This notebook loads pagination plans from Stage 1 and executes the actual data fetching for each strategy.\n",
    "\n",
    "## Strategies:\n",
    "1. **Full Table** - Single query for entire table\n",
    "2. **Row by Row** - Fetch each row individually using key values\n",
    "3. **Attribute-based** - Fetch pages by partition values\n",
    "4. **Classic Pagination** - Offset-based iterative fetching\n",
    "5. **Range-based** - Fetch by defined ranges\n",
    "\n",
    "## Output:\n",
    "- CSV files: Aggregated table data (for metrics calculation)\n",
    "- JSON files: Detailed execution logs with per-page metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a37742",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dea23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy directory: /Users/bef/Desktop/TablePagination/processing/1_strategy/20251004_235228\n",
      "Ground truth directory: /Users/bef/Desktop/TablePagination/processing/0_data/20251004_213355\n",
      "Output base: /Users/bef/Desktop/TablePagination/processing/2_fetching\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openai\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# MAX_TABLES: Limit number of tables to process (None for all)\n",
    "MAX_TABLES = 5\n",
    "\n",
    "# MAX_PAGES: Limit pages per table per strategy (None for all, useful for testing)\n",
    "MAX_PAGES = None\n",
    "\n",
    "# PARALLEL_STRATEGIES: Run strategies in parallel per table\n",
    "PARALLEL_STRATEGIES = True\n",
    "\n",
    "# MAX_WORKERS: Number of parallel strategy executions per table\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "# MAX_RETRIES: Number of retries for failed LLM calls\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# MAX_PAGINATION_PAGES: Failsafe for classic_pagination\n",
    "MAX_PAGINATION_PAGES = 50\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "PROCESSING_ROOT = ROOT / 'processing'\n",
    "STRATEGY_DIR = PROCESSING_ROOT / '1_strategy'\n",
    "OUTPUT_ROOT = PROCESSING_ROOT / '2_fetching'\n",
    "\n",
    "# Find the most recent strategy directory\n",
    "strategy_subdirs = sorted([d for d in STRATEGY_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not strategy_subdirs:\n",
    "    raise FileNotFoundError(f\"No strategy data found in {STRATEGY_DIR}\")\n",
    "\n",
    "LATEST_STRATEGY_DIR = strategy_subdirs[0]\n",
    "\n",
    "# Find ground truth data directory\n",
    "DATA_DIR = PROCESSING_ROOT / '0_data'\n",
    "data_subdirs = sorted([d for d in DATA_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not data_subdirs:\n",
    "    raise FileNotFoundError(f\"No ground truth data found in {DATA_DIR}\")\n",
    "\n",
    "LATEST_DATA_DIR = data_subdirs[0]\n",
    "\n",
    "print('Strategy directory:', LATEST_STRATEGY_DIR.resolve())\n",
    "print('Ground truth directory:', LATEST_DATA_DIR.resolve())\n",
    "print('Output base:', OUTPUT_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27259394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API key loaded from api_key.txt\n",
      "Using model: x-ai/grok-code-fast-1\n"
     ]
    }
   ],
   "source": [
    "# Configure OpenRouter\n",
    "# Read API key from file\n",
    "api_key_file = ROOT / 'api_key.txt'\n",
    "if not api_key_file.exists():\n",
    "    raise ValueError('No API key found. Please create api_key.txt or set OPENROUTER_API_KEY environment variable')\n",
    "with open(api_key_file, 'r') as f:\n",
    "    OPENROUTER_API_KEY = f.read().strip()\n",
    "print('✓ API key loaded from api_key.txt')\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "MODEL = 'x-ai/grok-code-fast-1'\n",
    "print(f'Using model: {MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429e4b2",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63accf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_with_metrics(prompt: str, retry_count: int = 0) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call LLM and return response with detailed metrics.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - success: bool\n",
    "    - response: str (if successful)\n",
    "    - error: str (if failed)\n",
    "    - metrics: dict with latency, tokens, prompt_length, response_length, etc.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    prompt_length = len(prompt)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Check if content is None\n",
    "        if content is None:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'Response content is None',\n",
    "                'metrics': {\n",
    "                    'latency': round(latency, 3),\n",
    "                    'prompt_length': prompt_length,\n",
    "                    'retry_count': retry_count,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        content = content.strip()\n",
    "        response_length = len(content)\n",
    "        \n",
    "        # Extract token usage\n",
    "        usage = response.usage\n",
    "        prompt_tokens = usage.prompt_tokens if usage else 0\n",
    "        completion_tokens = usage.completion_tokens if usage else 0\n",
    "        total_tokens = usage.total_tokens if usage else 0\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'response': content,\n",
    "            'metrics': {\n",
    "                'latency': round(latency, 3),\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'total_tokens': total_tokens,\n",
    "                'prompt_length': prompt_length,\n",
    "                'response_length': response_length,\n",
    "                'retry_count': retry_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        latency = time.time() - start_time\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'metrics': {\n",
    "                'latency': round(latency, 3),\n",
    "                'prompt_length': prompt_length,\n",
    "                'retry_count': retry_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def call_llm_with_retries(prompt: str, max_retries: int = MAX_RETRIES) -> Dict[str, Any]:\n",
    "    \"\"\"Call LLM with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        result = call_llm_with_metrics(prompt, retry_count=attempt)\n",
    "        if result['success']:\n",
    "            return result\n",
    "        print(f\"  Retry {attempt + 1}/{max_retries} after error: {result['error']}\")\n",
    "        time.sleep(1)  # Brief delay between retries\n",
    "    \n",
    "    return result  # Return last failed attempt\n",
    "\n",
    "\n",
    "def parse_json_from_response(response: str) -> Tuple[Optional[List[Dict]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract and parse JSON from LLM response.\n",
    "    \n",
    "    Returns:\n",
    "    - parsed_data: List of dicts or None\n",
    "    - parse_metrics: dict with parse_success, json_start_pos, json_end_pos\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'parse_success': False,\n",
    "        'json_start_pos': -1,\n",
    "        'json_end_pos': -1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Find JSON array in response\n",
    "        start_pos = response.find('[')\n",
    "        end_pos = response.rfind(']')\n",
    "        \n",
    "        if start_pos == -1 or end_pos == -1:\n",
    "            return None, metrics\n",
    "        \n",
    "        json_str = response[start_pos:end_pos + 1]\n",
    "        metrics['json_start_pos'] = start_pos\n",
    "        metrics['json_end_pos'] = end_pos + 1\n",
    "        \n",
    "        parsed = json.loads(json_str)\n",
    "        \n",
    "        if isinstance(parsed, list):\n",
    "            metrics['parse_success'] = True\n",
    "            return parsed, metrics\n",
    "        \n",
    "        return None, metrics\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        metrics['parse_error'] = str(e)\n",
    "        return None, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2aaafa",
   "metadata": {},
   "source": [
    "## Strategy 1: Full Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e49a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_table(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch entire table in one query with example row.\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Load ground truth to get example row\n",
    "    gt_file = LATEST_DATA_DIR / f\"{table_id}.csv\"\n",
    "    df_ref = pd.read_csv(gt_file)\n",
    "    example_row = df_ref.iloc[0]\n",
    "    \n",
    "    # Format example in JSON\n",
    "    example_json = ', '.join([f'\"{col}\": \"{example_row[col]}\"' for col in columns])\n",
    "    \n",
    "    prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to create a table with detailed information about {table_title}.\n",
    "Return all rows from the table.\n",
    "\n",
    "RESPONSE FORMAT (JSON array):\n",
    "[{{\n",
    "    {example_json}\n",
    "}}]\"\"\"\n",
    "    \n",
    "    print(f\"  Fetching full table...\")\n",
    "    \n",
    "    result = call_llm_with_retries(prompt)\n",
    "    \n",
    "    page_data = {\n",
    "        'page_number': 1,\n",
    "        'prompt': prompt,\n",
    "        'raw_response': result.get('response', ''),\n",
    "        'error': result.get('error'),\n",
    "        **result['metrics']\n",
    "    }\n",
    "    \n",
    "    if result['success']:\n",
    "        parsed_data, parse_metrics = parse_json_from_response(result['response'])\n",
    "        page_data.update(parse_metrics)\n",
    "        page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "        page_data['parsed_data'] = parsed_data\n",
    "    else:\n",
    "        page_data['parse_success'] = False\n",
    "        page_data['rows_returned'] = 0\n",
    "        page_data['parsed_data'] = []\n",
    "    \n",
    "    return {\n",
    "        'pages': [page_data],\n",
    "        'all_rows': page_data['parsed_data']\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b578b05",
   "metadata": {},
   "source": [
    "## Strategy 2: Row by Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a971c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_row_by_row(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch each row individually using key values from plan with example.\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    key_columns = config['key_columns']\n",
    "    key_values = config['key_values']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Load ground truth to get example row\n",
    "    gt_file = LATEST_DATA_DIR / f\"{table_id}.csv\"\n",
    "    df_ref = pd.read_csv(gt_file)\n",
    "    example_row = df_ref.iloc[0]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit if set\n",
    "    keys_to_fetch = key_values[:MAX_PAGES] if MAX_PAGES else key_values\n",
    "    total_keys = len(key_values)\n",
    "    \n",
    "    if MAX_PAGES and len(key_values) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} rows out of {total_keys}\")\n",
    "    \n",
    "    for page_num, key_combo in enumerate(keys_to_fetch, 1):\n",
    "        # Build WHERE clause for this row\n",
    "        where_clauses = [f\"{k}={key_combo.get(k.lower().replace(' ', '_'), 'UNKNOWN')}\" \n",
    "                        for k in key_columns]\n",
    "        where_str = \" and \".join(where_clauses)\n",
    "        \n",
    "        # Format example with current key values\n",
    "        example_json_parts = []\n",
    "        for col in columns:\n",
    "            col_norm = col.lower().replace(' ', '_')\n",
    "            if col_norm in key_combo:\n",
    "                example_json_parts.append(f'\"{col}\": \"{key_combo[col_norm]}\"')\n",
    "            else:\n",
    "                example_json_parts.append(f'\"{col}\": \"{example_row[col]}\"')\n",
    "        example_json = ', '.join(example_json_parts)\n",
    "        \n",
    "        prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to retrieve a specific row from a table about {table_title}.\n",
    "Return the row where {where_str}.\n",
    "\n",
    "RESPONSE FORMAT (JSON array with single object):\n",
    "[{{\n",
    "    {example_json}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching row {page_num}/{len(keys_to_fetch)}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(prompt)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'key_values': key_combo,\n",
    "            'prompt': prompt,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_from_response(result['response'])\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6174b1d",
   "metadata": {},
   "source": [
    "## Strategy 3: Attribute-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6eebb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_attribute_based(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch pages by partition values with example.\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    partition_column = config['partition_column']\n",
    "    partition_values = config['partition_values']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Load ground truth to get example row\n",
    "    gt_file = LATEST_DATA_DIR / f\"{table_id}.csv\"\n",
    "    df_ref = pd.read_csv(gt_file)\n",
    "    example_row = df_ref.iloc[0]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit\n",
    "    values_to_fetch = partition_values[:MAX_PAGES] if MAX_PAGES else partition_values\n",
    "    total_values = len(partition_values)\n",
    "    \n",
    "    if MAX_PAGES and len(partition_values) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} partitions out of {total_values}\")\n",
    "    \n",
    "    for page_num, value in enumerate(values_to_fetch, 1):\n",
    "        # Format example\n",
    "        example_json = ', '.join([f'\"{col}\": \"{example_row[col]}\"' for col in columns])\n",
    "        \n",
    "        prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to retrieve rows from a table about {table_title}.\n",
    "Return all rows where {partition_column} = {value}.\n",
    "\n",
    "RESPONSE FORMAT (JSON array):\n",
    "[{{\n",
    "    {example_json}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching partition {page_num}/{len(values_to_fetch)}: {partition_column}={value}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(prompt)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'partition_value': value,\n",
    "            'prompt': prompt,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_from_response(result['response'])\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9e075",
   "metadata": {},
   "source": [
    "## Strategy 4: Classic Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "921ace95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_classic_pagination(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Iterative offset-based pagination with example.\n",
    "    Stops when LLM says \"no more rows\" or hits MAX_PAGINATION_PAGES failsafe.\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    page_size = config['page_size']\n",
    "    sort_order = config['sort_order']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Load ground truth to get example row\n",
    "    gt_file = LATEST_DATA_DIR / f\"{table_id}.csv\"\n",
    "    df_ref = pd.read_csv(gt_file)\n",
    "    example_row = df_ref.iloc[0]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    offset = 0\n",
    "    page_num = 0\n",
    "    \n",
    "    max_pages = min(MAX_PAGES, MAX_PAGINATION_PAGES) if MAX_PAGES else MAX_PAGINATION_PAGES\n",
    "    \n",
    "    while page_num < max_pages:\n",
    "        page_num += 1\n",
    "        \n",
    "        # Format example\n",
    "        example_json = ', '.join([f'\"{col}\": \"{example_row[col]}\"' for col in columns])\n",
    "        \n",
    "        prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to retrieve page {page_num} from a table about {table_title}.\n",
    "Return rows {offset + 1} to {offset + page_size}, sorted by {', '.join(sort_order)}.\n",
    "\n",
    "If there are no more rows at this offset, respond with exactly: \"no more rows\"\n",
    "\n",
    "RESPONSE FORMAT (JSON array):\n",
    "[{{\n",
    "    {example_json}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching page {page_num} (offset {offset}, size {page_size})...\")\n",
    "        \n",
    "        result = call_llm_with_retries(prompt)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'offset': offset,\n",
    "            'page_size': page_size,\n",
    "            'prompt': prompt,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        # Check for stop condition\n",
    "        if result['success']:\n",
    "            response_lower = result['response'].lower()\n",
    "            if 'no more rows' in response_lower and '[' not in result['response']:\n",
    "                print(f\"  Stopping: LLM indicated no more rows\")\n",
    "                page_data['parse_success'] = True\n",
    "                page_data['rows_returned'] = 0\n",
    "                page_data['parsed_data'] = []\n",
    "                page_data['stop_reason'] = 'no_more_rows'\n",
    "                pages.append(page_data)\n",
    "                break\n",
    "            \n",
    "            parsed_data, parse_metrics = parse_json_from_response(result['response'])\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "                offset += page_size\n",
    "            else:\n",
    "                # Empty result, assume end\n",
    "                print(f\"  Stopping: Empty result\")\n",
    "                page_data['stop_reason'] = 'empty_result'\n",
    "                pages.append(page_data)\n",
    "                break\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    if page_num >= max_pages:\n",
    "        print(f\"  Stopping: Hit max pages limit ({max_pages})\")\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfb96f",
   "metadata": {},
   "source": [
    "## Strategy 5: Range-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17c11e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_range_based(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch pages by defined ranges with example.\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    partition_column = config['partition_column']\n",
    "    ranges = config['ranges']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Load ground truth to get example row\n",
    "    gt_file = LATEST_DATA_DIR / f\"{table_id}.csv\"\n",
    "    df_ref = pd.read_csv(gt_file)\n",
    "    example_row = df_ref.iloc[0]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit\n",
    "    ranges_to_fetch = ranges[:MAX_PAGES] if MAX_PAGES else ranges\n",
    "    total_ranges = len(ranges)\n",
    "    \n",
    "    if MAX_PAGES and len(ranges) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} ranges out of {total_ranges}\")\n",
    "    \n",
    "    for page_num, range_spec in enumerate(ranges_to_fetch, 1):\n",
    "        gte = range_spec.get('gte', '')\n",
    "        lt = range_spec.get('lt', '')\n",
    "        \n",
    "        # Format example\n",
    "        example_json = ', '.join([f'\"{col}\": \"{example_row[col]}\"' for col in columns])\n",
    "        \n",
    "        prompt = f\"\"\"You are a retriever of facts.\n",
    "We want to retrieve rows from a table about {table_title}.\n",
    "Return all rows where {partition_column} >= {gte} and {partition_column} < {lt}.\n",
    "\n",
    "RESPONSE FORMAT (JSON array):\n",
    "[{{\n",
    "    {example_json}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching range {page_num}/{len(ranges_to_fetch)}: {partition_column} [{gte}, {lt})...\")\n",
    "        \n",
    "        result = call_llm_with_retries(prompt)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'range': range_spec,\n",
    "            'prompt': prompt,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_from_response(result['response'])\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17e489",
   "metadata": {},
   "source": [
    "## Main Execution: Process All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07b59480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 strategy-table combinations\n",
      "\n",
      "Filtering to tables with plans for all 1 strategies:\n",
      "  Total unique tables: 48\n",
      "  Tables with all strategies: 48\n",
      "  Filtered to 48 strategy-table combinations\n",
      "Limited to 5 combinations for 5 tables\n",
      "Processing 5 strategy-table combinations...\n"
     ]
    }
   ],
   "source": [
    "# Strategy function mapping\n",
    "STRATEGY_FUNCTIONS = {\n",
    "    'full_table': fetch_full_table,\n",
    "    'row_by_row': fetch_row_by_row,\n",
    "    'attribute_based': fetch_attribute_based,\n",
    "    'classic_pagination': fetch_classic_pagination,\n",
    "    'range_based': fetch_range_based\n",
    "}\n",
    "\n",
    "# Load all strategies\n",
    "strategies_to_process = []\n",
    "for strategy_name in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_dir = LATEST_STRATEGY_DIR / strategy_name\n",
    "    if not strategy_dir.exists():\n",
    "        print(f\"Skipping {strategy_name}: directory not found\")\n",
    "        continue\n",
    "    \n",
    "    json_files = sorted(strategy_dir.glob('*.json'))\n",
    "    # Filter out error files\n",
    "    json_files = [f for f in json_files if not f.name.startswith('_')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            plan = json.load(f)\n",
    "        strategies_to_process.append((strategy_name, plan))\n",
    "\n",
    "print(f\"Found {len(strategies_to_process)} strategy-table combinations\")\n",
    "\n",
    "# Filter to only tables that have plans for ALL strategies (apples-to-apples comparison)\n",
    "from collections import Counter\n",
    "all_strategies = list(STRATEGY_FUNCTIONS.keys())\n",
    "table_strategy_counts = Counter(plan['table_id'] for _, plan in strategies_to_process)\n",
    "tables_with_all_strategies = [table_id for table_id, count in table_strategy_counts.items() \n",
    "                               if count == len(all_strategies)]\n",
    "\n",
    "print(f\"\\nFiltering to tables with plans for all {len(all_strategies)} strategies:\")\n",
    "print(f\"  Total unique tables: {len(table_strategy_counts)}\")\n",
    "print(f\"  Tables with all strategies: {len(tables_with_all_strategies)}\")\n",
    "\n",
    "strategies_to_process = [(s, p) for s, p in strategies_to_process \n",
    "                         if p['table_id'] in tables_with_all_strategies]\n",
    "print(f\"  Filtered to {len(strategies_to_process)} strategy-table combinations\")\n",
    "\n",
    "# Apply MAX_TABLES limit across all strategies\n",
    "if MAX_TABLES:\n",
    "    # Group by table_id to ensure we process complete sets\n",
    "    table_ids = list(set(p['table_id'] for _, p in strategies_to_process))[:MAX_TABLES]\n",
    "    strategies_to_process = [(s, p) for s, p in strategies_to_process if p['table_id'] in table_ids]\n",
    "    print(f\"Limited to {len(strategies_to_process)} combinations for {len(table_ids)} tables\")\n",
    "\n",
    "print(f\"Processing {len(strategies_to_process)} strategy-table combinations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e75e9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: processing/2_fetching/20251005_002153\n",
      "\n",
      "Grouped into 5 tables with strategies\n",
      "\n",
      "======================================================================\n",
      "[1/5] Processing table: 16_curling_teams_women_2013_2014\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✓ full_table/16_curling_teams_women_2013_2014: 21 rows, 2945 tokens, 13.07s\n",
      "\n",
      "======================================================================\n",
      "[2/5] Processing table: 20_new_zealand_demographics_1921_2011\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✓ full_table/16_curling_teams_women_2013_2014: 21 rows, 2945 tokens, 13.07s\n",
      "\n",
      "======================================================================\n",
      "[2/5] Processing table: 20_new_zealand_demographics_1921_2011\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✗ full_table/20_new_zealand_demographics_1921_2011: Error: object of type 'NoneType' has no len()\n",
      "\n",
      "======================================================================\n",
      "[3/5] Processing table: 36_south_cambridgeshire_district_council_1973_2012\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✗ full_table/20_new_zealand_demographics_1921_2011: Error: object of type 'NoneType' has no len()\n",
      "\n",
      "======================================================================\n",
      "[3/5] Processing table: 36_south_cambridgeshire_district_council_1973_2012\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✓ full_table/36_south_cambridgeshire_district_council_1973_2012: 1 rows, 6944 tokens, 37.62s\n",
      "\n",
      "======================================================================\n",
      "[4/5] Processing table: 39_uk_demographics_1960_2012\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✓ full_table/36_south_cambridgeshire_district_council_1973_2012: 1 rows, 6944 tokens, 37.62s\n",
      "\n",
      "======================================================================\n",
      "[4/5] Processing table: 39_uk_demographics_1960_2012\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✓ full_table/39_uk_demographics_1960_2012: 1 rows, 3377 tokens, 19.38s\n",
      "\n",
      "======================================================================\n",
      "[5/5] Processing table: 59_miss_new_york_usa_delegates_2012\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✓ full_table/39_uk_demographics_1960_2012: 1 rows, 3377 tokens, 19.38s\n",
      "\n",
      "======================================================================\n",
      "[5/5] Processing table: 59_miss_new_york_usa_delegates_2012\n",
      "======================================================================\n",
      "Strategies: full_table\n",
      "  Fetching full table...\n",
      "  ✓ full_table/59_miss_new_york_usa_delegates_2012: 62 rows, 2798 tokens, 21.27s\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Processed: 5 strategy-table combinations\n",
      "Successful: 4\n",
      "Failed: 1\n",
      "======================================================================\n",
      "  ✓ full_table/59_miss_new_york_usa_delegates_2012: 62 rows, 2798 tokens, 21.27s\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Processed: 5 strategy-table combinations\n",
      "Successful: 4\n",
      "Failed: 1\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = OUTPUT_ROOT / timestamp\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create subdirectories for each strategy\n",
    "for strategy_name in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_dir = output_dir / strategy_name\n",
    "    strategy_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Group strategies by table_id for parallel execution\n",
    "from collections import defaultdict\n",
    "tables_with_strategies = defaultdict(list)\n",
    "for strategy_name, plan in strategies_to_process:\n",
    "    table_id = plan['table_id']\n",
    "    tables_with_strategies[table_id].append((strategy_name, plan))\n",
    "\n",
    "print(f\"\\nGrouped into {len(tables_with_strategies)} tables with strategies\")\n",
    "\n",
    "# Process each table with parallel strategy execution\n",
    "results_summary = []\n",
    "\n",
    "\n",
    "def process_strategy(strategy_name: str, plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single strategy-table combination.\"\"\"\n",
    "    table_id = plan['table_id']\n",
    "    table_name = plan['table_name']\n",
    "    \n",
    "    try:\n",
    "        # Execute fetching strategy\n",
    "        fetch_func = STRATEGY_FUNCTIONS[strategy_name]\n",
    "        fetch_result = fetch_func(plan)\n",
    "        \n",
    "        pages = fetch_result['pages']\n",
    "        all_rows = fetch_result['all_rows']\n",
    "        \n",
    "        # Calculate aggregated metrics\n",
    "        total_pages = len(pages)\n",
    "        successful_pages = sum(1 for p in pages if p.get('parse_success', False))\n",
    "        failed_pages = total_pages - successful_pages\n",
    "        \n",
    "        total_latency = sum(p.get('latency', 0) for p in pages)\n",
    "        avg_latency = total_latency / total_pages if total_pages > 0 else 0\n",
    "        \n",
    "        total_tokens = sum(p.get('total_tokens', 0) for p in pages)\n",
    "        total_llm_calls = sum(1 + p.get('retry_count', 0) for p in pages)\n",
    "        \n",
    "        total_rows_fetched = len(all_rows)\n",
    "        \n",
    "        # Check for duplicate rows\n",
    "        unique_rows = set()\n",
    "        duplicate_count = 0\n",
    "        for row in all_rows:\n",
    "            row_key = tuple(sorted(row.items()))\n",
    "            if row_key in unique_rows:\n",
    "                duplicate_count += 1\n",
    "            unique_rows.add(row_key)\n",
    "        \n",
    "        # Check column consistency\n",
    "        if all_rows:\n",
    "            column_sets = [set(row.keys()) for row in all_rows]\n",
    "            columns_consistent = all(cs == column_sets[0] for cs in column_sets)\n",
    "        else:\n",
    "            columns_consistent = True\n",
    "        \n",
    "        error_rate = failed_pages / total_pages if total_pages > 0 else 0\n",
    "        \n",
    "        # Build execution summary\n",
    "        execution_summary = {\n",
    "            'table_id': table_id,\n",
    "            'table_name': table_name,\n",
    "            'strategy': strategy_name,\n",
    "            'metadata': plan['metadata'],\n",
    "            'pagination_config': plan['pagination_config'],\n",
    "            'execution_metadata': {\n",
    "                'timestamp': timestamp,\n",
    "                'total_pages': total_pages,\n",
    "                'successful_pages': successful_pages,\n",
    "                'failed_pages': failed_pages,\n",
    "                'total_llm_calls': total_llm_calls,\n",
    "                'total_latency': round(total_latency, 3),\n",
    "                'avg_latency': round(avg_latency, 3),\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_rows_fetched': total_rows_fetched,\n",
    "                'unique_rows': len(unique_rows),\n",
    "                'duplicate_rows': duplicate_count,\n",
    "                'columns_consistent': columns_consistent,\n",
    "                'error_rate': round(error_rate, 4)\n",
    "            },\n",
    "            'pages': pages\n",
    "        }\n",
    "        \n",
    "        # Save JSON log\n",
    "        json_path = output_dir / strategy_name / f\"{table_id}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(execution_summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save CSV if we have data\n",
    "        if all_rows:\n",
    "            try:\n",
    "                df = pd.DataFrame(all_rows)\n",
    "                csv_path = output_dir / strategy_name / f\"{table_id}.csv\"\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "                print(f\"  ✓ {strategy_name}/{table_id}: {len(df)} rows, {total_tokens} tokens, {total_latency:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ {strategy_name}/{table_id}: CSV save failed: {e}\")\n",
    "        else:\n",
    "            print(f\"  ⚠ {strategy_name}/{table_id}: No rows to save\")\n",
    "        \n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'table_id': table_id,\n",
    "            'success': True,\n",
    "            **execution_summary['execution_metadata']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {strategy_name}/{table_id}: Error: {e}\")\n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'table_id': table_id,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Process each table with parallel strategies\n",
    "for table_num, (table_id, strategies) in enumerate(tables_with_strategies.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{table_num}/{len(tables_with_strategies)}] Processing table: {table_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Strategies: {', '.join(s for s, _ in strategies)}\")\n",
    "    \n",
    "    if PARALLEL_STRATEGIES:\n",
    "        # Execute strategies in parallel\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {\n",
    "                executor.submit(process_strategy, strategy_name, plan): (strategy_name, plan['table_id'])\n",
    "                for strategy_name, plan in strategies\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                results_summary.append(result)\n",
    "    else:\n",
    "        # Sequential execution (for debugging)\n",
    "        for strategy_name, plan in strategies:\n",
    "            print(f\"\\n  {strategy_name.upper()}\")\n",
    "            result = process_strategy(strategy_name, plan)\n",
    "            results_summary.append(result)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed: {len(results_summary)} strategy-table combinations\")\n",
    "print(f\"Successful: {sum(1 for r in results_summary if r['success'])}\")\n",
    "print(f\"Failed: {sum(1 for r in results_summary if not r['success'])}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb5616",
   "metadata": {},
   "source": [
    "## Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9c0ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXECUTION SUMMARY\n",
      "======================================================================\n",
      "Total execution time: 2.33 minutes\n",
      "Tables processed: 4\n",
      "Strategy combinations: 4 successful, 1 failed\n",
      "Total pages fetched: 4\n",
      "Total LLM calls: 4\n",
      "Total tokens used: 16,064\n",
      "Total rows fetched: 85\n",
      "Avg latency per page: 22.83s\n",
      "Avg tokens per call: 4016.0\n",
      "\n",
      "⚠ 1 errors occurred (see _summary.json for details)\n",
      "\n",
      "Final summary saved to processing/2_fetching/20251005_002153/_summary.json\n",
      "All results saved to processing/2_fetching/20251005_002153\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate aggregate statistics across all strategies\n",
    "execution_end_time = datetime.now()\n",
    "# Parse timestamp: format is YYYYMMDD_HHMMSS\n",
    "execution_start_time = datetime.strptime(timestamp, '%Y%m%d_%H%M%S')\n",
    "\n",
    "total_execution_time = (execution_end_time - execution_start_time).total_seconds()\n",
    "\n",
    "# Aggregate metrics\n",
    "total_tables_processed = len(set(r['table_id'] for r in results_summary if r['success']))\n",
    "total_strategies_run = len([r for r in results_summary if r['success']])\n",
    "total_failures = len([r for r in results_summary if not r['success']])\n",
    "\n",
    "successful_results = [r for r in results_summary if r.get('success', False)]\n",
    "\n",
    "total_pages_fetched = sum(r.get('total_pages', 0) for r in successful_results)\n",
    "total_llm_calls_made = sum(r.get('total_llm_calls', 0) for r in successful_results)\n",
    "total_tokens_used = sum(r.get('total_tokens', 0) for r in successful_results)\n",
    "total_latency_seconds = sum(r.get('total_latency', 0) for r in successful_results)\n",
    "total_rows_fetched = sum(r.get('total_rows_fetched', 0) for r in successful_results)\n",
    "\n",
    "# Calculate averages\n",
    "avg_latency_per_page = total_latency_seconds / total_pages_fetched if total_pages_fetched > 0 else 0\n",
    "avg_tokens_per_call = total_tokens_used / total_llm_calls_made if total_llm_calls_made > 0 else 0\n",
    "avg_pages_per_strategy = total_pages_fetched / total_strategies_run if total_strategies_run > 0 else 0\n",
    "\n",
    "# Collect all errors\n",
    "errors_list = [\n",
    "    {\n",
    "        'strategy': r['strategy'],\n",
    "        'table_id': r['table_id'],\n",
    "        'error': r.get('error', 'Unknown error')\n",
    "    }\n",
    "    for r in results_summary if not r.get('success', False)\n",
    "]\n",
    "\n",
    "# Strategy breakdown\n",
    "strategy_breakdown = {}\n",
    "for strategy in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_results = [r for r in successful_results if r.get('strategy') == strategy]\n",
    "    if strategy_results:\n",
    "        strategy_breakdown[strategy] = {\n",
    "            'tables_processed': len(strategy_results),\n",
    "            'total_pages': sum(r.get('total_pages', 0) for r in strategy_results),\n",
    "            'total_tokens': sum(r.get('total_tokens', 0) for r in strategy_results),\n",
    "            'total_latency': round(sum(r.get('total_latency', 0) for r in strategy_results), 3),\n",
    "            'avg_latency': round(sum(r.get('avg_latency', 0) for r in strategy_results) / len(strategy_results), 3),\n",
    "            'total_rows': sum(r.get('total_rows_fetched', 0) for r in strategy_results),\n",
    "            'error_rate': round(sum(r.get('error_rate', 0) for r in strategy_results) / len(strategy_results), 4)\n",
    "        }\n",
    "\n",
    "# Build comprehensive summary\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'execution_time': {\n",
    "        'start': timestamp,\n",
    "        'end': execution_end_time.strftime('%Y%m%d_%H%M%S'),\n",
    "        'total_seconds': round(total_execution_time, 2),\n",
    "        'total_minutes': round(total_execution_time / 60, 2)\n",
    "    },\n",
    "    'configuration': {\n",
    "        'strategy_directory': str(LATEST_STRATEGY_DIR),\n",
    "        'max_tables': MAX_TABLES,\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'max_retries': MAX_RETRIES,\n",
    "        'max_pagination_pages': MAX_PAGINATION_PAGES,\n",
    "        'model': MODEL\n",
    "    },\n",
    "    'aggregate_metrics': {\n",
    "        'total_tables_processed': total_tables_processed,\n",
    "        'total_strategy_table_combinations': total_strategies_run,\n",
    "        'total_failures': total_failures,\n",
    "        'success_rate': round(total_strategies_run / (total_strategies_run + total_failures), 4) if (total_strategies_run + total_failures) > 0 else 0,\n",
    "        'total_pages_fetched': total_pages_fetched,\n",
    "        'total_llm_calls': total_llm_calls_made,\n",
    "        'total_tokens_used': total_tokens_used,\n",
    "        'total_latency_seconds': round(total_latency_seconds, 2),\n",
    "        'total_rows_fetched': total_rows_fetched,\n",
    "        'avg_latency_per_page': round(avg_latency_per_page, 3),\n",
    "        'avg_tokens_per_call': round(avg_tokens_per_call, 1),\n",
    "        'avg_pages_per_strategy': round(avg_pages_per_strategy, 1)\n",
    "    },\n",
    "    'strategy_breakdown': strategy_breakdown,\n",
    "    'errors': errors_list,\n",
    "    'detailed_results': results_summary\n",
    "}\n",
    "\n",
    "summary_file = output_dir / '_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total execution time: {summary['execution_time']['total_minutes']:.2f} minutes\")\n",
    "print(f\"Tables processed: {total_tables_processed}\")\n",
    "print(f\"Strategy combinations: {total_strategies_run} successful, {total_failures} failed\")\n",
    "print(f\"Total pages fetched: {total_pages_fetched}\")\n",
    "print(f\"Total LLM calls: {total_llm_calls_made}\")\n",
    "print(f\"Total tokens used: {total_tokens_used:,}\")\n",
    "print(f\"Total rows fetched: {total_rows_fetched}\")\n",
    "print(f\"Avg latency per page: {avg_latency_per_page:.2f}s\")\n",
    "print(f\"Avg tokens per call: {avg_tokens_per_call:.1f}\")\n",
    "if errors_list:\n",
    "    print(f\"\\n⚠ {len(errors_list)} errors occurred (see _summary.json for details)\")\n",
    "print(f\"\\nFinal summary saved to {summary_file}\")\n",
    "print(f\"All results saved to {output_dir}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496207f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The fetched data has been saved to `processing/2_fetching/<timestamp>/`.\n",
    "\n",
    "Each strategy subdirectory contains:\n",
    "- **JSON files**: Detailed execution logs with per-page metrics, prompts, responses, and aggregated statistics\n",
    "- **CSV files**: Aggregated table data ready for metrics calculation\n",
    "\n",
    "### Metrics Available:\n",
    "\n",
    "**Per-page metrics:**\n",
    "- latency, tokens (prompt/completion/total), retry_count\n",
    "- parse_success, rows_returned, JSON extraction position\n",
    "- timestamp, raw_response, parsed_data\n",
    "\n",
    "**Per-table-strategy metrics:**\n",
    "- total_pages, successful/failed_pages, total_llm_calls\n",
    "- total/avg latency, total_tokens\n",
    "- total_rows_fetched, unique_rows, duplicate_rows\n",
    "- columns_consistent, error_rate\n",
    "\n",
    "### To calculate accuracy metrics:\n",
    "Use the CSV files with the evaluation logic from `X101_Calculate_Metrics.ipynb` to compute:\n",
    "- Keys F1, Precision, Recall\n",
    "- Non-keys F1, Precision, Recall\n",
    "- Overall F1, Precision, Recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
