{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6167645",
   "metadata": {},
   "source": [
    "# Page Fetching for Pagination Strategies\n",
    "\n",
    "This notebook loads pagination plans from Stage 1 and executes the actual data fetching for each strategy.\n",
    "\n",
    "## Strategies:\n",
    "1. **Full Table** - Single query for entire table\n",
    "2. **Row by Row** - Fetch each row individually using key values\n",
    "3. **Attribute-based** - Fetch pages by partition values\n",
    "4. **Classic Pagination** - Offset-based iterative fetching\n",
    "5. **Range-based** - Fetch by defined ranges\n",
    "\n",
    "## Output:\n",
    "- CSV files: Aggregated table data (for metrics calculation)\n",
    "- JSON files: Detailed execution logs with per-page metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a37742",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0dea23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy directory: /Users/bef/Desktop/TablePagination/processing/1_strategy/20251005_004530\n",
      "Ground truth directory: /Users/bef/Desktop/TablePagination/processing/0_data/20251004_213355\n",
      "Output base: /Users/bef/Desktop/TablePagination/processing/2_fetching\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openai\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# MAX_TABLES: Limit number of tables to process (None for all)\n",
    "MAX_TABLES = 30\n",
    "\n",
    "# MAX_PAGES: Limit pages per table per strategy (None for all, useful for testing)\n",
    "MAX_PAGES = None\n",
    "\n",
    "# PARALLEL_STRATEGIES: Run strategies in parallel per table\n",
    "PARALLEL_STRATEGIES = True\n",
    "\n",
    "# MAX_WORKERS: Number of parallel strategy executions per table\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "# MAX_RETRIES: Number of retries for failed LLM calls\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# MAX_PAGINATION_PAGES: Failsafe for classic_pagination\n",
    "MAX_PAGINATION_PAGES = 10\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "PROCESSING_ROOT = ROOT / 'processing'\n",
    "STRATEGY_DIR = PROCESSING_ROOT / '1_strategy'\n",
    "OUTPUT_ROOT = PROCESSING_ROOT / '2_fetching'\n",
    "\n",
    "# Find the most recent strategy directory\n",
    "strategy_subdirs = sorted([d for d in STRATEGY_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not strategy_subdirs:\n",
    "    raise FileNotFoundError(f\"No strategy data found in {STRATEGY_DIR}\")\n",
    "\n",
    "LATEST_STRATEGY_DIR = strategy_subdirs[0]\n",
    "\n",
    "# Find ground truth data directory\n",
    "DATA_DIR = PROCESSING_ROOT / '0_data'\n",
    "data_subdirs = sorted([d for d in DATA_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not data_subdirs:\n",
    "    raise FileNotFoundError(f\"No ground truth data found in {DATA_DIR}\")\n",
    "\n",
    "LATEST_DATA_DIR = data_subdirs[0]\n",
    "\n",
    "print('Strategy directory:', LATEST_STRATEGY_DIR.resolve())\n",
    "print('Ground truth directory:', LATEST_DATA_DIR.resolve())\n",
    "print('Output base:', OUTPUT_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27259394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API key loaded from api_key.txt\n",
      "Using model: deepseek/deepseek-chat-v3-0324\n"
     ]
    }
   ],
   "source": [
    "# Configure OpenRouter\n",
    "# Read API key from file\n",
    "api_key_file = ROOT / 'api_key.txt'\n",
    "if not api_key_file.exists():\n",
    "    raise ValueError('No API key found. Please create api_key.txt or set OPENROUTER_API_KEY environment variable')\n",
    "with open(api_key_file, 'r') as f:\n",
    "    OPENROUTER_API_KEY = f.read().strip()\n",
    "print('✓ API key loaded from api_key.txt')\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "MODEL = 'deepseek/deepseek-chat-v3-0324'\n",
    "print(f'Using model: {MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429e4b2",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63accf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_field(s: str) -> str:\n",
    "    \"\"\"Normalize field names (matching original experiment logic).\"\"\"\n",
    "    s = s.lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\".\", \"\").replace(\",\",\"_\")\\\n",
    "            .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"','').replace(\"'\",\"\")\\\n",
    "            .replace(\"/\", \"\")\n",
    "    return re.sub('_+', '_', s)\n",
    "\n",
    "\n",
    "def call_llm_with_metrics_split(system_msg: str, user_msg: str, retry_count: int = 0) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call LLM with separate system and user messages (matching original experiment).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    prompt_length = len(system_msg) + len(user_msg)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                     {\"role\": \"user\", \"content\": user_msg}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Check if content is None\n",
    "        if content is None:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'Response content is None',\n",
    "                'metrics': {\n",
    "                    'latency': round(latency, 3),\n",
    "                    'prompt_length': prompt_length,\n",
    "                    'retry_count': retry_count,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        content = content.strip()\n",
    "        response_length = len(content)\n",
    "        \n",
    "        # Extract token usage\n",
    "        usage = response.usage\n",
    "        prompt_tokens = usage.prompt_tokens if usage else 0\n",
    "        completion_tokens = usage.completion_tokens if usage else 0\n",
    "        total_tokens = usage.total_tokens if usage else 0\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'response': content,\n",
    "            'metrics': {\n",
    "                'latency': round(latency, 3),\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'total_tokens': total_tokens,\n",
    "                'prompt_length': prompt_length,\n",
    "                'response_length': response_length,\n",
    "                'retry_count': retry_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        latency = time.time() - start_time\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'metrics': {\n",
    "                'latency': round(latency, 3),\n",
    "                'prompt_length': prompt_length,\n",
    "                'retry_count': retry_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def call_llm_with_retries(prompt: str = None, system_msg: str = None, user_msg: str = None, max_retries: int = MAX_RETRIES) -> Dict[str, Any]:\n",
    "    \"\"\"Call LLM with retry logic. Supports either single prompt or system+user messages.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        if system_msg and user_msg:\n",
    "            result = call_llm_with_metrics_split(system_msg, user_msg, retry_count=attempt)\n",
    "        else:\n",
    "            result = call_llm_with_metrics_split(\"You are a retriever of facts.\", prompt, retry_count=attempt)\n",
    "        if result['success']:\n",
    "            return result\n",
    "        print(f\"  Retry {attempt + 1}/{max_retries} after error: {result['error']}\")\n",
    "        time.sleep(1)  # Brief delay between retries\n",
    "    \n",
    "    return result  # Return last failed attempt\n",
    "\n",
    "\n",
    "def parse_json_response(response: str, expect_list: bool = True) -> Tuple[Optional[Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract and parse JSON from LLM response (matching original parsing logic).\n",
    "    \n",
    "    Args:\n",
    "        response: The raw LLM response text\n",
    "        expect_list: True for list responses, False for dict responses\n",
    "    \n",
    "    Returns:\n",
    "        - parsed_data: List/Dict or None\n",
    "        - parse_metrics: dict with parse_success, json_start_pos, json_end_pos\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'parse_success': False,\n",
    "        'json_start_pos': -1,\n",
    "        'json_end_pos': -1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Matching original logic: find JSON boundaries\n",
    "        if expect_list:\n",
    "            start_char, end_char = '[', ']'\n",
    "        else:\n",
    "            start_char, end_char = '{', '}'\n",
    "        \n",
    "        # Extract JSON portion\n",
    "        start_pos = response.find(start_char)\n",
    "        end_pos = response.rfind(end_char)\n",
    "        \n",
    "        if start_pos != -1 and end_pos != -1:\n",
    "            json_str = response[start_pos:end_pos + 1]\n",
    "            metrics['json_start_pos'] = start_pos\n",
    "            metrics['json_end_pos'] = end_pos + 1\n",
    "            \n",
    "            parsed = json.loads(json_str)\n",
    "            \n",
    "            # Handle wrapped responses\n",
    "            if isinstance(parsed, dict) and len(parsed.keys()) == 1:\n",
    "                parsed = list(parsed.values())[0]\n",
    "            \n",
    "            metrics['parse_success'] = True\n",
    "            return parsed, metrics\n",
    "        \n",
    "        # Fallback: manual parsing (matching original fallback logic)\n",
    "        if not expect_list and start_char not in response and end_char not in response:\n",
    "            return None, metrics\n",
    "            \n",
    "        split_response = response.split(\"{\")\n",
    "        response_json = []\n",
    "        for s in split_response[1:]:\n",
    "            split_s = s.split(\"}\")\n",
    "            if len(split_s) > 1:\n",
    "                content = split_s[0]\n",
    "                attributes = content.split(\",\")\n",
    "                elements = {}\n",
    "                for attr in attributes:\n",
    "                    knv = attr.split(\":\")\n",
    "                    if len(knv) > 1:\n",
    "                        parsed_k = \"%s\" % knv[0].replace('\"','').strip()\n",
    "                        parsed_v = \"%s\" % knv[1].replace('\"','').strip()\n",
    "                        elements[parsed_k] = parsed_v\n",
    "                \n",
    "                if elements:\n",
    "                    response_json.append(elements)\n",
    "        \n",
    "        if response_json:\n",
    "            metrics['parse_success'] = True\n",
    "            if expect_list:\n",
    "                return response_json, metrics\n",
    "            else:\n",
    "                return response_json[0] if response_json else {}, metrics\n",
    "        \n",
    "        return None, metrics\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        metrics['parse_error'] = str(e)\n",
    "        return None, metrics\n",
    "    except Exception as e:\n",
    "        metrics['parse_error'] = str(e)\n",
    "        return None, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2aaafa",
   "metadata": {},
   "source": [
    "## Strategy 1: Full Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e49a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_table(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch entire table in one query (matching original B004 approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names and build response format (matching B004)\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    fields_json = []\n",
    "    for field in norm_fields:\n",
    "        fields_json.append(f'\"{field}\": \"{field}\"')\n",
    "    response_format = ', '.join(fields_json)\n",
    "    \n",
    "    # Build prompt matching original B004 template exactly\n",
    "    system_msg = \"You are a retriever of facts.\"\n",
    "    user_msg = f\"\"\"List {table_title} - as many as possible to fit into response.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "    \n",
    "    print(f\"  Fetching full table...\")\n",
    "    \n",
    "    # Call LLM with separate system and user messages\n",
    "    result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "    \n",
    "    page_data = {\n",
    "        'page_number': 1,\n",
    "        'system_msg': system_msg,\n",
    "        'user_msg': user_msg,\n",
    "        'prompt': user_msg,  # For backwards compatibility\n",
    "        'raw_response': result.get('response', ''),\n",
    "        'error': result.get('error'),\n",
    "        **result['metrics']\n",
    "    }\n",
    "    \n",
    "    if result['success']:\n",
    "        parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "        page_data.update(parse_metrics)\n",
    "        page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "        page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "    else:\n",
    "        page_data['parse_success'] = False\n",
    "        page_data['rows_returned'] = 0\n",
    "        page_data['parsed_data'] = []\n",
    "    \n",
    "    return {\n",
    "        'pages': [page_data],\n",
    "        'all_rows': page_data['parsed_data']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b578b05",
   "metadata": {},
   "source": [
    "## Strategy 2: Row by Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a971c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_row_by_row(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch each row individually using key values from plan (matching original B008 approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    key_columns = config['key_columns']\n",
    "    key_values = config['key_values']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    norm_keys = [normalize_field(k) for k in key_columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit if set\n",
    "    keys_to_fetch = key_values[:MAX_PAGES] if MAX_PAGES else key_values\n",
    "    total_keys = len(key_values)\n",
    "    \n",
    "    if MAX_PAGES and len(key_values) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} rows out of {total_keys}\")\n",
    "    \n",
    "    for page_num, key_combo in enumerate(keys_to_fetch, 1):\n",
    "        # Build WHERE clause for this row\n",
    "        key_conditions = []\n",
    "        for key in key_columns:\n",
    "            norm_key = normalize_field(key)\n",
    "            key_value = key_combo.get(norm_key, 'UNKNOWN')\n",
    "            key_conditions.append(f\"{key} = {key_value}\")\n",
    "        row_key = '(' + ', '.join(key_conditions) + ')'\n",
    "        \n",
    "        # Build response format with key values filled in (matching B008)\n",
    "        fields_json = []\n",
    "        for field in norm_fields:\n",
    "            # Use key values where available, field names otherwise\n",
    "            if field in key_combo:\n",
    "                fields_json.append(f'\"{field}\": \"{key_combo[field]}\"')\n",
    "            else:\n",
    "                fields_json.append(f'\"{field}\": \"{field}\"')\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original B008 template exactly\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        key_column_desc = f\"The key column{'s' if len(key_columns) > 1 else ''} in the table {'are' if len(key_columns) > 1 else 'is'} {', '.join(key_columns)}\"\n",
    "        user_msg = f\"\"\"We want to create a table with the detailed information about {table_title}.\n",
    "Columns in the table are {', '.join(columns)}.\n",
    "{key_column_desc}.\n",
    "Retrieve a single row whose key is {row_key}.\n",
    "The response will be formatted as JSON dictionary shown below.\n",
    "Pay special attention to wrap all property names and values in double quotes!\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "{{\n",
    "    {response_format}\n",
    "}}\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching row {page_num}/{len(keys_to_fetch)}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'key_values': key_combo,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            # Parse as dict (single row), then wrap in list\n",
    "            parsed_dict, parse_metrics = parse_json_response(result['response'], expect_list=False)\n",
    "            page_data.update(parse_metrics)\n",
    "            \n",
    "            if parsed_dict:\n",
    "                parsed_data = [parsed_dict]  # Wrap dict in list\n",
    "                page_data['rows_returned'] = 1\n",
    "                page_data['parsed_data'] = parsed_data\n",
    "                all_rows.extend(parsed_data)\n",
    "            else:\n",
    "                page_data['rows_returned'] = 0\n",
    "                page_data['parsed_data'] = []\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6174b1d",
   "metadata": {},
   "source": [
    "## Strategy 3: Attribute-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6eebb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_attribute_based(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch pages by partition values (matching original approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    partition_column = config['partition_column']\n",
    "    partition_values = config['partition_values']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit\n",
    "    values_to_fetch = partition_values[:MAX_PAGES] if MAX_PAGES else partition_values\n",
    "    total_values = len(partition_values)\n",
    "    \n",
    "    if MAX_PAGES and len(partition_values) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} partitions out of {total_values}\")\n",
    "    \n",
    "    for page_num, value in enumerate(values_to_fetch, 1):\n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        user_msg = f\"\"\"List rows from {table_title} where {partition_column} = {value}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching partition {page_num}/{len(values_to_fetch)}: {partition_column}={value}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'partition_value': value,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9e075",
   "metadata": {},
   "source": [
    "## Strategy 4: Classic Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "921ace95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_classic_pagination(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Iterative offset-based pagination (matching original approach).\n",
    "    Stops when LLM returns empty result or hits MAX_PAGINATION_PAGES failsafe.\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    page_size = config['page_size']\n",
    "    sort_order = config['sort_order']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    offset = 0\n",
    "    page_num = 0\n",
    "    \n",
    "    max_pages = min(MAX_PAGES, MAX_PAGINATION_PAGES) if MAX_PAGES else MAX_PAGINATION_PAGES\n",
    "    \n",
    "    while page_num < max_pages:\n",
    "        page_num += 1\n",
    "        \n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        sort_desc = ', '.join(sort_order)\n",
    "        user_msg = f\"\"\"List rows {offset + 1} to {offset + page_size} from {table_title}, sorted by {sort_desc}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "If there are no more rows at this offset, respond with an empty list: []\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching page {page_num} (offset {offset}, size {page_size})...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'offset': offset,\n",
    "            'page_size': page_size,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data and len(parsed_data) > 0:\n",
    "                all_rows.extend(parsed_data)\n",
    "                offset += page_size\n",
    "            else:\n",
    "                # Empty result, stop pagination\n",
    "                print(f\"  Stopping: Empty result at offset {offset}\")\n",
    "                page_data['stop_reason'] = 'empty_result'\n",
    "                pages.append(page_data)\n",
    "                break\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    if page_num >= max_pages:\n",
    "        print(f\"  Stopping: Hit max pages limit ({max_pages})\")\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfb96f",
   "metadata": {},
   "source": [
    "## Strategy 5: Range-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17c11e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_range_based(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch pages by defined ranges (matching original approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    partition_column = config['partition_column']\n",
    "    ranges = config['ranges']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit\n",
    "    ranges_to_fetch = ranges[:MAX_PAGES] if MAX_PAGES else ranges\n",
    "    total_ranges = len(ranges)\n",
    "    \n",
    "    if MAX_PAGES and len(ranges) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} ranges out of {total_ranges}\")\n",
    "    \n",
    "    for page_num, range_spec in enumerate(ranges_to_fetch, 1):\n",
    "        gte = range_spec.get('gte', '')\n",
    "        lt = range_spec.get('lt', '')\n",
    "        \n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        user_msg = f\"\"\"List rows from {table_title} where {partition_column} >= {gte} and {partition_column} < {lt}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching range {page_num}/{len(ranges_to_fetch)}: {partition_column} [{gte}, {lt})...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'range': range_spec,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17e489",
   "metadata": {},
   "source": [
    "## Main Execution: Process All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07b59480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 239 strategy-table combinations\n",
      "\n",
      "Filtering to tables with plans for all 5 strategies:\n",
      "  Total unique tables: 48\n",
      "  Tables with all strategies: 47\n",
      "  Filtered to 235 strategy-table combinations\n",
      "Limited to 150 combinations for 30 tables\n",
      "Processing 150 strategy-table combinations...\n"
     ]
    }
   ],
   "source": [
    "# Strategy function mapping\n",
    "STRATEGY_FUNCTIONS = {\n",
    "    'full_table': fetch_full_table,\n",
    "    'row_by_row': fetch_row_by_row,\n",
    "    'attribute_based': fetch_attribute_based,\n",
    "    'classic_pagination': fetch_classic_pagination,\n",
    "    'range_based': fetch_range_based\n",
    "}\n",
    "\n",
    "# Load all strategies\n",
    "strategies_to_process = []\n",
    "for strategy_name in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_dir = LATEST_STRATEGY_DIR / strategy_name\n",
    "    if not strategy_dir.exists():\n",
    "        print(f\"Skipping {strategy_name}: directory not found\")\n",
    "        continue\n",
    "    \n",
    "    json_files = sorted(strategy_dir.glob('*.json'))\n",
    "    # Filter out error files\n",
    "    json_files = [f for f in json_files if not f.name.startswith('_')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            plan = json.load(f)\n",
    "        strategies_to_process.append((strategy_name, plan))\n",
    "\n",
    "print(f\"Found {len(strategies_to_process)} strategy-table combinations\")\n",
    "\n",
    "# Filter to only tables that have plans for ALL strategies (apples-to-apples comparison)\n",
    "from collections import Counter\n",
    "all_strategies = list(STRATEGY_FUNCTIONS.keys())\n",
    "table_strategy_counts = Counter(plan['table_id'] for _, plan in strategies_to_process)\n",
    "tables_with_all_strategies = [table_id for table_id, count in table_strategy_counts.items() \n",
    "                               if count == len(all_strategies)]\n",
    "\n",
    "print(f\"\\nFiltering to tables with plans for all {len(all_strategies)} strategies:\")\n",
    "print(f\"  Total unique tables: {len(table_strategy_counts)}\")\n",
    "print(f\"  Tables with all strategies: {len(tables_with_all_strategies)}\")\n",
    "\n",
    "strategies_to_process = [(s, p) for s, p in strategies_to_process \n",
    "                         if p['table_id'] in tables_with_all_strategies]\n",
    "print(f\"  Filtered to {len(strategies_to_process)} strategy-table combinations\")\n",
    "\n",
    "# Apply MAX_TABLES limit across all strategies\n",
    "if MAX_TABLES:\n",
    "    # Group by table_id to ensure we process complete sets\n",
    "    table_ids = list(set(p['table_id'] for _, p in strategies_to_process))[:MAX_TABLES]\n",
    "    strategies_to_process = [(s, p) for s, p in strategies_to_process if p['table_id'] in table_ids]\n",
    "    print(f\"Limited to {len(strategies_to_process)} combinations for {len(table_ids)} tables\")\n",
    "\n",
    "print(f\"Processing {len(strategies_to_process)} strategy-table combinations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: processing/2_fetching/20251005_093335\n",
      "\n",
      "Grouped into 30 tables with strategies\n",
      "\n",
      "======================================================================\n",
      "[1/30] Processing table: 10_men_butterfly_100m_2009\n",
      "======================================================================\n",
      "Strategies: full_table, row_by_row, attribute_based, classic_pagination, range_based\n",
      "  Fetching full table...\n",
      "  Fetching row 1/20...\n",
      "  Fetching partition 1/9: Heat=Heat 1...\n",
      "  Fetching page 1 (offset 0, size 10)...\n",
      "  Fetching range 1/20: Time [48.0, 48.5)...\n",
      "  Fetching row 2/20...\n",
      "  Fetching row 2/20...\n",
      "  Fetching range 2/20: Time [48.5, 49.0)...\n",
      "  Fetching range 2/20: Time [48.5, 49.0)...\n",
      "  Fetching row 3/20...\n",
      "  Fetching row 3/20...\n",
      "  ✓ full_table/10_men_butterfly_100m_2009: 20 rows, 966 tokens, 10.12s\n",
      "  ✓ full_table/10_men_butterfly_100m_2009: 20 rows, 966 tokens, 10.12s\n",
      "  Fetching row 4/20...\n",
      "  Fetching row 4/20...\n",
      "  Fetching partition 2/9: Heat=Heat 2...\n",
      "  Fetching partition 2/9: Heat=Heat 2...\n",
      "  Fetching row 5/20...\n",
      "  Fetching row 5/20...\n",
      "  Fetching row 6/20...\n",
      "  Fetching row 6/20...\n",
      "  Fetching range 3/20: Time [49.0, 49.5)...\n",
      "  Fetching range 3/20: Time [49.0, 49.5)...\n",
      "  Fetching row 7/20...\n",
      "  Fetching row 7/20...\n",
      "  Fetching page 2 (offset 10, size 10)...\n",
      "  Fetching page 2 (offset 10, size 10)...\n",
      "  Stopping: Empty result at offset 10\n",
      "  ✓ classic_pagination/10_men_butterfly_100m_2009: 10 rows, 703 tokens, 20.64s\n",
      "  Stopping: Empty result at offset 10\n",
      "  ✓ classic_pagination/10_men_butterfly_100m_2009: 10 rows, 703 tokens, 20.64s\n",
      "  Fetching row 8/20...\n",
      "  Fetching row 8/20...\n",
      "  Fetching row 9/20...\n",
      "  Fetching row 9/20...\n",
      "  Fetching range 4/20: Time [49.5, 50.0)...\n",
      "  Fetching range 4/20: Time [49.5, 50.0)...\n",
      "  Fetching row 10/20...\n",
      "  Fetching row 10/20...\n",
      "  Fetching partition 3/9: Heat=Heat 3...\n",
      "  Fetching partition 3/9: Heat=Heat 3...\n",
      "  Fetching row 11/20...\n",
      "  Fetching row 11/20...\n",
      "  Fetching row 12/20...\n",
      "  Fetching row 12/20...\n",
      "  Fetching range 5/20: Time [50.0, 50.5)...\n",
      "  Fetching range 5/20: Time [50.0, 50.5)...\n",
      "  Fetching row 13/20...\n",
      "  Fetching row 13/20...\n",
      "  Fetching row 14/20...\n",
      "  Fetching row 14/20...\n",
      "  Fetching partition 4/9: Heat=Heat 4...\n",
      "  Fetching row 15/20...\n",
      "  Fetching partition 4/9: Heat=Heat 4...\n",
      "  Fetching row 15/20...\n",
      "  Fetching row 16/20...\n",
      "  Fetching row 16/20...\n",
      "  Fetching row 17/20...\n",
      "  Fetching row 17/20...\n",
      "  Fetching range 6/20: Time [50.5, 51.0)...\n",
      "  Fetching range 6/20: Time [50.5, 51.0)...\n",
      "  Fetching row 18/20...\n",
      "  Fetching row 18/20...\n",
      "  Fetching row 19/20...\n",
      "  Fetching row 19/20...\n",
      "  Fetching range 7/20: Time [51.0, 51.5)...\n",
      "  Fetching range 7/20: Time [51.0, 51.5)...\n",
      "  Fetching row 20/20...\n",
      "  Fetching row 20/20...\n",
      "  Fetching partition 5/9: Heat=Heat 5...\n",
      "  Fetching partition 5/9: Heat=Heat 5...\n",
      "  ✓ row_by_row/10_men_butterfly_100m_2009: 18 rows, 4108 tokens, 65.01s\n",
      "  ✓ row_by_row/10_men_butterfly_100m_2009: 18 rows, 4108 tokens, 65.01s\n",
      "  Fetching partition 6/9: Heat=Heat 6...\n",
      "  Fetching partition 6/9: Heat=Heat 6...\n",
      "  Fetching range 8/20: Time [51.5, 52.0)...\n",
      "  Fetching range 8/20: Time [51.5, 52.0)...\n",
      "  Fetching partition 7/9: Heat=Heat 7...\n",
      "  Fetching partition 7/9: Heat=Heat 7...\n",
      "  Fetching range 9/20: Time [52.0, 52.5)...\n",
      "  Fetching range 9/20: Time [52.0, 52.5)...\n",
      "  Fetching partition 8/9: Heat=Heat 8...\n",
      "  Fetching partition 8/9: Heat=Heat 8...\n",
      "  Fetching partition 9/9: Heat=Heat 9...\n",
      "  Fetching partition 9/9: Heat=Heat 9...\n",
      "  Fetching range 10/20: Time [52.5, 53.0)...\n",
      "  Fetching range 10/20: Time [52.5, 53.0)...\n",
      "  Fetching range 11/20: Time [53.0, 53.5)...\n",
      "  Fetching range 11/20: Time [53.0, 53.5)...\n",
      "  ✓ attribute_based/10_men_butterfly_100m_2009: 29 rows, 4593 tokens, 134.09s\n",
      "  ✓ attribute_based/10_men_butterfly_100m_2009: 29 rows, 4593 tokens, 134.09s\n",
      "  Fetching range 12/20: Time [53.5, 54.0)...\n",
      "  Fetching range 12/20: Time [53.5, 54.0)...\n",
      "  Fetching range 13/20: Time [54.0, 54.5)...\n",
      "  Fetching range 13/20: Time [54.0, 54.5)...\n",
      "  Fetching range 14/20: Time [54.5, 55.0)...\n",
      "  Fetching range 14/20: Time [54.5, 55.0)...\n",
      "  Fetching range 15/20: Time [55.0, 55.5)...\n",
      "  Fetching range 15/20: Time [55.0, 55.5)...\n",
      "  Fetching range 16/20: Time [55.5, 56.0)...\n",
      "  Fetching range 16/20: Time [55.5, 56.0)...\n",
      "  Fetching range 17/20: Time [56.0, 56.5)...\n",
      "  Fetching range 17/20: Time [56.0, 56.5)...\n",
      "  Fetching range 18/20: Time [56.5, 57.0)...\n",
      "  Fetching range 18/20: Time [56.5, 57.0)...\n",
      "  Fetching range 19/20: Time [57.0, 57.5)...\n",
      "  Fetching range 19/20: Time [57.0, 57.5)...\n",
      "  Fetching range 20/20: Time [57.5, 58.0)...\n",
      "  Fetching range 20/20: Time [57.5, 58.0)...\n",
      "  ✓ range_based/10_men_butterfly_100m_2009: 56 rows, 10289 tokens, 330.86s\n",
      "\n",
      "======================================================================\n",
      "[2/30] Processing table: 11_playstation_3_cooperative_games\n",
      "======================================================================\n",
      "Strategies: full_table, row_by_row, attribute_based, classic_pagination, range_based\n",
      "  Fetching full table...\n",
      "  Fetching row 1/20...\n",
      "  Fetching partition 1/2: Co-op campaign=Yes...\n",
      "  Fetching page 1 (offset 0, size 10)...\n",
      "  Fetching range 1/32: Release Date [2006-01-01, 2006-04-01)...\n",
      "  ✓ range_based/10_men_butterfly_100m_2009: 56 rows, 10289 tokens, 330.86s\n",
      "\n",
      "======================================================================\n",
      "[2/30] Processing table: 11_playstation_3_cooperative_games\n",
      "======================================================================\n",
      "Strategies: full_table, row_by_row, attribute_based, classic_pagination, range_based\n",
      "  Fetching full table...\n",
      "  Fetching row 1/20...\n",
      "  Fetching partition 1/2: Co-op campaign=Yes...\n",
      "  Fetching page 1 (offset 0, size 10)...\n",
      "  Fetching range 1/32: Release Date [2006-01-01, 2006-04-01)...\n",
      "  Fetching range 2/32: Release Date [2006-04-01, 2006-07-01)...\n",
      "  Stopping: Empty result at offset 0\n",
      "  ⚠ classic_pagination/11_playstation_3_cooperative_games: No rows to save\n",
      "  Fetching range 2/32: Release Date [2006-04-01, 2006-07-01)...\n",
      "  Stopping: Empty result at offset 0\n",
      "  ⚠ classic_pagination/11_playstation_3_cooperative_games: No rows to save\n",
      "  Fetching range 3/32: Release Date [2006-07-01, 2006-10-01)...\n",
      "  Fetching range 3/32: Release Date [2006-07-01, 2006-10-01)...\n",
      "  Fetching row 2/20...\n",
      "  Fetching row 2/20...\n",
      "  Fetching row 3/20...\n",
      "  Fetching row 3/20...\n",
      "  Fetching range 4/32: Release Date [2006-10-01, 2007-01-01)...\n",
      "  Fetching range 4/32: Release Date [2006-10-01, 2007-01-01)...\n",
      "  Fetching row 4/20...\n",
      "  Fetching row 4/20...\n",
      "  Fetching range 5/32: Release Date [2007-01-01, 2007-04-01)...\n",
      "  Fetching row 5/20...\n",
      "  Fetching range 5/32: Release Date [2007-01-01, 2007-04-01)...\n",
      "  Fetching row 5/20...\n",
      "  Fetching partition 2/2: Co-op campaign=No...\n",
      "  Fetching partition 2/2: Co-op campaign=No...\n",
      "  Fetching range 6/32: Release Date [2007-04-01, 2007-07-01)...\n",
      "  Fetching range 6/32: Release Date [2007-04-01, 2007-07-01)...\n",
      "  Fetching range 7/32: Release Date [2007-07-01, 2007-10-01)...\n",
      "  Fetching range 7/32: Release Date [2007-07-01, 2007-10-01)...\n",
      "  Fetching row 6/20...\n",
      "  Fetching row 6/20...\n",
      "  Fetching row 7/20...\n",
      "  Fetching row 7/20...\n",
      "  Fetching range 8/32: Release Date [2007-10-01, 2008-01-01)...\n",
      "  Fetching range 8/32: Release Date [2007-10-01, 2008-01-01)...\n",
      "  Fetching row 8/20...\n",
      "  Fetching row 8/20...\n",
      "  ✓ full_table/11_playstation_3_cooperative_games: 10 rows, 972 tokens, 33.88s\n",
      "  ✓ full_table/11_playstation_3_cooperative_games: 10 rows, 972 tokens, 33.88s\n",
      "  Fetching row 9/20...\n",
      "  Fetching row 9/20...\n",
      "  ✓ attribute_based/11_playstation_3_cooperative_games: 10 rows, 1208 tokens, 39.64s\n",
      "  ✓ attribute_based/11_playstation_3_cooperative_games: 10 rows, 1208 tokens, 39.64s\n",
      "  Fetching range 9/32: Release Date [2008-01-01, 2008-04-01)...\n",
      "  Fetching range 9/32: Release Date [2008-01-01, 2008-04-01)...\n",
      "  Fetching row 10/20...\n",
      "  Fetching row 10/20...\n",
      "  Fetching row 11/20...\n",
      "  Fetching row 11/20...\n",
      "  Fetching row 12/20...\n",
      "  Fetching row 12/20...\n",
      "  Fetching range 10/32: Release Date [2008-04-01, 2008-07-01)...\n",
      "  Fetching range 10/32: Release Date [2008-04-01, 2008-07-01)...\n",
      "  Fetching row 13/20...\n",
      "  Fetching row 13/20...\n",
      "  Fetching row 14/20...\n",
      "  Fetching row 14/20...\n",
      "  Fetching range 11/32: Release Date [2008-07-01, 2008-10-01)...\n",
      "  Fetching range 11/32: Release Date [2008-07-01, 2008-10-01)...\n",
      "  Fetching row 15/20...\n",
      "  Fetching row 15/20...\n",
      "  Fetching row 16/20...\n",
      "  Fetching row 16/20...\n",
      "  Fetching row 17/20...\n",
      "  Fetching row 17/20...\n",
      "  Fetching range 12/32: Release Date [2008-10-01, 2009-01-01)...\n",
      "  Fetching range 12/32: Release Date [2008-10-01, 2009-01-01)...\n",
      "  Fetching row 18/20...\n",
      "  Fetching row 18/20...\n",
      "  Fetching row 19/20...\n",
      "  Fetching row 19/20...\n",
      "  Fetching row 20/20...\n",
      "  Fetching row 20/20...\n",
      "  ✓ row_by_row/11_playstation_3_cooperative_games: 20 rows, 5976 tokens, 76.64s\n",
      "  ✓ row_by_row/11_playstation_3_cooperative_games: 20 rows, 5976 tokens, 76.64s\n",
      "  Fetching range 13/32: Release Date [2009-01-01, 2009-04-01)...\n",
      "  Fetching range 13/32: Release Date [2009-01-01, 2009-04-01)...\n",
      "  Fetching range 14/32: Release Date [2009-04-01, 2009-07-01)...\n",
      "  Fetching range 14/32: Release Date [2009-04-01, 2009-07-01)...\n",
      "  Fetching range 15/32: Release Date [2009-07-01, 2009-10-01)...\n",
      "  Fetching range 15/32: Release Date [2009-07-01, 2009-10-01)...\n",
      "  Fetching range 16/32: Release Date [2009-10-01, 2010-01-01)...\n",
      "  Fetching range 16/32: Release Date [2009-10-01, 2010-01-01)...\n",
      "  Fetching range 17/32: Release Date [2010-01-01, 2010-04-01)...\n",
      "  Fetching range 17/32: Release Date [2010-01-01, 2010-04-01)...\n",
      "  Fetching range 18/32: Release Date [2010-04-01, 2010-07-01)...\n",
      "  Fetching range 18/32: Release Date [2010-04-01, 2010-07-01)...\n",
      "  Fetching range 19/32: Release Date [2010-07-01, 2010-10-01)...\n",
      "  Fetching range 19/32: Release Date [2010-07-01, 2010-10-01)...\n",
      "  Fetching range 20/32: Release Date [2010-10-01, 2011-01-01)...\n",
      "  Fetching range 20/32: Release Date [2010-10-01, 2011-01-01)...\n",
      "  Fetching range 21/32: Release Date [2011-01-01, 2011-04-01)...\n",
      "  Fetching range 21/32: Release Date [2011-01-01, 2011-04-01)...\n",
      "  Fetching range 22/32: Release Date [2011-04-01, 2011-07-01)...\n",
      "  Fetching range 22/32: Release Date [2011-04-01, 2011-07-01)...\n",
      "  Fetching range 23/32: Release Date [2011-07-01, 2011-10-01)...\n",
      "  Fetching range 23/32: Release Date [2011-07-01, 2011-10-01)...\n",
      "  Fetching range 24/32: Release Date [2011-10-01, 2012-01-01)...\n",
      "  Fetching range 24/32: Release Date [2011-10-01, 2012-01-01)...\n",
      "  Fetching range 25/32: Release Date [2012-01-01, 2012-04-01)...\n",
      "  Fetching range 25/32: Release Date [2012-01-01, 2012-04-01)...\n",
      "  Fetching range 26/32: Release Date [2012-04-01, 2012-07-01)...\n",
      "  Fetching range 26/32: Release Date [2012-04-01, 2012-07-01)...\n",
      "  Fetching range 27/32: Release Date [2012-07-01, 2012-10-01)...\n",
      "  Fetching range 27/32: Release Date [2012-07-01, 2012-10-01)...\n",
      "  Fetching range 28/32: Release Date [2012-10-01, 2013-01-01)...\n",
      "  Fetching range 28/32: Release Date [2012-10-01, 2013-01-01)...\n",
      "  Fetching range 29/32: Release Date [2013-01-01, 2013-04-01)...\n",
      "  Fetching range 29/32: Release Date [2013-01-01, 2013-04-01)...\n",
      "  Fetching range 30/32: Release Date [2013-04-01, 2013-07-01)...\n",
      "  Fetching range 30/32: Release Date [2013-04-01, 2013-07-01)...\n",
      "  Fetching range 31/32: Release Date [2013-07-01, 2013-10-01)...\n",
      "  Fetching range 31/32: Release Date [2013-07-01, 2013-10-01)...\n",
      "  Fetching range 32/32: Release Date [2013-10-01, 2014-01-01)...\n",
      "  Fetching range 32/32: Release Date [2013-10-01, 2014-01-01)...\n",
      "  ✓ range_based/11_playstation_3_cooperative_games: 83 rows, 13963 tokens, 364.50s\n",
      "\n",
      "======================================================================\n",
      "[3/30] Processing table: 12_rock_band_downloadable_2011\n",
      "======================================================================\n",
      "Strategies: full_table, row_by_row, attribute_based, classic_pagination, range_based\n",
      "  Fetching full table...\n",
      "  Fetching row 1/10...\n",
      "  Fetching partition 1/52: Release date=2011-01-04...\n",
      "  Fetching page 1 (offset 0, size 10)...\n",
      "  Fetching range 1/12: Release date [2011-01, 2011-02)...\n",
      "  ✓ range_based/11_playstation_3_cooperative_games: 83 rows, 13963 tokens, 364.50s\n",
      "\n",
      "======================================================================\n",
      "[3/30] Processing table: 12_rock_band_downloadable_2011\n",
      "======================================================================\n",
      "Strategies: full_table, row_by_row, attribute_based, classic_pagination, range_based\n",
      "  Fetching full table...\n",
      "  Fetching row 1/10...\n",
      "  Fetching partition 1/52: Release date=2011-01-04...\n",
      "  Fetching page 1 (offset 0, size 10)...\n",
      "  Fetching range 1/12: Release date [2011-01, 2011-02)...\n",
      "  Fetching row 2/10...\n",
      "  Fetching row 2/10...\n",
      "  Fetching range 2/12: Release date [2011-02, 2011-03)...\n",
      "  Fetching range 2/12: Release date [2011-02, 2011-03)...\n",
      "  Fetching partition 2/52: Release date=2011-01-11...\n",
      "  Fetching partition 2/52: Release date=2011-01-11...\n",
      "  Fetching row 3/10...\n",
      "  Fetching row 3/10...\n",
      "  Fetching row 4/10...\n",
      "  Fetching row 4/10...\n",
      "  Fetching partition 3/52: Release date=2011-01-18...\n",
      "  Fetching partition 3/52: Release date=2011-01-18...\n",
      "  Fetching range 3/12: Release date [2011-03, 2011-04)...\n",
      "  Fetching range 3/12: Release date [2011-03, 2011-04)...\n",
      "  Fetching row 5/10...\n",
      "  Fetching row 5/10...\n",
      "  Fetching partition 4/52: Release date=2011-01-25...\n",
      "  Fetching page 2 (offset 10, size 10)...\n",
      "  Fetching partition 4/52: Release date=2011-01-25...\n",
      "  Fetching page 2 (offset 10, size 10)...\n",
      "  Fetching row 6/10...\n",
      "  Fetching row 6/10...\n",
      "  Fetching row 7/10...\n",
      "  Fetching row 7/10...\n",
      "  Fetching partition 5/52: Release date=2011-02-01...\n",
      "  Fetching partition 5/52: Release date=2011-02-01...\n",
      "  Fetching row 8/10...\n",
      "  Fetching row 8/10...\n",
      "  Fetching row 9/10...\n",
      "  Fetching row 9/10...\n",
      "  Fetching partition 6/52: Release date=2011-02-08...\n",
      "  Fetching partition 6/52: Release date=2011-02-08...\n",
      "  Fetching partition 7/52: Release date=2011-02-15...\n",
      "  Fetching partition 7/52: Release date=2011-02-15...\n",
      "  Fetching row 10/10...\n",
      "  Fetching row 10/10...\n",
      "  ✓ row_by_row/12_rock_band_downloadable_2011: 10 rows, 2763 tokens, 46.80s\n",
      "  ✓ row_by_row/12_rock_band_downloadable_2011: 10 rows, 2763 tokens, 46.80s\n",
      "  Fetching partition 8/52: Release date=2011-02-22...\n",
      "  Fetching partition 8/52: Release date=2011-02-22...\n",
      "  Fetching range 4/12: Release date [2011-04, 2011-05)...\n",
      "  Fetching range 4/12: Release date [2011-04, 2011-05)...\n",
      "  ✓ full_table/12_rock_band_downloadable_2011: 22 rows, 1774 tokens, 51.59s\n",
      "  ✓ full_table/12_rock_band_downloadable_2011: 22 rows, 1774 tokens, 51.59s\n",
      "  Fetching partition 9/52: Release date=2011-03-01...\n",
      "  Fetching partition 9/52: Release date=2011-03-01...\n",
      "  Fetching partition 10/52: Release date=2011-03-08...\n",
      "  Fetching partition 10/52: Release date=2011-03-08...\n",
      "  Fetching partition 11/52: Release date=2011-03-15...\n",
      "  Fetching partition 11/52: Release date=2011-03-15...\n",
      "  Fetching range 5/12: Release date [2011-05, 2011-06)...\n",
      "  Fetching range 5/12: Release date [2011-05, 2011-06)...\n",
      "  Fetching page 3 (offset 20, size 10)...\n",
      "  Fetching page 3 (offset 20, size 10)...\n",
      "  Stopping: Empty result at offset 20\n",
      "  ✓ classic_pagination/12_rock_band_downloadable_2011: 17 rows, 2056 tokens, 70.27s\n",
      "  Stopping: Empty result at offset 20\n",
      "  ✓ classic_pagination/12_rock_band_downloadable_2011: 17 rows, 2056 tokens, 70.27s\n",
      "  Fetching partition 12/52: Release date=2011-03-22...\n",
      "  Fetching partition 12/52: Release date=2011-03-22...\n",
      "  Fetching partition 13/52: Release date=2011-03-29...\n",
      "  Fetching partition 13/52: Release date=2011-03-29...\n",
      "  Fetching partition 14/52: Release date=2011-04-05...\n",
      "  Fetching partition 14/52: Release date=2011-04-05...\n",
      "  Fetching partition 15/52: Release date=2011-04-12...\n",
      "  Fetching partition 15/52: Release date=2011-04-12...\n",
      "  Fetching range 6/12: Release date [2011-06, 2011-07)...\n",
      "  Fetching range 6/12: Release date [2011-06, 2011-07)...\n",
      "  Fetching partition 16/52: Release date=2011-04-19...\n",
      "  Fetching partition 16/52: Release date=2011-04-19...\n",
      "  Fetching partition 17/52: Release date=2011-04-26...\n",
      "  Fetching partition 17/52: Release date=2011-04-26...\n",
      "  Fetching partition 18/52: Release date=2011-05-03...\n",
      "  Fetching partition 18/52: Release date=2011-05-03...\n",
      "  Fetching partition 19/52: Release date=2011-05-10...\n",
      "  Fetching partition 19/52: Release date=2011-05-10...\n",
      "  Fetching range 7/12: Release date [2011-07, 2011-08)...\n",
      "  Fetching range 7/12: Release date [2011-07, 2011-08)...\n",
      "  Fetching partition 20/52: Release date=2011-05-17...\n",
      "  Fetching partition 20/52: Release date=2011-05-17...\n",
      "  Fetching partition 21/52: Release date=2011-05-24...\n",
      "  Fetching partition 21/52: Release date=2011-05-24...\n",
      "  Fetching partition 22/52: Release date=2011-05-31...\n",
      "  Fetching range 8/12: Release date [2011-08, 2011-09)...\n",
      "  Fetching partition 22/52: Release date=2011-05-31...\n",
      "  Fetching range 8/12: Release date [2011-08, 2011-09)...\n",
      "  Fetching partition 23/52: Release date=2011-06-07...\n",
      "  Fetching partition 23/52: Release date=2011-06-07...\n",
      "  Fetching partition 24/52: Release date=2011-06-14...\n",
      "  Fetching partition 24/52: Release date=2011-06-14...\n",
      "  Fetching partition 25/52: Release date=2011-06-21...\n",
      "  Fetching partition 25/52: Release date=2011-06-21...\n",
      "  Fetching partition 26/52: Release date=2011-06-28...\n",
      "  Fetching partition 26/52: Release date=2011-06-28...\n",
      "  Fetching partition 27/52: Release date=2011-07-05...\n",
      "  Fetching partition 27/52: Release date=2011-07-05...\n",
      "  Fetching partition 28/52: Release date=2011-07-12...\n",
      "  Fetching partition 28/52: Release date=2011-07-12...\n",
      "  Fetching range 9/12: Release date [2011-09, 2011-10)...\n",
      "  Fetching range 9/12: Release date [2011-09, 2011-10)...\n",
      "  Fetching partition 29/52: Release date=2011-07-19...\n",
      "  Fetching partition 29/52: Release date=2011-07-19...\n",
      "  Fetching partition 50/52: Release date=2011-12-13...\n",
      "  Fetching partition 51/52: Release date=2011-12-20...\n",
      "  Fetching partition 52/52: Release date=2011-12-27...\n",
      "  ✓ attribute_based/12_rock_band_downloadable_2011: 33 rows, 17086 tokens, 434.30s\n",
      "\n",
      "======================================================================\n",
      "[4/30] Processing table: 19_living_proof_the_farewell_tour\n",
      "======================================================================\n",
      "Strategies: full_table, row_by_row, attribute_based, classic_pagination, range_based\n",
      "  Fetching full table...\n",
      "  Fetching row 1/20...\n",
      "  Fetching partition 1/10: Date=2023-09-15...\n",
      "  Fetching page 1 (offset 0, size 10)...\n",
      "  Fetching range 1/12: Date [2023-01, 2023-02)...\n",
      "  Fetching row 2/20...\n",
      "  Fetching partition 2/10: Date=2023-09-16...\n",
      "  Fetching row 3/20...\n",
      "  Fetching partition 3/10: Date=2023-09-18...\n",
      "  Fetching row 4/20...\n",
      "  Fetching row 5/20...\n",
      "  Fetching partition 4/10: Date=2023-09-20...\n",
      "  Fetching row 6/20...\n",
      "  Fetching range 2/12: Date [2023-02, 2023-03)...\n",
      "  Fetching page 2 (offset 10, size 10)...\n",
      "  Fetching partition 5/10: Date=2023-09-22...\n",
      "  Stopping: Empty result at offset 10\n",
      "  ✓ classic_pagination/19_living_proof_the_farewell_tour: 10 rows, 619 tokens, 15.37s\n",
      "  Fetching row 7/20...\n",
      "  Fetching partition 6/10: Date=2023-09-24...\n",
      "  Fetching row 8/20...\n",
      "  Fetching partition 7/10: Date=2023-09-26...\n",
      "  Fetching row 9/20...\n",
      "  Fetching partition 8/10: Date=2023-09-28...\n",
      "  Fetching row 10/20...\n",
      "  Fetching partition 9/10: Date=2023-09-30...\n",
      "  Fetching range 3/12: Date [2023-03, 2023-04)...\n",
      "  Fetching row 11/20...\n",
      "  Fetching row 12/20...\n",
      "  Fetching partition 10/10: Date=2023-10-02...\n",
      "  Fetching row 13/20...\n",
      "  ✓ attribute_based/19_living_proof_the_farewell_tour: 10 rows, 1518 tokens, 37.62s\n",
      "  Fetching row 14/20...\n",
      "  ✓ full_table/19_living_proof_the_farewell_tour: 23 rows, 908 tokens, 39.63s\n",
      "  Fetching row 15/20...\n",
      "  Fetching range 4/12: Date [2023-04, 2023-05)...\n",
      "  Fetching row 16/20...\n",
      "  Fetching range 5/12: Date [2023-05, 2023-06)...\n",
      "  Fetching row 17/20...\n",
      "  Fetching row 18/20...\n",
      "  Fetching row 19/20...\n",
      "  Fetching row 20/20...\n",
      "  ✓ row_by_row/19_living_proof_the_farewell_tour: 20 rows, 3671 tokens, 59.10s\n",
      "  Fetching range 6/12: Date [2023-06, 2023-07)...\n",
      "  Fetching range 7/12: Date [2023-07, 2023-08)...\n",
      "  Fetching range 8/12: Date [2023-08, 2023-09)...\n",
      "  Fetching range 9/12: Date [2023-09, 2023-10)...\n",
      "  Fetching range 10/12: Date [2023-10, 2023-11)...\n",
      "  Fetching range 11/12: Date [2023-11, 2023-12)...\n",
      "  Fetching range 12/12: Date [2023-12, 2024-01)...\n",
      "  Fetching range 6/10: Year [1970, 1980)...\n",
      "  Fetching range 7/10: Year [1980, 1990)...\n",
      "  Fetching range 8/10: Year [1990, 2000)...\n",
      "  Fetching range 9/10: Year [2000, 2010)...\n",
      "  Fetching range 10/10: Year [2010, 2020)...\n",
      "  ✓ range_based/20_new_zealand_demographics_1921_2011: 91 rows, 13394 tokens, 483.73s\n",
      "\n",
      "======================================================================\n",
      "[6/30] Processing table: 21_liechtenstein_demographics_1901_2011\n",
      "======================================================================\n",
      "Strategies: full_table, row_by_row, attribute_based, classic_pagination, range_based\n",
      "  Fetching full table...\n",
      "  Fetching row 1/111...\n",
      "  Fetching partition 1/12: Year=1901...\n",
      "  Fetching page 1 (offset 0, size 10)...\n",
      "  Fetching range 1/12: year [1900, 1910)...\n",
      "  Fetching partition 2/12: Year=1910...\n",
      "  Fetching row 2/111...\n",
      "  Fetching partition 3/12: Year=1920...\n",
      "  Fetching row 3/111...\n",
      "  Fetching partition 4/12: Year=1930...\n",
      "  Fetching row 4/111...\n",
      "  Fetching partition 5/12: Year=1940...\n",
      "  Fetching row 5/111...\n",
      "  Fetching partition 6/12: Year=1950...\n",
      "  Fetching row 8/111...\n",
      "  Fetching partition 9/12: Year=1980...\n",
      "  Fetching row 9/111...\n",
      "  Fetching partition 10/12: Year=1990...\n",
      "  Fetching row 10/111...\n",
      "  Fetching range 2/12: year [1910, 1920)...\n",
      "  Fetching partition 11/12: Year=2000...\n",
      "  Fetching page 2 (offset 10, size 10)...\n",
      "  Stopping: Empty result at offset 10\n",
      "  ✓ classic_pagination/21_liechtenstein_demographics_1901_2011: 10 rows, 1466 tokens, 62.79s\n",
      "  Fetching row 11/111...\n",
      "  Fetching partition 12/12: Year=2011...\n",
      "  Fetching row 12/111...\n",
      "  ✓ attribute_based/21_liechtenstein_demographics_1901_2011: 12 rows, 3847 tokens, 66.03s\n",
      "  Fetching row 13/111...\n",
      "  Fetching row 14/111...\n",
      "  Fetching row 15/111...\n",
      "  Fetching row 16/111...\n",
      "  Fetching range 3/12: year [1920, 1930)...\n",
      "  Fetching row 17/111...\n",
      "  Fetching row 18/111...\n",
      "  Fetching range 4/12: year [1930, 1940)...\n",
      "  Fetching row 19/111...\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = OUTPUT_ROOT / timestamp\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create subdirectories for each strategy\n",
    "for strategy_name in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_dir = output_dir / strategy_name\n",
    "    strategy_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Group strategies by table_id for parallel execution\n",
    "from collections import defaultdict\n",
    "tables_with_strategies = defaultdict(list)\n",
    "for strategy_name, plan in strategies_to_process:\n",
    "    table_id = plan['table_id']\n",
    "    tables_with_strategies[table_id].append((strategy_name, plan))\n",
    "\n",
    "print(f\"\\nGrouped into {len(tables_with_strategies)} tables with strategies\")\n",
    "\n",
    "# Process each table with parallel strategy execution\n",
    "results_summary = []\n",
    "\n",
    "\n",
    "def process_strategy(strategy_name: str, plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single strategy-table combination.\"\"\"\n",
    "    table_id = plan['table_id']\n",
    "    table_name = plan['table_name']\n",
    "    \n",
    "    try:\n",
    "        # Execute fetching strategy\n",
    "        fetch_func = STRATEGY_FUNCTIONS[strategy_name]\n",
    "        fetch_result = fetch_func(plan)\n",
    "        \n",
    "        pages = fetch_result['pages']\n",
    "        all_rows = fetch_result['all_rows']\n",
    "        \n",
    "        # Calculate aggregated metrics\n",
    "        total_pages = len(pages)\n",
    "        successful_pages = sum(1 for p in pages if p.get('parse_success', False))\n",
    "        failed_pages = total_pages - successful_pages\n",
    "        \n",
    "        total_latency = sum(p.get('latency', 0) for p in pages)\n",
    "        avg_latency = total_latency / total_pages if total_pages > 0 else 0\n",
    "        \n",
    "        total_tokens = sum(p.get('total_tokens', 0) for p in pages)\n",
    "        total_llm_calls = sum(1 + p.get('retry_count', 0) for p in pages)\n",
    "        \n",
    "        total_rows_fetched = len(all_rows)\n",
    "        \n",
    "        # Check for duplicate rows\n",
    "        unique_rows = set()\n",
    "        duplicate_count = 0\n",
    "        for row in all_rows:\n",
    "            row_key = tuple(sorted(row.items()))\n",
    "            if row_key in unique_rows:\n",
    "                duplicate_count += 1\n",
    "            unique_rows.add(row_key)\n",
    "        \n",
    "        # Check column consistency\n",
    "        if all_rows:\n",
    "            column_sets = [set(row.keys()) for row in all_rows]\n",
    "            columns_consistent = all(cs == column_sets[0] for cs in column_sets)\n",
    "        else:\n",
    "            columns_consistent = True\n",
    "        \n",
    "        error_rate = failed_pages / total_pages if total_pages > 0 else 0\n",
    "        \n",
    "        # Build execution summary\n",
    "        execution_summary = {\n",
    "            'table_id': table_id,\n",
    "            'table_name': table_name,\n",
    "            'strategy': strategy_name,\n",
    "            'metadata': plan['metadata'],\n",
    "            'pagination_config': plan['pagination_config'],\n",
    "            'execution_metadata': {\n",
    "                'timestamp': timestamp,\n",
    "                'total_pages': total_pages,\n",
    "                'successful_pages': successful_pages,\n",
    "                'failed_pages': failed_pages,\n",
    "                'total_llm_calls': total_llm_calls,\n",
    "                'total_latency': round(total_latency, 3),\n",
    "                'avg_latency': round(avg_latency, 3),\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_rows_fetched': total_rows_fetched,\n",
    "                'unique_rows': len(unique_rows),\n",
    "                'duplicate_rows': duplicate_count,\n",
    "                'columns_consistent': columns_consistent,\n",
    "                'error_rate': round(error_rate, 4)\n",
    "            },\n",
    "            'pages': pages\n",
    "        }\n",
    "        \n",
    "        # Save JSON log\n",
    "        json_path = output_dir / strategy_name / f\"{table_id}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(execution_summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save CSV if we have data (matching B004 approach: normalize columns to match reference)\n",
    "        if all_rows:\n",
    "            try:\n",
    "                # Load reference table to get proper column names\n",
    "                ref_csv_path = LATEST_DATA_DIR / f\"{table_id}.csv\"\n",
    "                df_ref = pd.read_csv(ref_csv_path)\n",
    "                \n",
    "                # Create dataframe from fetched rows\n",
    "                df = pd.DataFrame(all_rows)\n",
    "                \n",
    "                # Normalize fetched column names\n",
    "                df.columns = [normalize_field(col) for col in df.columns]\n",
    "                \n",
    "                # Normalize reference column names to create mapping\n",
    "                norm_ref_cols = [normalize_field(col) for col in df_ref.columns]\n",
    "                \n",
    "                # Ensure fetched df has same columns as reference (reorder and add missing)\n",
    "                missing_cols = [col for col in norm_ref_cols if col not in df.columns]\n",
    "                for col in missing_cols:\n",
    "                    df[col] = None  # Add missing columns with None\n",
    "                \n",
    "                # Reorder to match reference\n",
    "                df = df[norm_ref_cols]\n",
    "                \n",
    "                # Restore original column names from reference\n",
    "                df.columns = df_ref.columns\n",
    "                \n",
    "                # Drop duplicates based on key columns\n",
    "                key_columns = plan['metadata']['key_columns']\n",
    "                if key_columns:\n",
    "                    df = df.drop_duplicates(subset=key_columns)\n",
    "                \n",
    "                csv_path = output_dir / strategy_name / f\"{table_id}.csv\"\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "                print(f\"  ✓ {strategy_name}/{table_id}: {len(df)} rows, {total_tokens} tokens, {total_latency:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ {strategy_name}/{table_id}: CSV save failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"  ⚠ {strategy_name}/{table_id}: No rows to save\")\n",
    "        \n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'table_id': table_id,\n",
    "            'success': True,\n",
    "            **execution_summary['execution_metadata']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {strategy_name}/{table_id}: Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'table_id': table_id,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Process each table with parallel strategies\n",
    "for table_num, (table_id, strategies) in enumerate(tables_with_strategies.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{table_num}/{len(tables_with_strategies)}] Processing table: {table_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Strategies: {', '.join(s for s, _ in strategies)}\")\n",
    "    \n",
    "    if PARALLEL_STRATEGIES:\n",
    "        # Execute strategies in parallel\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {\n",
    "                executor.submit(process_strategy, strategy_name, plan): (strategy_name, plan['table_id'])\n",
    "                for strategy_name, plan in strategies\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                results_summary.append(result)\n",
    "    else:\n",
    "        # Sequential execution (for debugging)\n",
    "        for strategy_name, plan in strategies:\n",
    "            print(f\"\\n  {strategy_name.upper()}\")\n",
    "            result = process_strategy(strategy_name, plan)\n",
    "            results_summary.append(result)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed: {len(results_summary)} strategy-table combinations\")\n",
    "print(f\"Successful: {sum(1 for r in results_summary if r['success'])}\")\n",
    "print(f\"Failed: {sum(1 for r in results_summary if not r['success'])}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb5616",
   "metadata": {},
   "source": [
    "## Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXECUTION SUMMARY\n",
      "======================================================================\n",
      "Total execution time: 236.11 minutes\n",
      "Tables processed: 30\n",
      "Strategy combinations: 150 successful, 0 failed\n",
      "Total pages fetched: 3209\n",
      "Total LLM calls: 3209\n",
      "Total tokens used: 5,398,575\n",
      "Total rows fetched: 9251\n",
      "Avg latency per page: 8.04s\n",
      "Avg tokens per call: 1682.3\n",
      "\n",
      "Final summary saved to processing/2_fetching/20251005_024414/_summary.json\n",
      "All results saved to processing/2_fetching/20251005_024414\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate aggregate statistics across all strategies\n",
    "execution_end_time = datetime.now()\n",
    "# Parse timestamp: format is YYYYMMDD_HHMMSS\n",
    "execution_start_time = datetime.strptime(timestamp, '%Y%m%d_%H%M%S')\n",
    "\n",
    "total_execution_time = (execution_end_time - execution_start_time).total_seconds()\n",
    "\n",
    "# Aggregate metrics\n",
    "total_tables_processed = len(set(r['table_id'] for r in results_summary if r['success']))\n",
    "total_strategies_run = len([r for r in results_summary if r['success']])\n",
    "total_failures = len([r for r in results_summary if not r['success']])\n",
    "\n",
    "successful_results = [r for r in results_summary if r.get('success', False)]\n",
    "\n",
    "total_pages_fetched = sum(r.get('total_pages', 0) for r in successful_results)\n",
    "total_llm_calls_made = sum(r.get('total_llm_calls', 0) for r in successful_results)\n",
    "total_tokens_used = sum(r.get('total_tokens', 0) for r in successful_results)\n",
    "total_latency_seconds = sum(r.get('total_latency', 0) for r in successful_results)\n",
    "total_rows_fetched = sum(r.get('total_rows_fetched', 0) for r in successful_results)\n",
    "\n",
    "# Calculate averages\n",
    "avg_latency_per_page = total_latency_seconds / total_pages_fetched if total_pages_fetched > 0 else 0\n",
    "avg_tokens_per_call = total_tokens_used / total_llm_calls_made if total_llm_calls_made > 0 else 0\n",
    "avg_pages_per_strategy = total_pages_fetched / total_strategies_run if total_strategies_run > 0 else 0\n",
    "\n",
    "# Collect all errors\n",
    "errors_list = [\n",
    "    {\n",
    "        'strategy': r['strategy'],\n",
    "        'table_id': r['table_id'],\n",
    "        'error': r.get('error', 'Unknown error')\n",
    "    }\n",
    "    for r in results_summary if not r.get('success', False)\n",
    "]\n",
    "\n",
    "# Strategy breakdown\n",
    "strategy_breakdown = {}\n",
    "for strategy in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_results = [r for r in successful_results if r.get('strategy') == strategy]\n",
    "    if strategy_results:\n",
    "        strategy_breakdown[strategy] = {\n",
    "            'tables_processed': len(strategy_results),\n",
    "            'total_pages': sum(r.get('total_pages', 0) for r in strategy_results),\n",
    "            'total_tokens': sum(r.get('total_tokens', 0) for r in strategy_results),\n",
    "            'total_latency': round(sum(r.get('total_latency', 0) for r in strategy_results), 3),\n",
    "            'avg_latency': round(sum(r.get('avg_latency', 0) for r in strategy_results) / len(strategy_results), 3),\n",
    "            'total_rows': sum(r.get('total_rows_fetched', 0) for r in strategy_results),\n",
    "            'error_rate': round(sum(r.get('error_rate', 0) for r in strategy_results) / len(strategy_results), 4)\n",
    "        }\n",
    "\n",
    "# Build comprehensive summary\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'execution_time': {\n",
    "        'start': timestamp,\n",
    "        'end': execution_end_time.strftime('%Y%m%d_%H%M%S'),\n",
    "        'total_seconds': round(total_execution_time, 2),\n",
    "        'total_minutes': round(total_execution_time / 60, 2)\n",
    "    },\n",
    "    'configuration': {\n",
    "        'strategy_directory': str(LATEST_STRATEGY_DIR),\n",
    "        'max_tables': MAX_TABLES,\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'max_retries': MAX_RETRIES,\n",
    "        'max_pagination_pages': MAX_PAGINATION_PAGES,\n",
    "        'model': MODEL\n",
    "    },\n",
    "    'aggregate_metrics': {\n",
    "        'total_tables_processed': total_tables_processed,\n",
    "        'total_strategy_table_combinations': total_strategies_run,\n",
    "        'total_failures': total_failures,\n",
    "        'success_rate': round(total_strategies_run / (total_strategies_run + total_failures), 4) if (total_strategies_run + total_failures) > 0 else 0,\n",
    "        'total_pages_fetched': total_pages_fetched,\n",
    "        'total_llm_calls': total_llm_calls_made,\n",
    "        'total_tokens_used': total_tokens_used,\n",
    "        'total_latency_seconds': round(total_latency_seconds, 2),\n",
    "        'total_rows_fetched': total_rows_fetched,\n",
    "        'avg_latency_per_page': round(avg_latency_per_page, 3),\n",
    "        'avg_tokens_per_call': round(avg_tokens_per_call, 1),\n",
    "        'avg_pages_per_strategy': round(avg_pages_per_strategy, 1)\n",
    "    },\n",
    "    'strategy_breakdown': strategy_breakdown,\n",
    "    'errors': errors_list,\n",
    "    'detailed_results': results_summary\n",
    "}\n",
    "\n",
    "summary_file = output_dir / '_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total execution time: {summary['execution_time']['total_minutes']:.2f} minutes\")\n",
    "print(f\"Tables processed: {total_tables_processed}\")\n",
    "print(f\"Strategy combinations: {total_strategies_run} successful, {total_failures} failed\")\n",
    "print(f\"Total pages fetched: {total_pages_fetched}\")\n",
    "print(f\"Total LLM calls: {total_llm_calls_made}\")\n",
    "print(f\"Total tokens used: {total_tokens_used:,}\")\n",
    "print(f\"Total rows fetched: {total_rows_fetched}\")\n",
    "print(f\"Avg latency per page: {avg_latency_per_page:.2f}s\")\n",
    "print(f\"Avg tokens per call: {avg_tokens_per_call:.1f}\")\n",
    "if errors_list:\n",
    "    print(f\"\\n⚠ {len(errors_list)} errors occurred (see _summary.json for details)\")\n",
    "print(f\"\\nFinal summary saved to {summary_file}\")\n",
    "print(f\"All results saved to {output_dir}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496207f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The fetched data has been saved to `processing/2_fetching/<timestamp>/`.\n",
    "\n",
    "Each strategy subdirectory contains:\n",
    "- **JSON files**: Detailed execution logs with per-page metrics, prompts, responses, and aggregated statistics\n",
    "- **CSV files**: Aggregated table data ready for metrics calculation\n",
    "\n",
    "### Metrics Available:\n",
    "\n",
    "**Per-page metrics:**\n",
    "- latency, tokens (prompt/completion/total), retry_count\n",
    "- parse_success, rows_returned, JSON extraction position\n",
    "- timestamp, raw_response, parsed_data\n",
    "\n",
    "**Per-table-strategy metrics:**\n",
    "- total_pages, successful/failed_pages, total_llm_calls\n",
    "- total/avg latency, total_tokens\n",
    "- total_rows_fetched, unique_rows, duplicate_rows\n",
    "- columns_consistent, error_rate\n",
    "\n",
    "### To calculate accuracy metrics:\n",
    "Use the CSV files with the evaluation logic from `X101_Calculate_Metrics.ipynb` to compute:\n",
    "- Keys F1, Precision, Recall\n",
    "- Non-keys F1, Precision, Recall\n",
    "- Overall F1, Precision, Recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
