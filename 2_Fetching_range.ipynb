{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6167645",
   "metadata": {},
   "source": [
    "# Page Fetching for Pagination Strategies\n",
    "\n",
    "This notebook loads pagination plans from Stage 1 and executes the actual data fetching for each strategy.\n",
    "\n",
    "## Strategies:\n",
    "1. **Full Table** - Single query for entire table\n",
    "2. **Row by Row** - Fetch each row individually using key values\n",
    "3. **Attribute-based** - Fetch pages by partition values\n",
    "4. **Classic Pagination** - Offset-based iterative fetching\n",
    "5. **Range-based Alphabetic** - Fetch by alphabetic ranges (A-F, G-L, etc.)\n",
    "6. **Range-based Semantic** - Fetch by semantic categories (continents, etc.)\n",
    "7. **Range-based Unrestricted** - Fetch by unrestricted ranges (benchmark)\n",
    "\n",
    "## Output:\n",
    "- CSV files: Aggregated table data (for metrics calculation)\n",
    "- JSON files: Detailed execution logs with per-page metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a37742",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dea23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy directory: /Users/bef/Desktop/TablePagination/processing/1_strategy/20251015_230052\n",
      "Ground truth directory: /Users/bef/Desktop/TablePagination/processing/0_data/20251004_213355\n",
      "Output base: /Users/bef/Desktop/TablePagination/processing/2_fetching\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openai\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# MAX_TABLES: Limit number of tables to process (None for all)\n",
    "MAX_TABLES = 20\n",
    "\n",
    "# MAX_PAGES: Limit pages per table per strategy (None for all, useful for testing)\n",
    "MAX_PAGES = None\n",
    "\n",
    "# PARALLEL_STRATEGIES: Run strategies in parallel per table\n",
    "PARALLEL_STRATEGIES = True\n",
    "\n",
    "# MAX_WORKERS: Number of parallel strategy executions per table\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "# MAX_RETRIES: Number of retries for failed LLM calls\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# MAX_PAGINATION_PAGES: Failsafe for classic_pagination\n",
    "MAX_PAGINATION_PAGES = 10\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "PROCESSING_ROOT = ROOT / 'processing'\n",
    "STRATEGY_DIR = PROCESSING_ROOT / '1_strategy'\n",
    "OUTPUT_ROOT = PROCESSING_ROOT / '2_fetching'\n",
    "\n",
    "# Find the most recent strategy directory\n",
    "strategy_subdirs = sorted([d for d in STRATEGY_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not strategy_subdirs:\n",
    "    raise FileNotFoundError(f\"No strategy data found in {STRATEGY_DIR}\")\n",
    "\n",
    "LATEST_STRATEGY_DIR = strategy_subdirs[0]\n",
    "\n",
    "# Find ground truth data directory\n",
    "DATA_DIR = PROCESSING_ROOT / '0_data'\n",
    "data_subdirs = sorted([d for d in DATA_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not data_subdirs:\n",
    "    raise FileNotFoundError(f\"No ground truth data found in {DATA_DIR}\")\n",
    "\n",
    "LATEST_DATA_DIR = data_subdirs[0]\n",
    "\n",
    "print('Strategy directory:', LATEST_STRATEGY_DIR.resolve())\n",
    "print('Ground truth directory:', LATEST_DATA_DIR.resolve())\n",
    "print('Output base:', OUTPUT_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27259394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API key loaded from api_key.txt\n",
      "Using model: gpt-4o-mini-search-preview-2025-03-11\n"
     ]
    }
   ],
   "source": [
    "# Configure OpenRouter\n",
    "# Read API key from file\n",
    "api_key_file = ROOT / 'api_key.txt'\n",
    "if not api_key_file.exists():\n",
    "    raise ValueError('No API key found. Please create api_key.txt or set OPENROUTER_API_KEY environment variable')\n",
    "with open(api_key_file, 'r') as f:\n",
    "    OPENROUTER_API_KEY = f.read().strip()\n",
    "print('✓ API key loaded from api_key.txt')\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    # base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "MODEL = 'gpt-4o-mini-search-preview-2025-03-11'\n",
    "print(f'Using model: {MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429e4b2",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63accf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_field(s: str) -> str:\n",
    "    \"\"\"Normalize field names (matching original experiment logic).\"\"\"\n",
    "    s = s.lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\".\", \"\").replace(\",\",\"_\")\\\n",
    "            .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"','').replace(\"'\",\"\")\\\n",
    "            .replace(\"/\", \"\")\n",
    "    return re.sub('_+', '_', s)\n",
    "\n",
    "\n",
    "def call_llm_with_metrics_split(system_msg: str, user_msg: str, retry_count: int = 0) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call LLM with separate system and user messages (matching original experiment).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    prompt_length = len(system_msg) + len(user_msg)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                     {\"role\": \"user\", \"content\": user_msg}],\n",
    "            # temperature=0\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Check if content is None\n",
    "        if content is None:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'Response content is None',\n",
    "                'metrics': {\n",
    "                    'latency': round(latency, 3),\n",
    "                    'prompt_length': prompt_length,\n",
    "                    'retry_count': retry_count,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        content = content.strip()\n",
    "        response_length = len(content)\n",
    "        \n",
    "        # Extract token usage\n",
    "        usage = response.usage\n",
    "        prompt_tokens = usage.prompt_tokens if usage else 0\n",
    "        completion_tokens = usage.completion_tokens if usage else 0\n",
    "        total_tokens = usage.total_tokens if usage else 0\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'response': content,\n",
    "            'metrics': {\n",
    "                'latency': round(latency, 3),\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'total_tokens': total_tokens,\n",
    "                'prompt_length': prompt_length,\n",
    "                'response_length': response_length,\n",
    "                'retry_count': retry_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        latency = time.time() - start_time\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'metrics': {\n",
    "                'latency': round(latency, 3),\n",
    "                'prompt_length': prompt_length,\n",
    "                'retry_count': retry_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def call_llm_with_retries(prompt: str = None, system_msg: str = None, user_msg: str = None, max_retries: int = MAX_RETRIES) -> Dict[str, Any]:\n",
    "    \"\"\"Call LLM with retry logic. Supports either single prompt or system+user messages.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        if system_msg and user_msg:\n",
    "            result = call_llm_with_metrics_split(system_msg, user_msg, retry_count=attempt)\n",
    "        else:\n",
    "            result = call_llm_with_metrics_split(\"You are a retriever of facts.\", prompt, retry_count=attempt)\n",
    "        if result['success']:\n",
    "            return result\n",
    "        print(f\"  Retry {attempt + 1}/{max_retries} after error: {result['error']}\")\n",
    "        time.sleep(1)  # Brief delay between retries\n",
    "    \n",
    "    return result  # Return last failed attempt\n",
    "\n",
    "\n",
    "def parse_json_response(response: str, expect_list: bool = True) -> Tuple[Optional[Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract and parse JSON from LLM response (matching original parsing logic).\n",
    "    \n",
    "    Args:\n",
    "        response: The raw LLM response text\n",
    "        expect_list: True for list responses, False for dict responses\n",
    "    \n",
    "    Returns:\n",
    "        - parsed_data: List/Dict or None\n",
    "        - parse_metrics: dict with parse_success, json_start_pos, json_end_pos\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'parse_success': False,\n",
    "        'json_start_pos': -1,\n",
    "        'json_end_pos': -1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Matching original logic: find JSON boundaries\n",
    "        if expect_list:\n",
    "            start_char, end_char = '[', ']'\n",
    "        else:\n",
    "            start_char, end_char = '{', '}'\n",
    "        \n",
    "        # Extract JSON portion\n",
    "        start_pos = response.find(start_char)\n",
    "        end_pos = response.rfind(end_char)\n",
    "        \n",
    "        if start_pos != -1 and end_pos != -1:\n",
    "            json_str = response[start_pos:end_pos + 1]\n",
    "            metrics['json_start_pos'] = start_pos\n",
    "            metrics['json_end_pos'] = end_pos + 1\n",
    "            \n",
    "            parsed = json.loads(json_str)\n",
    "            \n",
    "            # Handle wrapped responses\n",
    "            if isinstance(parsed, dict) and len(parsed.keys()) == 1:\n",
    "                parsed = list(parsed.values())[0]\n",
    "            \n",
    "            metrics['parse_success'] = True\n",
    "            return parsed, metrics\n",
    "        \n",
    "        # Fallback: manual parsing (matching original fallback logic)\n",
    "        if not expect_list and start_char not in response and end_char not in response:\n",
    "            return None, metrics\n",
    "            \n",
    "        split_response = response.split(\"{\")\n",
    "        response_json = []\n",
    "        for s in split_response[1:]:\n",
    "            split_s = s.split(\"}\")\n",
    "            if len(split_s) > 1:\n",
    "                content = split_s[0]\n",
    "                attributes = content.split(\",\")\n",
    "                elements = {}\n",
    "                for attr in attributes:\n",
    "                    knv = attr.split(\":\")\n",
    "                    if len(knv) > 1:\n",
    "                        parsed_k = \"%s\" % knv[0].replace('\"','').strip()\n",
    "                        parsed_v = \"%s\" % knv[1].replace('\"','').strip()\n",
    "                        elements[parsed_k] = parsed_v\n",
    "                \n",
    "                if elements:\n",
    "                    response_json.append(elements)\n",
    "        \n",
    "        if response_json:\n",
    "            metrics['parse_success'] = True\n",
    "            if expect_list:\n",
    "                return response_json, metrics\n",
    "            else:\n",
    "                return response_json[0] if response_json else {}, metrics\n",
    "        \n",
    "        return None, metrics\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        metrics['parse_error'] = str(e)\n",
    "        return None, metrics\n",
    "    except Exception as e:\n",
    "        metrics['parse_error'] = str(e)\n",
    "        return None, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2aaafa",
   "metadata": {},
   "source": [
    "## Strategy 1: Full Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e49a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_table(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch entire table in one query (matching original B004 approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names and build response format (matching B004)\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    fields_json = []\n",
    "    for field in norm_fields:\n",
    "        fields_json.append(f'\"{field}\": \"{field}\"')\n",
    "    response_format = ', '.join(fields_json)\n",
    "    \n",
    "    # Build prompt matching original B004 template exactly\n",
    "    system_msg = \"You are a retriever of facts.\"\n",
    "    user_msg = f\"\"\"List {table_title} - as many as possible to fit into response.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "    \n",
    "    print(f\"  Fetching full table...\")\n",
    "    \n",
    "    # Call LLM with separate system and user messages\n",
    "    result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "    \n",
    "    page_data = {\n",
    "        'page_number': 1,\n",
    "        'system_msg': system_msg,\n",
    "        'user_msg': user_msg,\n",
    "        'prompt': user_msg,  # For backwards compatibility\n",
    "        'raw_response': result.get('response', ''),\n",
    "        'error': result.get('error'),\n",
    "        **result['metrics']\n",
    "    }\n",
    "    \n",
    "    if result['success']:\n",
    "        parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "        page_data.update(parse_metrics)\n",
    "        page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "        page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "    else:\n",
    "        page_data['parse_success'] = False\n",
    "        page_data['rows_returned'] = 0\n",
    "        page_data['parsed_data'] = []\n",
    "    \n",
    "    return {\n",
    "        'pages': [page_data],\n",
    "        'all_rows': page_data['parsed_data']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b578b05",
   "metadata": {},
   "source": [
    "## Strategy 2: Row by Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a971c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_row_by_row(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch each row individually using key values from plan (matching original B008 approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    key_columns = config['key_columns']\n",
    "    key_values = config['key_values']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    norm_keys = [normalize_field(k) for k in key_columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit if set\n",
    "    keys_to_fetch = key_values[:MAX_PAGES] if MAX_PAGES else key_values\n",
    "    total_keys = len(key_values)\n",
    "    \n",
    "    if MAX_PAGES and len(key_values) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} rows out of {total_keys}\")\n",
    "    \n",
    "    for page_num, key_combo in enumerate(keys_to_fetch, 1):\n",
    "        # Build WHERE clause for this row\n",
    "        key_conditions = []\n",
    "        for key in key_columns:\n",
    "            norm_key = normalize_field(key)\n",
    "            key_value = key_combo.get(norm_key, 'UNKNOWN')\n",
    "            key_conditions.append(f\"{key} = {key_value}\")\n",
    "        row_key = '(' + ', '.join(key_conditions) + ')'\n",
    "        \n",
    "        # Build response format with key values filled in (matching B008)\n",
    "        fields_json = []\n",
    "        for field in norm_fields:\n",
    "            # Use key values where available, field names otherwise\n",
    "            if field in key_combo:\n",
    "                fields_json.append(f'\"{field}\": \"{key_combo[field]}\"')\n",
    "            else:\n",
    "                fields_json.append(f'\"{field}\": \"{field}\"')\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original B008 template exactly\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        key_column_desc = f\"The key column{'s' if len(key_columns) > 1 else ''} in the table {'are' if len(key_columns) > 1 else 'is'} {', '.join(key_columns)}\"\n",
    "        user_msg = f\"\"\"We want to create a table with the detailed information about {table_title}.\n",
    "Columns in the table are {', '.join(columns)}.\n",
    "{key_column_desc}.\n",
    "Retrieve a single row whose key is {row_key}.\n",
    "The response will be formatted as JSON dictionary shown below.\n",
    "Pay special attention to wrap all property names and values in double quotes!\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "{{\n",
    "    {response_format}\n",
    "}}\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching row {page_num}/{len(keys_to_fetch)}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'key_values': key_combo,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            # Parse as dict (single row), then wrap in list\n",
    "            parsed_dict, parse_metrics = parse_json_response(result['response'], expect_list=False)\n",
    "            page_data.update(parse_metrics)\n",
    "            \n",
    "            if parsed_dict:\n",
    "                parsed_data = [parsed_dict]  # Wrap dict in list\n",
    "                page_data['rows_returned'] = 1\n",
    "                page_data['parsed_data'] = parsed_data\n",
    "                all_rows.extend(parsed_data)\n",
    "            else:\n",
    "                page_data['rows_returned'] = 0\n",
    "                page_data['parsed_data'] = []\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6174b1d",
   "metadata": {},
   "source": [
    "## Strategy 3: Attribute-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6eebb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_attribute_based(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch pages by partition values (matching original approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    partition_column = config['partition_column']\n",
    "    partition_values = config['partition_values']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit\n",
    "    values_to_fetch = partition_values[:MAX_PAGES] if MAX_PAGES else partition_values\n",
    "    total_values = len(partition_values)\n",
    "    \n",
    "    if MAX_PAGES and len(partition_values) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} partitions out of {total_values}\")\n",
    "    \n",
    "    for page_num, value in enumerate(values_to_fetch, 1):\n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        user_msg = f\"\"\"List rows from {table_title} where {partition_column} = {value}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching partition {page_num}/{len(values_to_fetch)}: {partition_column}={value}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'partition_value': value,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9e075",
   "metadata": {},
   "source": [
    "## Strategy 4: Classic Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921ace95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_classic_pagination(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Iterative offset-based pagination (matching original approach).\n",
    "    Stops when LLM returns empty result or hits MAX_PAGINATION_PAGES failsafe.\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    page_size = config['page_size']\n",
    "    sort_order = config['sort_order']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    offset = 0\n",
    "    page_num = 0\n",
    "    \n",
    "    max_pages = min(MAX_PAGES, MAX_PAGINATION_PAGES) if MAX_PAGES else MAX_PAGINATION_PAGES\n",
    "    \n",
    "    while page_num < max_pages:\n",
    "        page_num += 1\n",
    "        \n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        sort_desc = ', '.join(sort_order)\n",
    "        user_msg = f\"\"\"List rows {offset + 1} to {offset + page_size} from {table_title}, sorted by {sort_desc}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "If there are no more rows at this offset, respond with an empty list: []\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching page {page_num} (offset {offset}, size {page_size})...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'offset': offset,\n",
    "            'page_size': page_size,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data and len(parsed_data) > 0:\n",
    "                all_rows.extend(parsed_data)\n",
    "                offset += page_size\n",
    "            else:\n",
    "                # Empty result, stop pagination\n",
    "                print(f\"  Stopping: Empty result at offset {offset}\")\n",
    "                page_data['stop_reason'] = 'empty_result'\n",
    "                pages.append(page_data)\n",
    "                break\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    if page_num >= max_pages:\n",
    "        print(f\"  Stopping: Hit max pages limit ({max_pages})\")\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfb96f",
   "metadata": {},
   "source": [
    "## Strategy 5: Range-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17c11e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_range_based(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch pages by defined ranges (matching original approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    partition_column = config['partition_column']\n",
    "    ranges = config['ranges']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit\n",
    "    ranges_to_fetch = ranges[:MAX_PAGES] if MAX_PAGES else ranges\n",
    "    total_ranges = len(ranges)\n",
    "    \n",
    "    if MAX_PAGES and len(ranges) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} ranges out of {total_ranges}\")\n",
    "    \n",
    "    for page_num, range_spec in enumerate(ranges_to_fetch, 1):\n",
    "        # Handle different range types\n",
    "        if 'category' in range_spec:\n",
    "            # Semantic/category-based range\n",
    "            category = range_spec['category']\n",
    "            filter_condition = f\"{partition_column} represents {category}\"\n",
    "            range_display = f\"{category}\"\n",
    "        else:\n",
    "            # Numeric/alphabetical range with gte/lt\n",
    "            gte = range_spec.get('gte', '')\n",
    "            lt = range_spec.get('lt', '')\n",
    "            filter_condition = f\"{partition_column} >= {gte} and {partition_column} < {lt}\"\n",
    "            range_display = f\"[{gte}, {lt})\"\n",
    "        \n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        user_msg = f\"\"\"List rows from {table_title} where {filter_condition}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching range {page_num}/{len(ranges_to_fetch)}: {partition_column} {range_display}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'range': range_spec,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17e489",
   "metadata": {},
   "source": [
    "## Main Execution: Process All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b59480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping full_table: directory not found\n",
      "Skipping row_by_row: directory not found\n",
      "Skipping attribute_based: directory not found\n",
      "Skipping classic_pagination: directory not found\n",
      "Found 72 strategy-table combinations\n",
      "\n",
      "Filtering to tables with plans for all 3 strategies:\n",
      "  Total unique tables: 24\n",
      "  Tables with all strategies: 24\n",
      "  Filtered to 72 strategy-table combinations\n",
      "Limited to 60 combinations for 20 tables\n",
      "Processing 60 strategy-table combinations...\n"
     ]
    }
   ],
   "source": [
    "# Strategy function mapping\n",
    "STRATEGY_FUNCTIONS = {\n",
    "    'full_table': fetch_full_table,\n",
    "    'row_by_row': fetch_row_by_row,\n",
    "    'attribute_based': fetch_attribute_based,\n",
    "    'classic_pagination': fetch_classic_pagination,\n",
    "    'range_based_alphabetic': fetch_range_based,\n",
    "    'range_based_semantic': fetch_range_based,\n",
    "    'range_based_unrestricted': fetch_range_based\n",
    "}\n",
    "\n",
    "# Load all strategies\n",
    "strategies_to_process = []\n",
    "range_based_variants = ['alphabetic', 'semantic', 'unrestricted']\n",
    "\n",
    "for variant in range_based_variants:\n",
    "    strategy_name = f'range_based_{variant}'\n",
    "    strategy_dir = LATEST_STRATEGY_DIR / 'range_based'\n",
    "    if not strategy_dir.exists():\n",
    "        print(f\"Skipping {strategy_name}: directory not found\")\n",
    "        continue\n",
    "    \n",
    "    json_files = sorted(strategy_dir.glob('*.json'))\n",
    "    # Filter out error files\n",
    "    json_files = [f for f in json_files if not f.name.startswith('_')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            plan = json.load(f)\n",
    "        # Create a copy of the plan for this variant\n",
    "        variant_plan = plan.copy()\n",
    "        variant_plan['strategy'] = strategy_name\n",
    "        variant_plan['pagination_config'] = plan[f'pagination_config_{variant}']\n",
    "        strategies_to_process.append((strategy_name, variant_plan))\n",
    "\n",
    "# Now load other strategies\n",
    "for strategy_name in ['full_table', 'row_by_row', 'attribute_based', 'classic_pagination']:\n",
    "    strategy_dir = LATEST_STRATEGY_DIR / strategy_name\n",
    "    if not strategy_dir.exists():\n",
    "        print(f\"Skipping {strategy_name}: directory not found\")\n",
    "        continue\n",
    "    \n",
    "    json_files = sorted(strategy_dir.glob('*.json'))\n",
    "    # Filter out error files\n",
    "    json_files = [f for f in json_files if not f.name.startswith('_')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            plan = json.load(f)\n",
    "        strategies_to_process.append((strategy_name, plan))\n",
    "\n",
    "print(f\"Found {len(strategies_to_process)} strategy-table combinations\")\n",
    "\n",
    "# Filter to only tables that have plans for ALL range-based strategies (apples-to-apples comparison)\n",
    "from collections import Counter\n",
    "all_strategies = ['range_based_alphabetic', 'range_based_semantic', 'range_based_unrestricted']\n",
    "table_strategy_counts = Counter(plan['table_id'] for _, plan in strategies_to_process)\n",
    "tables_with_all_strategies = [table_id for table_id, count in table_strategy_counts.items() \n",
    "                               if count == len(all_strategies)]\n",
    "\n",
    "print(f\"\\nFiltering to tables with plans for all {len(all_strategies)} strategies:\")\n",
    "print(f\"  Total unique tables: {len(table_strategy_counts)}\")\n",
    "print(f\"  Tables with all strategies: {len(tables_with_all_strategies)}\")\n",
    "\n",
    "strategies_to_process = [(s, p) for s, p in strategies_to_process \n",
    "                         if p['table_id'] in tables_with_all_strategies]\n",
    "print(f\"  Filtered to {len(strategies_to_process)} strategy-table combinations\")\n",
    "\n",
    "# Apply MAX_TABLES limit across all strategies\n",
    "if MAX_TABLES:\n",
    "    # Group by table_id to ensure we process complete sets\n",
    "    table_ids = list(set(p['table_id'] for _, p in strategies_to_process))[:MAX_TABLES]\n",
    "    strategies_to_process = [(s, p) for s, p in strategies_to_process if p['table_id'] in table_ids]\n",
    "    print(f\"Limited to {len(strategies_to_process)} combinations for {len(table_ids)} tables\")\n",
    "\n",
    "print(f\"Processing {len(strategies_to_process)} strategy-table combinations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75e9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: processing/2_fetching/20251016_001443\n",
      "\n",
      "Grouped into 20 tables with strategies\n",
      "\n",
      "======================================================================\n",
      "[1/20] Processing table: 10_men_butterfly_100m_2009\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Name [A, G)...\n",
      "  Fetching range 1/6: Nationality Africa...\n",
      "  Fetching range 1/22: Time [49.00, 49.50)...\n",
      "  Fetching range 2/6: Nationality Asia...\n",
      "  Fetching range 2/22: Time [49.50, 50.00)...\n",
      "  Fetching range 3/22: Time [50.00, 50.50)...\n",
      "  Fetching range 4/22: Time [50.50, 51.00)...\n",
      "  Fetching range 5/22: Time [51.00, 51.50)...\n",
      "  Fetching range 2/4: Name [G, M)...\n",
      "  Fetching range 3/6: Nationality Europe...\n",
      "  Fetching range 3/4: Name [M, S)...\n",
      "  Fetching range 6/22: Time [51.50, 52.00)...\n",
      "  Fetching range 4/4: Name [S, [)...\n",
      "  Fetching range 4/6: Nationality North America...\n",
      "  Fetching range 7/22: Time [52.00, 52.50)...\n",
      "  ✓ range_based_alphabetic/10_men_butterfly_100m_2009: 105 rows, 5809 tokens, 34.20s\n",
      "  Fetching range 8/22: Time [52.50, 53.00)...\n",
      "  Fetching range 5/6: Nationality South America...\n",
      "  Fetching range 6/6: Nationality Oceania...\n",
      "  ✓ range_based_semantic/10_men_butterfly_100m_2009: 60 rows, 8117 tokens, 49.20s\n",
      "  Fetching range 9/22: Time [53.00, 53.50)...\n",
      "  Fetching range 10/22: Time [53.50, 54.00)...\n",
      "  Fetching range 11/22: Time [54.00, 54.50)...\n",
      "  Fetching range 12/22: Time [54.50, 55.00)...\n",
      "  Fetching range 13/22: Time [55.00, 55.50)...\n",
      "  Fetching range 14/22: Time [55.50, 56.00)...\n",
      "  Fetching range 15/22: Time [56.00, 56.50)...\n",
      "  Fetching range 16/22: Time [56.50, 57.00)...\n",
      "  Fetching range 17/22: Time [57.00, 57.50)...\n",
      "  Fetching range 18/22: Time [57.50, 58.00)...\n",
      "  Fetching range 19/22: Time [58.00, 58.50)...\n",
      "  Fetching range 20/22: Time [58.50, 59.00)...\n",
      "  Fetching range 21/22: Time [59.00, 59.50)...\n",
      "  Fetching range 22/22: Time [59.50, 60.00)...\n",
      "  ✓ range_based_unrestricted/10_men_butterfly_100m_2009: 87 rows, 29909 tokens, 244.17s\n",
      "\n",
      "======================================================================\n",
      "[2/20] Processing table: 12_rock_band_downloadable_2011\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Artist [A, G)...\n",
      "  Fetching range 1/15: Genre Classic Rock...\n",
      "  Fetching range 1/12: Release date [2011-12-01, 2012-01-01)...\n",
      "  Fetching range 2/12: Release date [2011-11-01, 2011-12-01)...\n",
      "  Fetching range 2/4: Artist [G, N)...\n",
      "  Fetching range 2/15: Genre Hard Rock...\n",
      "  Fetching range 3/12: Release date [2011-10-01, 2011-11-01)...\n",
      "  Fetching range 3/15: Genre Alternative Rock...\n",
      "  Fetching range 4/12: Release date [2011-09-01, 2011-10-01)...\n",
      "  Fetching range 4/15: Genre Indie...\n",
      "  Fetching range 3/4: Artist [N, T)...\n",
      "  Fetching range 5/12: Release date [2011-08-01, 2011-09-01)...\n",
      "  Fetching range 5/15: Genre Metal...\n",
      "  Fetching range 4/4: Artist [T, [)...\n",
      "  Fetching range 6/12: Release date [2011-07-01, 2011-08-01)...\n",
      "  Fetching range 6/15: Genre Punk...\n",
      "  Fetching range 7/12: Release date [2011-06-01, 2011-07-01)...\n",
      "  Fetching range 7/15: Genre Pop...\n",
      "  Fetching range 8/12: Release date [2011-05-01, 2011-06-01)...\n",
      "  Fetching range 9/12: Release date [2011-04-01, 2011-05-01)...\n",
      "  Fetching range 8/15: Genre Country...\n",
      "  ✓ range_based_alphabetic/12_rock_band_downloadable_2011: 39 rows, 3537 tokens, 22.60s\n",
      "  Fetching range 9/15: Genre Hip-Hop/Rap...\n",
      "  Fetching range 10/15: Genre R&B/Soul/Funk...\n",
      "  Fetching range 11/15: Genre Electronic/Dance...\n",
      "  Fetching range 10/12: Release date [2011-03-01, 2011-04-01)...\n",
      "  Fetching range 12/15: Genre Blues/Jazz...\n",
      "  Fetching range 13/15: Genre Reggae/World...\n",
      "  Fetching range 11/12: Release date [2011-02-01, 2011-03-01)...\n",
      "  Fetching range 12/12: Release date [2011-01-01, 2011-02-01)...\n",
      "  Fetching range 14/15: Genre Folk/Americana...\n",
      "  Fetching range 15/15: Genre Soundtrack/Novelty/Other...\n",
      "  ✓ range_based_unrestricted/12_rock_band_downloadable_2011: 55 rows, 6801 tokens, 48.79s\n",
      "  ✓ range_based_semantic/12_rock_band_downloadable_2011: 71 rows, 7888 tokens, 53.08s\n",
      "\n",
      "======================================================================\n",
      "[3/20] Processing table: 13_figure_skating_ladies_2009_2010\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Name [A, G)...\n",
      "  Fetching range 1/9: Event World Championships...\n",
      "  Fetching range 1/23: Points [120, 125)...\n",
      "  Fetching range 2/23: Points [125, 130)...\n",
      "  Fetching range 2/4: Name [G, N)...\n",
      "  Fetching range 2/9: Event European Championships...\n",
      "  Fetching range 3/23: Points [130, 135)...\n",
      "  Fetching range 4/23: Points [135, 140)...\n",
      "  Fetching range 3/4: Name [N, T)...\n",
      "  Fetching range 3/9: Event Four Continents Championships...\n",
      "  Fetching range 5/23: Points [140, 145)...\n",
      "  Fetching range 4/4: Name [T, [)...\n",
      "  Fetching range 4/9: Event World Junior Championships...\n",
      "  Fetching range 6/23: Points [145, 150)...\n",
      "  Fetching range 7/23: Points [150, 155)...\n",
      "  ✓ range_based_alphabetic/13_figure_skating_ladies_2009_2010: 29 rows, 2832 tokens, 17.56s\n",
      "  Fetching range 5/9: Event Grand Prix Series...\n",
      "  Fetching range 8/23: Points [155, 160)...\n",
      "  Fetching range 6/9: Event Grand Prix Final...\n",
      "  Fetching range 9/23: Points [160, 165)...\n",
      "  Fetching range 7/9: Event National Championships...\n",
      "  Fetching range 10/23: Points [165, 170)...\n",
      "  Fetching range 8/9: Event ISU Challenger Series / International B Events...\n",
      "  Fetching range 9/9: Event International Invitational / Exhibition Events...\n",
      "  Fetching range 11/23: Points [170, 175)...\n",
      "  Fetching range 12/23: Points [175, 180)...\n",
      "  ✓ range_based_semantic/13_figure_skating_ladies_2009_2010: 32 rows, 5089 tokens, 32.90s\n",
      "  Fetching range 13/23: Points [180, 185)...\n",
      "  Fetching range 14/23: Points [185, 190)...\n",
      "  Fetching range 15/23: Points [190, 195)...\n",
      "  Fetching range 16/23: Points [195, 200)...\n",
      "  Fetching range 17/23: Points [200, 205)...\n",
      "  Fetching range 18/23: Points [205, 210)...\n",
      "  Fetching range 19/23: Points [210, 215)...\n",
      "  Fetching range 20/23: Points [215, 220)...\n",
      "  Fetching range 21/23: Points [220, 225)...\n",
      "  Fetching range 22/23: Points [225, 230)...\n",
      "  Fetching range 23/23: Points [230, 235)...\n",
      "  ✓ range_based_unrestricted/13_figure_skating_ladies_2009_2010: 39 rows, 7695 tokens, 59.41s\n",
      "\n",
      "======================================================================\n",
      "[4/20] Processing table: 16_curling_teams_women_2013_2014\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Skip [A, G)...\n",
      "  Fetching range 1/6: Locale Europe...\n",
      "  Fetching range 1/4: Skip [A, G)...\n",
      "  Fetching range 2/6: Locale North America...\n",
      "  Fetching range 2/4: Skip [G, M)...\n",
      "  Fetching range 3/4: Skip [M, S)...\n",
      "  Fetching range 3/6: Locale Asia...\n",
      "  Fetching range 4/4: Skip [S, [)...\n",
      "  Fetching range 4/6: Locale Oceania...\n",
      "  Fetching range 5/6: Locale South America...\n",
      "  Fetching range 2/4: Skip [G, M)...\n",
      "  Fetching range 6/6: Locale Africa...\n",
      "  ✓ range_based_semantic/16_curling_teams_women_2013_2014: 65 rows, 4415 tokens, 27.53s\n",
      "  ✓ range_based_unrestricted/16_curling_teams_women_2013_2014: 60 rows, 5975 tokens, 35.40s\n",
      "  Fetching range 3/4: Skip [M, S)...\n",
      "  Fetching range 4/4: Skip [S, [)...\n",
      "  ✓ range_based_alphabetic/16_curling_teams_women_2013_2014: 72 rows, 7773 tokens, 89.42s\n",
      "\n",
      "======================================================================\n",
      "[5/20] Processing table: 19_living_proof_the_farewell_tour\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: City [A, G)...\n",
      "  Fetching range 1/8: Venue Stadium...\n",
      "  Fetching range 1/5: date [2023-11, 2023-12)...\n",
      "  Fetching range 2/5: date [2023-12, 2024-01)...\n",
      "  Fetching range 2/8: Venue Arena...\n",
      "  Fetching range 3/5: date [2024-01, 2024-02)...\n",
      "  Fetching range 4/5: date [2024-02, 2024-03)...\n",
      "  Fetching range 5/5: date [2024-03, 2024-04)...\n",
      "  Fetching range 2/4: City [G, M)...\n",
      "  ✓ range_based_unrestricted/19_living_proof_the_farewell_tour: 27 rows, 1557 tokens, 30.78s\n",
      "  Fetching range 3/8: Venue Amphitheater...\n",
      "  Fetching range 4/8: Venue Theater...\n",
      "  Fetching range 3/4: City [M, S)...\n",
      "  Fetching range 5/8: Venue Festival Site / Open-air...\n",
      "  Fetching range 4/4: City [S, [)...\n",
      "  ✓ range_based_alphabetic/19_living_proof_the_farewell_tour: 84 rows, 9632 tokens, 93.52s\n",
      "  Fetching range 6/8: Venue Club / Small Music Hall...\n",
      "  Fetching range 7/8: Venue Convention Center / Exhibition Hall...\n",
      "  Fetching range 8/8: Venue Other / Unspecified...\n",
      "  ✓ range_based_semantic/19_living_proof_the_farewell_tour: 198 rows, 10185 tokens, 138.61s\n",
      "\n",
      "======================================================================\n",
      "[6/20] Processing table: 21_liechtenstein_demographics_1901_2011\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Notes [A, G)...\n",
      "  Fetching range 1/3: Demographic trend Growth...\n",
      "  Fetching range 1/12: year [1901, 1911)...\n",
      "  Fetching range 2/3: Demographic trend Decline...\n",
      "  Fetching range 2/12: year [1911, 1921)...\n",
      "  Fetching range 3/3: Demographic trend Stable...\n",
      "  ✓ range_based_semantic/21_liechtenstein_demographics_1901_2011: 9 rows, 2064 tokens, 11.14s\n",
      "  Fetching range 3/12: year [1921, 1931)...\n",
      "  Fetching range 2/4: Notes [G, N)...\n",
      "  Fetching range 4/12: year [1931, 1941)...\n",
      "  Fetching range 5/12: year [1941, 1951)...\n",
      "  Fetching range 3/4: Notes [N, T)...\n",
      "  Fetching range 6/12: year [1951, 1961)...\n",
      "  Fetching range 4/4: Notes [T, [)...\n",
      "  Fetching range 7/12: year [1961, 1971)...\n",
      "  ✓ range_based_alphabetic/21_liechtenstein_demographics_1901_2011: 49 rows, 8412 tokens, 49.08s\n",
      "  Fetching range 8/12: year [1971, 1981)...\n",
      "  Fetching range 9/12: year [1981, 1991)...\n",
      "  Fetching range 10/12: year [1991, 2001)...\n",
      "  Fetching range 11/12: year [2001, 2011)...\n",
      "  Fetching range 12/12: year [2011, 2012)...\n",
      "  ✓ range_based_unrestricted/21_liechtenstein_demographics_1901_2011: 111 rows, 13323 tokens, 76.57s\n",
      "\n",
      "======================================================================\n",
      "[7/20] Processing table: 22_usa_demographics_1935_2010\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: State [A, G)...\n",
      "  Fetching range 1/4: Region Northeast...\n",
      "  Fetching range 1/9: Year [1935, 1940)...\n",
      "  Fetching range 2/9: Year [1940, 1950)...\n",
      "  Fetching range 2/4: State [G, N)...\n",
      "  Fetching range 2/4: Region Midwest...\n",
      "  Fetching range 3/9: Year [1950, 1960)...\n",
      "  Fetching range 3/4: State [N, T)...\n",
      "  Fetching range 3/4: Region South...\n",
      "  Fetching range 4/4: Region West...\n",
      "  Fetching range 4/9: Year [1960, 1970)...\n",
      "  Fetching range 4/4: State [T, [)...\n",
      "  ✓ range_based_semantic/22_usa_demographics_1935_2010: 10 rows, 4837 tokens, 29.37s\n",
      "  Fetching range 5/9: Year [1970, 1980)...\n",
      "  Fetching range 6/9: Year [1980, 1990)...\n",
      "  ✓ range_based_alphabetic/22_usa_demographics_1935_2010: 28 rows, 7153 tokens, 39.05s\n",
      "  Fetching range 7/9: Year [1990, 2000)...\n",
      "  Fetching range 8/9: Year [2000, 2010)...\n",
      "  Fetching range 9/9: Year [2010, 2020)...\n",
      "  ✓ range_based_unrestricted/22_usa_demographics_1935_2010: 85 rows, 13273 tokens, 72.54s\n",
      "\n",
      "======================================================================\n",
      "[8/20] Processing table: 25_english_latin_rivalry_1887_2012\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Winner [A, G)...\n",
      "  Fetching range 1/3: Winner Boston Latin School win...\n",
      "  Fetching range 1/14: year [1880, 1890)...\n",
      "  Fetching range 2/14: year [1890, 1900)...\n",
      "  Fetching range 3/14: year [1900, 1910)...\n",
      "  Fetching range 4/14: year [1910, 1920)...\n",
      "  Fetching range 5/14: year [1920, 1930)...\n",
      "  Fetching range 6/14: year [1930, 1940)...\n",
      "  Fetching range 2/4: Winner [G, N)...\n",
      "  Fetching range 7/14: year [1940, 1950)...\n",
      "  Fetching range 8/14: year [1950, 1960)...\n",
      "  Fetching range 2/3: Winner English High School win...\n",
      "  Fetching range 9/14: year [1960, 1970)...\n",
      "  Fetching range 10/14: year [1970, 1980)...\n",
      "  Fetching range 11/14: year [1980, 1990)...\n",
      "  Fetching range 3/4: Winner [N, T)...\n",
      "  Fetching range 3/3: Winner Tie/Draw/No Contest/Forfeit...\n",
      "  Fetching range 12/14: year [1990, 2000)...\n",
      "  ✓ range_based_semantic/25_english_latin_rivalry_1887_2012: 119 rows, 4583 tokens, 32.89s\n",
      "  Fetching range 13/14: year [2000, 2010)...\n",
      "  Fetching range 14/14: year [2010, 2020)...\n",
      "  Fetching range 4/4: Winner [T, [)...\n",
      "  ✓ range_based_alphabetic/25_english_latin_rivalry_1887_2012: 87 rows, 7911 tokens, 48.81s\n",
      "  ✓ range_based_unrestricted/25_english_latin_rivalry_1887_2012: 132 rows, 5696 tokens, 51.51s\n",
      "\n",
      "======================================================================\n",
      "[9/20] Processing table: 28_equestrian_2012\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Rider [A, G)...\n",
      "  Fetching range 1/5: Nation Europe...\n",
      "  Fetching range 1/8: Rank [1, 11)...\n",
      "  Fetching range 2/8: Rank [11, 21)...\n",
      "  Fetching range 2/4: Rider [G, M)...\n",
      "  Fetching range 3/8: Rank [21, 31)...\n",
      "  Fetching range 3/4: Rider [M, S)...\n",
      "  Fetching range 2/5: Nation Americas...\n",
      "  Fetching range 4/8: Rank [31, 41)...\n",
      "  Fetching range 3/5: Nation Asia...\n",
      "  Fetching range 4/5: Nation Africa...\n",
      "  Fetching range 5/8: Rank [41, 51)...\n",
      "  Fetching range 5/5: Nation Oceania...\n",
      "  Fetching range 6/8: Rank [51, 61)...\n",
      "  ✓ range_based_semantic/28_equestrian_2012: 49 rows, 4214 tokens, 25.23s\n",
      "  Fetching range 4/4: Rider [S, [)...\n",
      "  Fetching range 7/8: Rank [61, 71)...\n",
      "  ✓ range_based_alphabetic/28_equestrian_2012: 36 rows, 5330 tokens, 31.00s\n",
      "  Fetching range 8/8: Rank [71, 81)...\n",
      "  ✓ range_based_unrestricted/28_equestrian_2012: 58 rows, 5731 tokens, 36.84s\n",
      "\n",
      "======================================================================\n",
      "[10/20] Processing table: 29_tennessee_vanderbilt_rivalry_1900_2012\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Location [A, G)...\n",
      "  Fetching range 1/3: Location Tennessee home...\n",
      "  Fetching range 1/12: year [1900, 1910)...\n",
      "  Fetching range 2/12: year [1910, 1920)...\n",
      "  Fetching range 3/12: year [1920, 1930)...\n",
      "  Fetching range 2/4: Location [G, N)...\n",
      "  Fetching range 2/3: Location Vanderbilt home...\n",
      "  Fetching range 4/12: year [1930, 1940)...\n",
      "  Fetching range 3/4: Location [N, T)...\n",
      "  Fetching range 5/12: year [1940, 1950)...\n",
      "  Fetching range 3/3: Location Neutral/other locations...\n",
      "  Fetching range 6/12: year [1950, 1960)...\n",
      "  Fetching range 4/4: Location [T, [)...\n",
      "  ✓ range_based_semantic/29_tennessee_vanderbilt_rivalry_1900_2012: 56 rows, 3356 tokens, 19.89s\n",
      "  Fetching range 7/12: year [1960, 1970)...\n",
      "  ✓ range_based_alphabetic/29_tennessee_vanderbilt_rivalry_1900_2012: 21 rows, 4201 tokens, 24.87s\n",
      "  Fetching range 8/12: year [1970, 1980)...\n",
      "  Fetching range 9/12: year [1980, 1990)...\n",
      "  Fetching range 10/12: year [1990, 2000)...\n",
      "  Fetching range 11/12: year [2000, 2010)...\n",
      "  Fetching range 12/12: year [2010, 2013)...\n",
      "  ✓ range_based_unrestricted/29_tennessee_vanderbilt_rivalry_1900_2012: 101 rows, 6493 tokens, 50.66s\n",
      "\n",
      "======================================================================\n",
      "[11/20] Processing table: 2_belgium_demographics_1900_2011\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Province [A, G)...\n",
      "  Fetching range 1/5: Fertility category Very low...\n",
      "  Fetching range 1/12: Year [1900, 1910)...\n",
      "  Fetching range 2/5: Fertility category Low...\n",
      "  Fetching range 2/12: Year [1910, 1920)...\n",
      "  Fetching range 2/4: Province [G, N)...\n",
      "  Fetching range 3/12: Year [1920, 1930)...\n",
      "  Fetching range 3/5: Fertility category Near-replacement...\n",
      "  Fetching range 4/12: Year [1930, 1940)...\n",
      "  Fetching range 3/4: Province [N, T)...\n",
      "  Fetching range 5/12: Year [1940, 1950)...\n",
      "  Fetching range 4/5: Fertility category Above-replacement...\n",
      "  Fetching range 4/4: Province [T, [)...\n",
      "  Fetching range 6/12: Year [1950, 1960)...\n",
      "  Fetching range 5/5: Fertility category High...\n",
      "  ✗ range_based_semantic/2_belgium_demographics_1900_2011: CSV save failed: Length mismatch: Expected axis has 10 elements, new values have 9 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/s8/dzjx_6bx24960zc7sd87xsq400xh_z/T/ipykernel_33115/1136395299.py\", line 125, in process_strategy\n",
      "    df.columns = df_ref.columns\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py\", line 6313, in __setattr__\n",
      "    return object.__setattr__(self, name, value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"properties.pyx\", line 69, in pandas._libs.properties.AxisProperty.__set__\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py\", line 814, in _set_axis\n",
      "    self._mgr.set_axis(axis, labels)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 238, in set_axis\n",
      "    self._validate_set_axis(axis, new_labels)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/base.py\", line 98, in _validate_set_axis\n",
      "    raise ValueError(\n",
      "ValueError: Length mismatch: Expected axis has 10 elements, new values have 9 elements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching range 7/12: Year [1960, 1970)...\n",
      "  ✓ range_based_alphabetic/2_belgium_demographics_1900_2011: 16 rows, 8668 tokens, 46.78s\n",
      "  Fetching range 8/12: Year [1970, 1980)...\n",
      "  Fetching range 9/12: Year [1980, 1990)...\n",
      "  Fetching range 10/12: Year [1990, 2000)...\n",
      "  Fetching range 11/12: Year [2000, 2010)...\n",
      "  Fetching range 12/12: Year [2010, 2012)...\n",
      "  ✓ range_based_unrestricted/2_belgium_demographics_1900_2011: 111 rows, 16408 tokens, 90.14s\n",
      "\n",
      "======================================================================\n",
      "[12/20] Processing table: 32_fa_cup_qualifying_rounds_1999_2000\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Home Team [A, G)...\n",
      "  Fetching range 1/5: Home Team Premier League (top-tier professional clubs)...\n",
      "  Fetching range 1/43: Tie [Preliminary:1, Preliminary:26)...\n",
      "  Fetching range 2/5: Home Team Football League (professional clubs)...\n",
      "  Fetching range 2/43: Tie [Preliminary:26, Preliminary:51)...\n",
      "  Fetching range 2/4: Home Team [G, N)...\n",
      "  Fetching range 3/5: Home Team National League / Conference (national non-league clubs)...\n",
      "  Fetching range 3/43: Tie [Preliminary:51, Preliminary:76)...\n",
      "  Fetching range 4/43: Tie [Preliminary:76, Preliminary:101)...\n",
      "  Fetching range 3/4: Home Team [N, T)...\n",
      "  Fetching range 4/5: Home Team Regional semi-professional leagues (e.g., Isthmian, Southern, Northern Premier)...\n",
      "  Fetching range 5/43: Tie [Preliminary:101, Preliminary:126)...\n",
      "  Fetching range 6/43: Tie [Preliminary:126, Preliminary:151)...\n",
      "  Fetching range 5/5: Home Team County and amateur leagues (local grassroots clubs)...\n",
      "  Fetching range 4/4: Home Team [T, [)...\n",
      "  Fetching range 7/43: Tie [Preliminary:151, Preliminary:176)...\n",
      "  ✓ range_based_alphabetic/32_fa_cup_qualifying_rounds_1999_2000: 75 rows, 7173 tokens, 49.25s\n",
      "  Fetching range 8/43: Tie [Preliminary:176, Preliminary:201)...\n",
      "  Fetching range 9/43: Tie [Preliminary:201, Preliminary:226)...\n",
      "  Fetching range 10/43: Tie [Preliminary:226, Preliminary:251)...\n",
      "  Fetching range 11/43: Tie [Preliminary:251, Preliminary:276)...\n",
      "  ✓ range_based_semantic/32_fa_cup_qualifying_rounds_1999_2000: 171 rows, 9813 tokens, 69.17s\n",
      "  Fetching range 12/43: Tie [Preliminary:276, Preliminary:301)...\n",
      "  Fetching range 13/43: Tie [Preliminary:301, Preliminary:326)...\n",
      "  Fetching range 14/43: Tie [Preliminary:326, Preliminary:351)...\n",
      "  Fetching range 15/43: Tie [Preliminary:351, Preliminary:376)...\n",
      "  Fetching range 16/43: Tie [Preliminary:376, Preliminary:401)...\n",
      "  Fetching range 17/43: Tie [Preliminary:401, Preliminary:426)...\n",
      "  Fetching range 18/43: Tie [Preliminary:426, Preliminary:451)...\n",
      "  Fetching range 19/43: Tie [Preliminary:451, Preliminary:476)...\n",
      "  Fetching range 20/43: Tie [Preliminary:476, Preliminary:501)...\n",
      "  Fetching range 21/43: Tie [Preliminary:501, Preliminary:513)...\n",
      "  Fetching range 22/43: Tie [1st Qualifying:1, 1st Qualifying:26)...\n",
      "  Fetching range 23/43: Tie [1st Qualifying:26, 1st Qualifying:51)...\n",
      "  Fetching range 24/43: Tie [1st Qualifying:51, 1st Qualifying:76)...\n",
      "  Fetching range 25/43: Tie [1st Qualifying:76, 1st Qualifying:101)...\n",
      "  Fetching range 26/43: Tie [1st Qualifying:101, 1st Qualifying:126)...\n",
      "  Fetching range 27/43: Tie [1st Qualifying:126, 1st Qualifying:151)...\n",
      "  Fetching range 28/43: Tie [1st Qualifying:151, 1st Qualifying:176)...\n",
      "  Fetching range 29/43: Tie [1st Qualifying:176, 1st Qualifying:201)...\n",
      "  Fetching range 30/43: Tie [1st Qualifying:201, 1st Qualifying:226)...\n",
      "  Fetching range 31/43: Tie [1st Qualifying:226, 1st Qualifying:251)...\n",
      "  Fetching range 32/43: Tie [1st Qualifying:251, 1st Qualifying:257)...\n",
      "  Fetching range 33/43: Tie [2nd Qualifying:1, 2nd Qualifying:26)...\n",
      "  Fetching range 34/43: Tie [2nd Qualifying:26, 2nd Qualifying:51)...\n",
      "  Fetching range 35/43: Tie [2nd Qualifying:51, 2nd Qualifying:76)...\n",
      "  Fetching range 36/43: Tie [2nd Qualifying:76, 2nd Qualifying:101)...\n",
      "  Fetching range 37/43: Tie [2nd Qualifying:101, 2nd Qualifying:126)...\n",
      "  Fetching range 38/43: Tie [2nd Qualifying:126, 2nd Qualifying:129)...\n",
      "  Fetching range 39/43: Tie [3rd Qualifying:1, 3rd Qualifying:26)...\n",
      "  Fetching range 40/43: Tie [3rd Qualifying:26, 3rd Qualifying:51)...\n",
      "  Fetching range 41/43: Tie [3rd Qualifying:51, 3rd Qualifying:65)...\n",
      "  Fetching range 42/43: Tie [4th Qualifying:1, 4th Qualifying:26)...\n",
      "  Fetching range 43/43: Tie [4th Qualifying:26, 4th Qualifying:33)...\n",
      "  ✓ range_based_unrestricted/32_fa_cup_qualifying_rounds_1999_2000: 329 rows, 46338 tokens, 332.91s\n",
      "\n",
      "======================================================================\n",
      "[13/20] Processing table: 33_portuguese_grape_varieties\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Grape [A, G)...\n",
      "  Fetching range 1/4: Color White...\n",
      "  Fetching range 1/8: Grape [A, D)...\n",
      "  Fetching range 2/4: Grape [G, N)...\n",
      "  Fetching range 2/8: Grape [D, G)...\n",
      "  Fetching range 2/4: Color Red...\n",
      "  Fetching range 3/4: Grape [N, T)...\n",
      "  Fetching range 3/8: Grape [G, J)...\n",
      "  Fetching range 3/4: Color Rosé/Pink...\n",
      "  Fetching range 4/8: Grape [J, M)...\n",
      "  Fetching range 4/4: Grape [T, [)...\n",
      "  Fetching range 5/8: Grape [M, P)...\n",
      "  ✓ range_based_alphabetic/33_portuguese_grape_varieties: 27 rows, 949 tokens, 8.50s\n",
      "  Fetching range 6/8: Grape [P, S)...\n",
      "  Fetching range 7/8: Grape [S, V)...\n",
      "  Fetching range 4/4: Color Other/Unknown...\n",
      "  ✓ range_based_semantic/33_portuguese_grape_varieties: 39 rows, 3211 tokens, 21.06s\n",
      "  Fetching range 8/8: Grape [V, [)...\n",
      "  ✓ range_based_unrestricted/33_portuguese_grape_varieties: 92 rows, 5845 tokens, 41.36s\n",
      "\n",
      "======================================================================\n",
      "[14/20] Processing table: 35_guitar_hero_5_songs\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Song [A, G)...\n",
      "  Fetching range 1/10: Genre Rock...\n",
      "  Fetching range 1/5: year [1960, 1970)...\n",
      "  Fetching range 2/5: year [1970, 1980)...\n",
      "  Fetching range 3/5: year [1980, 1990)...\n",
      "  Fetching range 2/10: Genre Metal...\n",
      "  Fetching range 2/4: Song [G, N)...\n",
      "  Fetching range 4/5: year [1990, 2000)...\n",
      "  Fetching range 5/5: year [2000, 2010)...\n",
      "  Fetching range 3/4: Song [N, T)...\n",
      "  Fetching range 3/10: Genre Pop...\n",
      "  Fetching range 4/4: Song [T, [)...\n",
      "  ✓ range_based_unrestricted/35_guitar_hero_5_songs: 53 rows, 4109 tokens, 25.14s\n",
      "  ✓ range_based_alphabetic/35_guitar_hero_5_songs: 42 rows, 4517 tokens, 26.54s\n",
      "  Fetching range 4/10: Genre Blues...\n",
      "  Fetching range 5/10: Genre Country...\n",
      "  Fetching range 6/10: Genre Electronic/Dance...\n",
      "  Fetching range 7/10: Genre Funk/Soul/R&B...\n",
      "  Fetching range 8/10: Genre Jazz/Progressive...\n",
      "  Fetching range 9/10: Genre World/Latin/Reggae...\n",
      "  Fetching range 10/10: Genre Other/Novelty...\n",
      "  ✓ range_based_semantic/35_guitar_hero_5_songs: 71 rows, 5908 tokens, 53.76s\n",
      "\n",
      "======================================================================\n",
      "[15/20] Processing table: 36_south_cambridgeshire_district_council_1973_2012\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Ward [A, G)...\n",
      "  Fetching range 1/5: Political party Right...\n",
      "  Fetching range 1/5: year [1973, 1980)...\n",
      "  Fetching range 2/5: year [1980, 1990)...\n",
      "  Fetching range 3/5: year [1990, 2000)...\n",
      "  Fetching range 4/5: year [2000, 2010)...\n",
      "  Fetching range 2/4: Ward [G, N)...\n",
      "  Fetching range 5/5: year [2010, 2013)...\n",
      "  ✓ range_based_unrestricted/36_south_cambridgeshire_district_council_1973_2012: 29 rows, 3445 tokens, 22.39s\n",
      "  Fetching range 3/4: Ward [N, T)...\n",
      "  Fetching range 2/5: Political party Centre...\n",
      "  Fetching range 4/4: Ward [T, [)...\n",
      "  ✓ range_based_alphabetic/36_south_cambridgeshire_district_council_1973_2012: 28 rows, 7722 tokens, 47.20s\n",
      "  Fetching range 3/5: Political party Left...\n",
      "  Fetching range 4/5: Political party Non-affiliated...\n",
      "  Fetching range 5/5: Political party Vacancy...\n",
      "  ✓ range_based_semantic/36_south_cambridgeshire_district_council_1973_2012: 27 rows, 8642 tokens, 75.02s\n",
      "\n",
      "======================================================================\n",
      "[16/20] Processing table: 38_ship_launches_january_1944\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Ship [A, G)...\n",
      "  Fetching range 1/14: Class / type Battleships...\n",
      "  Fetching range 1/4: Date [1944-01-01, 1944-01-08)...\n",
      "  Fetching range 2/14: Class / type Cruisers...\n",
      "  Fetching range 2/4: Ship [G, M)...\n",
      "  Fetching range 2/4: Date [1944-01-08, 1944-01-15)...\n",
      "  Fetching range 3/14: Class / type Destroyers...\n",
      "  Fetching range 3/4: Ship [M, S)...\n",
      "  Fetching range 3/4: Date [1944-01-15, 1944-01-22)...\n",
      "  Fetching range 4/14: Class / type Frigates...\n",
      "  Fetching range 4/4: Ship [S, [)...\n",
      "  Fetching range 4/4: Date [1944-01-22, 1944-02-01)...\n",
      "  Fetching range 5/14: Class / type Corvettes...\n",
      "  ✓ range_based_unrestricted/38_ship_launches_january_1944: 5 rows, 886 tokens, 11.28s\n",
      "  Fetching range 6/14: Class / type Submarines...\n",
      "  ✓ range_based_alphabetic/38_ship_launches_january_1944: 8 rows, 1019 tokens, 12.78s\n",
      "  Fetching range 7/14: Class / type Aircraft carriers...\n",
      "  Fetching range 8/14: Class / type Escort carriers...\n",
      "  Fetching range 9/14: Class / type Escort vessels...\n",
      "  Fetching range 10/14: Class / type Minesweepers...\n",
      "  Fetching range 11/14: Class / type Patrol craft...\n",
      "  Fetching range 12/14: Class / type Auxiliaries...\n",
      "  Fetching range 13/14: Class / type Merchant vessels...\n",
      "  Fetching range 14/14: Class / type Support vessels...\n",
      "  ✓ range_based_semantic/38_ship_launches_january_1944: 17 rows, 2997 tokens, 31.70s\n",
      "\n",
      "======================================================================\n",
      "[17/20] Processing table: 39_uk_demographics_1960_2012\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: County [A, G)...\n",
      "  Fetching range 1/5: Fertility category Very low fertility...\n",
      "  Fetching range 1/6: Year [1960, 1970)...\n",
      "  Fetching range 2/5: Fertility category Low fertility...\n",
      "  Fetching range 2/4: County [G, N)...\n",
      "  Fetching range 2/6: Year [1970, 1980)...\n",
      "  Fetching range 3/4: County [N, T)...\n",
      "  Fetching range 3/5: Fertility category Near-replacement fertility...\n",
      "  Fetching range 3/6: Year [1980, 1990)...\n",
      "  Fetching range 4/6: Year [1990, 2000)...\n",
      "  Fetching range 4/4: County [T, [)...\n",
      "  Fetching range 4/5: Fertility category Replacement fertility...\n",
      "  Fetching range 5/6: Year [2000, 2010)...\n",
      "  Fetching range 5/5: Fertility category High fertility...\n",
      "  ✓ range_based_semantic/39_uk_demographics_1960_2012: 37 rows, 8603 tokens, 46.00s\n",
      "  ✓ range_based_alphabetic/39_uk_demographics_1960_2012: 20 rows, 8846 tokens, 50.11s\n",
      "  Fetching range 6/6: Year [2010, 2020)...\n",
      "  ✗ range_based_unrestricted/39_uk_demographics_1960_2012: CSV save failed: Length mismatch: Expected axis has 12 elements, new values have 9 elements\n",
      "\n",
      "======================================================================\n",
      "[18/20] Processing table: 41_new_zealand_football_results_1922_2012\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Opponent [A, G)...\n",
      "  Fetching range 1/5: Opponent Oceania...\n",
      "  Fetching range 1/10: Year [1920, 1930)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/s8/dzjx_6bx24960zc7sd87xsq400xh_z/T/ipykernel_33115/1136395299.py\", line 125, in process_strategy\n",
      "    df.columns = df_ref.columns\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py\", line 6313, in __setattr__\n",
      "    return object.__setattr__(self, name, value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"properties.pyx\", line 69, in pandas._libs.properties.AxisProperty.__set__\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py\", line 814, in _set_axis\n",
      "    self._mgr.set_axis(axis, labels)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 238, in set_axis\n",
      "    self._validate_set_axis(axis, new_labels)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/base.py\", line 98, in _validate_set_axis\n",
      "    raise ValueError(\n",
      "ValueError: Length mismatch: Expected axis has 12 elements, new values have 9 elements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching range 2/10: Year [1930, 1940)...\n",
      "  Fetching range 2/4: Opponent [G, N)...\n",
      "  Fetching range 3/10: Year [1940, 1950)...\n",
      "  Fetching range 4/10: Year [1950, 1960)...\n",
      "  Fetching range 2/5: Opponent Asia...\n",
      "  Fetching range 5/10: Year [1960, 1970)...\n",
      "  Fetching range 3/4: Opponent [N, T)...\n",
      "  Fetching range 3/5: Opponent Europe...\n",
      "  Fetching range 6/10: Year [1970, 1980)...\n",
      "  Fetching range 4/5: Opponent Americas...\n",
      "  Fetching range 4/4: Opponent [T, [)...\n",
      "  Fetching range 7/10: Year [1980, 1990)...\n",
      "  Fetching range 5/5: Opponent Africa...\n",
      "  ✓ range_based_alphabetic/41_new_zealand_football_results_1922_2012: 24 rows, 3279 tokens, 24.27s\n",
      "  ✓ range_based_semantic/41_new_zealand_football_results_1922_2012: 43 rows, 3441 tokens, 24.41s\n",
      "  Fetching range 8/10: Year [1990, 2000)...\n",
      "  Fetching range 9/10: Year [2000, 2010)...\n",
      "  Fetching range 10/10: Year [2010, 2020)...\n",
      "  ✓ range_based_unrestricted/41_new_zealand_football_results_1922_2012: 62 rows, 4400 tokens, 42.30s\n",
      "\n",
      "======================================================================\n",
      "[19/20] Processing table: 42_jack_nicklaus_achievements_1962_2005\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Tournament Name [A, G)...\n",
      "  Fetching range 1/5: Event type Major championship...\n",
      "  Fetching range 1/5: year [1962, 1970)...\n",
      "  Fetching range 2/5: year [1970, 1980)...\n",
      "  Fetching range 2/4: Tournament Name [G, N)...\n",
      "  Fetching range 2/5: Event type PGA Tour regular event...\n",
      "  Fetching range 3/5: year [1980, 1990)...\n",
      "  Fetching range 3/4: Tournament Name [N, T)...\n",
      "  Fetching range 4/5: year [1990, 2000)...\n",
      "  Fetching range 5/5: year [2000, 2006)...\n",
      "  Fetching range 3/5: Event type Senior/Champions Tour event...\n",
      "  Fetching range 4/4: Tournament Name [T, [)...\n",
      "  ✓ range_based_unrestricted/42_jack_nicklaus_achievements_1962_2005: 44 rows, 4094 tokens, 26.16s\n",
      "  Fetching range 4/5: Event type Invitational/exhibition...\n",
      "  ✓ range_based_alphabetic/42_jack_nicklaus_achievements_1962_2005: 19 rows, 5663 tokens, 33.08s\n",
      "  Fetching range 5/5: Event type Other...\n",
      "  ✓ range_based_semantic/42_jack_nicklaus_achievements_1962_2005: 34 rows, 5630 tokens, 39.71s\n",
      "\n",
      "======================================================================\n",
      "[20/20] Processing table: 51_just_dance_kids_2_tracks\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Song [A, G)...\n",
      "  Fetching range 1/18: Song Filters Pop...\n",
      "  Fetching range 1/6: Year [1960, 1970)...\n",
      "  Fetching range 2/6: Year [1970, 1980)...\n",
      "  Fetching range 3/6: Year [1980, 1990)...\n",
      "  Fetching range 4/6: Year [1990, 2000)...\n",
      "  Fetching range 5/6: Year [2000, 2010)...\n",
      "  Fetching range 2/18: Song Filters Rock...\n",
      "  Fetching range 2/4: Song [G, N)...\n",
      "  Fetching range 3/18: Song Filters Hip-Hop...\n",
      "  Fetching range 4/18: Song Filters Dance...\n",
      "  Fetching range 5/18: Song Filters R&B...\n",
      "  Fetching range 6/18: Song Filters Movie...\n",
      "  Fetching range 3/4: Song [N, T)...\n",
      "  Fetching range 7/18: Song Filters TV...\n",
      "  Fetching range 8/18: Song Filters Cartoon/Theme song...\n",
      "  Fetching range 6/6: Year [2010, 2020)...\n",
      "  Fetching range 9/18: Song Filters Kids...\n",
      "  Fetching range 4/4: Song [T, [)...\n",
      "  ✓ range_based_alphabetic/51_just_dance_kids_2_tracks: 43 rows, 7043 tokens, 43.03s\n",
      "  Fetching range 10/18: Song Filters Family...\n",
      "  ✓ range_based_unrestricted/51_just_dance_kids_2_tracks: 47 rows, 6078 tokens, 50.19s\n",
      "  Fetching range 11/18: Song Filters Holiday...\n",
      "  Fetching range 12/18: Song Filters Educational...\n",
      "  Fetching range 13/18: Song Filters English...\n",
      "  Fetching range 14/18: Song Filters Spanish...\n",
      "  Fetching range 15/18: Song Filters Other...\n",
      "  Fetching range 16/18: Song Filters Upbeat...\n",
      "  Fetching range 17/18: Song Filters Ballad...\n",
      "  Fetching range 18/18: Song Filters Mid-tempo...\n",
      "  ✓ range_based_semantic/51_just_dance_kids_2_tracks: 49 rows, 19321 tokens, 214.66s\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Processed: 60 strategy-table combinations\n",
      "Successful: 60\n",
      "Failed: 0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = OUTPUT_ROOT / timestamp\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create subdirectories for each strategy\n",
    "for strategy_name in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_dir = output_dir / strategy_name\n",
    "    strategy_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Group strategies by table_id for parallel execution\n",
    "from collections import defaultdict\n",
    "tables_with_strategies = defaultdict(list)\n",
    "for strategy_name, plan in strategies_to_process:\n",
    "    table_id = plan['table_id']\n",
    "    tables_with_strategies[table_id].append((strategy_name, plan))\n",
    "\n",
    "print(f\"\\nGrouped into {len(tables_with_strategies)} tables with strategies\")\n",
    "\n",
    "# Process each table with parallel strategy execution\n",
    "results_summary = []\n",
    "\n",
    "\n",
    "def process_strategy(strategy_name: str, plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single strategy-table combination.\"\"\"\n",
    "    table_id = plan['table_id']\n",
    "    table_name = plan['table_name']\n",
    "    \n",
    "    try:\n",
    "        # Execute fetching strategy\n",
    "        fetch_func = STRATEGY_FUNCTIONS[strategy_name]\n",
    "        fetch_result = fetch_func(plan)\n",
    "        \n",
    "        pages = fetch_result['pages']\n",
    "        all_rows = fetch_result['all_rows']\n",
    "        \n",
    "        # Calculate aggregated metrics\n",
    "        total_pages = len(pages)\n",
    "        successful_pages = sum(1 for p in pages if p.get('parse_success', False))\n",
    "        failed_pages = total_pages - successful_pages\n",
    "        \n",
    "        total_latency = sum(p.get('latency', 0) for p in pages)\n",
    "        avg_latency = total_latency / total_pages if total_pages > 0 else 0\n",
    "        \n",
    "        total_tokens = sum(p.get('total_tokens', 0) for p in pages)\n",
    "        total_llm_calls = sum(1 + p.get('retry_count', 0) for p in pages)\n",
    "        \n",
    "        total_rows_fetched = len(all_rows)\n",
    "        \n",
    "        # Check for duplicate rows\n",
    "        unique_rows = set()\n",
    "        duplicate_count = 0\n",
    "        for row in all_rows:\n",
    "            row_key = tuple(sorted(row.items()))\n",
    "            if row_key in unique_rows:\n",
    "                duplicate_count += 1\n",
    "            unique_rows.add(row_key)\n",
    "        \n",
    "        # Check column consistency\n",
    "        if all_rows:\n",
    "            column_sets = [set(row.keys()) for row in all_rows]\n",
    "            columns_consistent = all(cs == column_sets[0] for cs in column_sets)\n",
    "        else:\n",
    "            columns_consistent = True\n",
    "        \n",
    "        error_rate = failed_pages / total_pages if total_pages > 0 else 0\n",
    "        \n",
    "        # Build execution summary\n",
    "        execution_summary = {\n",
    "            'table_id': table_id,\n",
    "            'table_name': table_name,\n",
    "            'strategy': strategy_name,\n",
    "            'metadata': plan['metadata'],\n",
    "            'pagination_config': plan['pagination_config'],\n",
    "            'execution_metadata': {\n",
    "                'timestamp': timestamp,\n",
    "                'total_pages': total_pages,\n",
    "                'successful_pages': successful_pages,\n",
    "                'failed_pages': failed_pages,\n",
    "                'total_llm_calls': total_llm_calls,\n",
    "                'total_latency': round(total_latency, 3),\n",
    "                'avg_latency': round(avg_latency, 3),\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_rows_fetched': total_rows_fetched,\n",
    "                'unique_rows': len(unique_rows),\n",
    "                'duplicate_rows': duplicate_count,\n",
    "                'columns_consistent': columns_consistent,\n",
    "                'error_rate': round(error_rate, 4)\n",
    "            },\n",
    "            'pages': pages\n",
    "        }\n",
    "        \n",
    "        # Save JSON log\n",
    "        json_path = output_dir / strategy_name / f\"{table_id}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(execution_summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save CSV if we have data (matching B004 approach: normalize columns to match reference)\n",
    "        if all_rows:\n",
    "            try:\n",
    "                # Load reference table to get proper column names\n",
    "                ref_csv_path = LATEST_DATA_DIR / f\"{table_id}.csv\"\n",
    "                df_ref = pd.read_csv(ref_csv_path)\n",
    "                \n",
    "                # Create dataframe from fetched rows\n",
    "                df = pd.DataFrame(all_rows)\n",
    "                \n",
    "                # Normalize fetched column names\n",
    "                df.columns = [normalize_field(col) for col in df.columns]\n",
    "                \n",
    "                # Normalize reference column names to create mapping\n",
    "                norm_ref_cols = [normalize_field(col) for col in df_ref.columns]\n",
    "                \n",
    "                # Ensure fetched df has same columns as reference (reorder and add missing)\n",
    "                missing_cols = [col for col in norm_ref_cols if col not in df.columns]\n",
    "                for col in missing_cols:\n",
    "                    df[col] = None  # Add missing columns with None\n",
    "                \n",
    "                # Reorder to match reference\n",
    "                df = df[norm_ref_cols]\n",
    "                \n",
    "                # Restore original column names from reference\n",
    "                df.columns = df_ref.columns\n",
    "                \n",
    "                # Drop duplicates based on key columns\n",
    "                key_columns = plan['metadata']['key_columns']\n",
    "                if key_columns:\n",
    "                    df = df.drop_duplicates(subset=key_columns)\n",
    "                \n",
    "                csv_path = output_dir / strategy_name / f\"{table_id}.csv\"\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "                print(f\"  ✓ {strategy_name}/{table_id}: {len(df)} rows, {total_tokens} tokens, {total_latency:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ {strategy_name}/{table_id}: CSV save failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"  ⚠ {strategy_name}/{table_id}: No rows to save\")\n",
    "        \n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'table_id': table_id,\n",
    "            'success': True,\n",
    "            **execution_summary['execution_metadata']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {strategy_name}/{table_id}: Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'table_id': table_id,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Process each table with parallel strategies\n",
    "for table_num, (table_id, strategies) in enumerate(tables_with_strategies.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{table_num}/{len(tables_with_strategies)}] Processing table: {table_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Strategies: {', '.join(s for s, _ in strategies)}\")\n",
    "    \n",
    "    if PARALLEL_STRATEGIES:\n",
    "        # Execute strategies in parallel\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {\n",
    "                executor.submit(process_strategy, strategy_name, plan): (strategy_name, plan['table_id'])\n",
    "                for strategy_name, plan in strategies\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                results_summary.append(result)\n",
    "    else:\n",
    "        # Sequential execution (for debugging)\n",
    "        for strategy_name, plan in strategies:\n",
    "            print(f\"\\n  {strategy_name.upper()}\")\n",
    "            result = process_strategy(strategy_name, plan)\n",
    "            results_summary.append(result)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed: {len(results_summary)} strategy-table combinations\")\n",
    "print(f\"Successful: {sum(1 for r in results_summary if r['success'])}\")\n",
    "print(f\"Failed: {sum(1 for r in results_summary if not r['success'])}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb5616",
   "metadata": {},
   "source": [
    "## Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c0ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXECUTION SUMMARY\n",
      "======================================================================\n",
      "Total execution time: 30.93 minutes\n",
      "Tables processed: 20\n",
      "Strategy combinations: 60 successful, 0 failed\n",
      "Total pages fetched: 443\n",
      "Total LLM calls: 443\n",
      "Total tokens used: 444,318\n",
      "Total rows fetched: 7132\n",
      "Avg latency per page: 7.31s\n",
      "Avg tokens per call: 1003.0\n",
      "\n",
      "Final summary saved to processing/2_fetching/20251016_001443/_summary.json\n",
      "All results saved to processing/2_fetching/20251016_001443\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate aggregate statistics across all strategies\n",
    "execution_end_time = datetime.now()\n",
    "# Parse timestamp: format is YYYYMMDD_HHMMSS\n",
    "execution_start_time = datetime.strptime(timestamp, '%Y%m%d_%H%M%S')\n",
    "\n",
    "total_execution_time = (execution_end_time - execution_start_time).total_seconds()\n",
    "\n",
    "# Aggregate metrics\n",
    "total_tables_processed = len(set(r['table_id'] for r in results_summary if r['success']))\n",
    "total_strategies_run = len([r for r in results_summary if r['success']])\n",
    "total_failures = len([r for r in results_summary if not r['success']])\n",
    "\n",
    "successful_results = [r for r in results_summary if r.get('success', False)]\n",
    "\n",
    "total_pages_fetched = sum(r.get('total_pages', 0) for r in successful_results)\n",
    "total_llm_calls_made = sum(r.get('total_llm_calls', 0) for r in successful_results)\n",
    "total_tokens_used = sum(r.get('total_tokens', 0) for r in successful_results)\n",
    "total_latency_seconds = sum(r.get('total_latency', 0) for r in successful_results)\n",
    "total_rows_fetched = sum(r.get('total_rows_fetched', 0) for r in successful_results)\n",
    "\n",
    "# Calculate averages\n",
    "avg_latency_per_page = total_latency_seconds / total_pages_fetched if total_pages_fetched > 0 else 0\n",
    "avg_tokens_per_call = total_tokens_used / total_llm_calls_made if total_llm_calls_made > 0 else 0\n",
    "avg_pages_per_strategy = total_pages_fetched / total_strategies_run if total_strategies_run > 0 else 0\n",
    "\n",
    "# Collect all errors\n",
    "errors_list = [\n",
    "    {\n",
    "        'strategy': r['strategy'],\n",
    "        'table_id': r['table_id'],\n",
    "        'error': r.get('error', 'Unknown error')\n",
    "    }\n",
    "    for r in results_summary if not r.get('success', False)\n",
    "]\n",
    "\n",
    "# Strategy breakdown\n",
    "strategy_breakdown = {}\n",
    "for strategy in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_results = [r for r in successful_results if r.get('strategy') == strategy]\n",
    "    if strategy_results:\n",
    "        strategy_breakdown[strategy] = {\n",
    "            'tables_processed': len(strategy_results),\n",
    "            'total_pages': sum(r.get('total_pages', 0) for r in strategy_results),\n",
    "            'total_tokens': sum(r.get('total_tokens', 0) for r in strategy_results),\n",
    "            'total_latency': round(sum(r.get('total_latency', 0) for r in strategy_results), 3),\n",
    "            'avg_latency': round(sum(r.get('avg_latency', 0) for r in strategy_results) / len(strategy_results), 3),\n",
    "            'total_rows': sum(r.get('total_rows_fetched', 0) for r in strategy_results),\n",
    "            'error_rate': round(sum(r.get('error_rate', 0) for r in strategy_results) / len(strategy_results), 4)\n",
    "        }\n",
    "\n",
    "# Build comprehensive summary\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'execution_time': {\n",
    "        'start': timestamp,\n",
    "        'end': execution_end_time.strftime('%Y%m%d_%H%M%S'),\n",
    "        'total_seconds': round(total_execution_time, 2),\n",
    "        'total_minutes': round(total_execution_time / 60, 2)\n",
    "    },\n",
    "    'configuration': {\n",
    "        'strategy_directory': str(LATEST_STRATEGY_DIR),\n",
    "        'max_tables': MAX_TABLES,\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'max_retries': MAX_RETRIES,\n",
    "        'max_pagination_pages': MAX_PAGINATION_PAGES,\n",
    "        'model': MODEL\n",
    "    },\n",
    "    'aggregate_metrics': {\n",
    "        'total_tables_processed': total_tables_processed,\n",
    "        'total_strategy_table_combinations': total_strategies_run,\n",
    "        'total_failures': total_failures,\n",
    "        'success_rate': round(total_strategies_run / (total_strategies_run + total_failures), 4) if (total_strategies_run + total_failures) > 0 else 0,\n",
    "        'total_pages_fetched': total_pages_fetched,\n",
    "        'total_llm_calls': total_llm_calls_made,\n",
    "        'total_tokens_used': total_tokens_used,\n",
    "        'total_latency_seconds': round(total_latency_seconds, 2),\n",
    "        'total_rows_fetched': total_rows_fetched,\n",
    "        'avg_latency_per_page': round(avg_latency_per_page, 3),\n",
    "        'avg_tokens_per_call': round(avg_tokens_per_call, 1),\n",
    "        'avg_pages_per_strategy': round(avg_pages_per_strategy, 1)\n",
    "    },\n",
    "    'strategy_breakdown': strategy_breakdown,\n",
    "    'errors': errors_list,\n",
    "    'detailed_results': results_summary\n",
    "}\n",
    "\n",
    "summary_file = output_dir / '_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total execution time: {summary['execution_time']['total_minutes']:.2f} minutes\")\n",
    "print(f\"Tables processed: {total_tables_processed}\")\n",
    "print(f\"Strategy combinations: {total_strategies_run} successful, {total_failures} failed\")\n",
    "print(f\"Total pages fetched: {total_pages_fetched}\")\n",
    "print(f\"Total LLM calls: {total_llm_calls_made}\")\n",
    "print(f\"Total tokens used: {total_tokens_used:,}\")\n",
    "print(f\"Total rows fetched: {total_rows_fetched}\")\n",
    "print(f\"Avg latency per page: {avg_latency_per_page:.2f}s\")\n",
    "print(f\"Avg tokens per call: {avg_tokens_per_call:.1f}\")\n",
    "if errors_list:\n",
    "    print(f\"\\n⚠ {len(errors_list)} errors occurred (see _summary.json for details)\")\n",
    "print(f\"\\nFinal summary saved to {summary_file}\")\n",
    "print(f\"All results saved to {output_dir}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496207f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The fetched data has been saved to `processing/2_fetching/<timestamp>/`.\n",
    "\n",
    "Each strategy subdirectory contains:\n",
    "- **JSON files**: Detailed execution logs with per-page metrics, prompts, responses, and aggregated statistics\n",
    "- **CSV files**: Aggregated table data ready for metrics calculation\n",
    "\n",
    "### Metrics Available:\n",
    "\n",
    "**Per-page metrics:**\n",
    "- latency, tokens (prompt/completion/total), retry_count\n",
    "- parse_success, rows_returned, JSON extraction position\n",
    "- timestamp, raw_response, parsed_data\n",
    "\n",
    "**Per-table-strategy metrics:**\n",
    "- total_pages, successful/failed_pages, total_llm_calls\n",
    "- total/avg latency, total_tokens\n",
    "- total_rows_fetched, unique_rows, duplicate_rows\n",
    "- columns_consistent, error_rate\n",
    "\n",
    "### To calculate accuracy metrics:\n",
    "Use the CSV files with the evaluation logic from `X101_Calculate_Metrics.ipynb` to compute:\n",
    "- Keys F1, Precision, Recall\n",
    "- Non-keys F1, Precision, Recall\n",
    "- Overall F1, Precision, Recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
