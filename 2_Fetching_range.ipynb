{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6167645",
   "metadata": {},
   "source": [
    "# Page Fetching for Pagination Strategies\n",
    "\n",
    "This notebook loads pagination plans from Stage 1 and executes the actual data fetching for each strategy.\n",
    "\n",
    "## Strategies:\n",
    "1. **Full Table** - Single query for entire table\n",
    "2. **Row by Row** - Fetch each row individually using key values\n",
    "3. **Attribute-based** - Fetch pages by partition values\n",
    "4. **Classic Pagination** - Offset-based iterative fetching\n",
    "5. **Range-based Alphabetic** - Fetch by alphabetic ranges (A-F, G-L, etc.)\n",
    "6. **Range-based Semantic** - Fetch by semantic categories (continents, etc.)\n",
    "7. **Range-based Unrestricted** - Fetch by unrestricted ranges (benchmark)\n",
    "\n",
    "## Output:\n",
    "- CSV files: Aggregated table data (for metrics calculation)\n",
    "- JSON files: Detailed execution logs with per-page metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a37742",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy directory: /Users/bef/Desktop/TablePagination/processing/1_strategy/20251015_022607\n",
      "Ground truth directory: /Users/bef/Desktop/TablePagination/processing/0_data/20251004_213355\n",
      "Output base: /Users/bef/Desktop/TablePagination/processing/2_fetching\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openai\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# MAX_TABLES: Limit number of tables to process (None for all)\n",
    "MAX_TABLES = 10\n",
    "\n",
    "# MAX_PAGES: Limit pages per table per strategy (None for all, useful for testing)\n",
    "MAX_PAGES = None\n",
    "\n",
    "# PARALLEL_STRATEGIES: Run strategies in parallel per table\n",
    "PARALLEL_STRATEGIES = True\n",
    "\n",
    "# MAX_WORKERS: Number of parallel strategy executions per table\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "# MAX_RETRIES: Number of retries for failed LLM calls\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# MAX_PAGINATION_PAGES: Failsafe for classic_pagination\n",
    "MAX_PAGINATION_PAGES = 10\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "PROCESSING_ROOT = ROOT / 'processing'\n",
    "STRATEGY_DIR = PROCESSING_ROOT / '1_strategy'\n",
    "OUTPUT_ROOT = PROCESSING_ROOT / '2_fetching'\n",
    "\n",
    "# Find the most recent strategy directory\n",
    "strategy_subdirs = sorted([d for d in STRATEGY_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not strategy_subdirs:\n",
    "    raise FileNotFoundError(f\"No strategy data found in {STRATEGY_DIR}\")\n",
    "\n",
    "LATEST_STRATEGY_DIR = strategy_subdirs[0]\n",
    "\n",
    "# Find ground truth data directory\n",
    "DATA_DIR = PROCESSING_ROOT / '0_data'\n",
    "data_subdirs = sorted([d for d in DATA_DIR.iterdir() if d.is_dir()], reverse=True)\n",
    "if not data_subdirs:\n",
    "    raise FileNotFoundError(f\"No ground truth data found in {DATA_DIR}\")\n",
    "\n",
    "LATEST_DATA_DIR = data_subdirs[0]\n",
    "\n",
    "print('Strategy directory:', LATEST_STRATEGY_DIR.resolve())\n",
    "print('Ground truth directory:', LATEST_DATA_DIR.resolve())\n",
    "print('Output base:', OUTPUT_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27259394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API key loaded from api_key.txt\n",
      "Using model: openai/gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "# Configure OpenRouter\n",
    "# Read API key from file\n",
    "api_key_file = ROOT / 'api_key.txt'\n",
    "if not api_key_file.exists():\n",
    "    raise ValueError('No API key found. Please create api_key.txt or set OPENROUTER_API_KEY environment variable')\n",
    "with open(api_key_file, 'r') as f:\n",
    "    OPENROUTER_API_KEY = f.read().strip()\n",
    "print('✓ API key loaded from api_key.txt')\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "MODEL = 'openai/gpt-5-mini'\n",
    "print(f'Using model: {MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429e4b2",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63accf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_field(s: str) -> str:\n",
    "    \"\"\"Normalize field names (matching original experiment logic).\"\"\"\n",
    "    s = s.lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\".\", \"\").replace(\",\",\"_\")\\\n",
    "            .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"','').replace(\"'\",\"\")\\\n",
    "            .replace(\"/\", \"\")\n",
    "    return re.sub('_+', '_', s)\n",
    "\n",
    "\n",
    "def call_llm_with_metrics_split(system_msg: str, user_msg: str, retry_count: int = 0) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call LLM with separate system and user messages (matching original experiment).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    prompt_length = len(system_msg) + len(user_msg)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                     {\"role\": \"user\", \"content\": user_msg}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Check if content is None\n",
    "        if content is None:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'Response content is None',\n",
    "                'metrics': {\n",
    "                    'latency': round(latency, 3),\n",
    "                    'prompt_length': prompt_length,\n",
    "                    'retry_count': retry_count,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        content = content.strip()\n",
    "        response_length = len(content)\n",
    "        \n",
    "        # Extract token usage\n",
    "        usage = response.usage\n",
    "        prompt_tokens = usage.prompt_tokens if usage else 0\n",
    "        completion_tokens = usage.completion_tokens if usage else 0\n",
    "        total_tokens = usage.total_tokens if usage else 0\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'response': content,\n",
    "            'metrics': {\n",
    "                'latency': round(latency, 3),\n",
    "                'prompt_tokens': prompt_tokens,\n",
    "                'completion_tokens': completion_tokens,\n",
    "                'total_tokens': total_tokens,\n",
    "                'prompt_length': prompt_length,\n",
    "                'response_length': response_length,\n",
    "                'retry_count': retry_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        latency = time.time() - start_time\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'metrics': {\n",
    "                'latency': round(latency, 3),\n",
    "                'prompt_length': prompt_length,\n",
    "                'retry_count': retry_count,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def call_llm_with_retries(prompt: str = None, system_msg: str = None, user_msg: str = None, max_retries: int = MAX_RETRIES) -> Dict[str, Any]:\n",
    "    \"\"\"Call LLM with retry logic. Supports either single prompt or system+user messages.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        if system_msg and user_msg:\n",
    "            result = call_llm_with_metrics_split(system_msg, user_msg, retry_count=attempt)\n",
    "        else:\n",
    "            result = call_llm_with_metrics_split(\"You are a retriever of facts.\", prompt, retry_count=attempt)\n",
    "        if result['success']:\n",
    "            return result\n",
    "        print(f\"  Retry {attempt + 1}/{max_retries} after error: {result['error']}\")\n",
    "        time.sleep(1)  # Brief delay between retries\n",
    "    \n",
    "    return result  # Return last failed attempt\n",
    "\n",
    "\n",
    "def parse_json_response(response: str, expect_list: bool = True) -> Tuple[Optional[Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract and parse JSON from LLM response (matching original parsing logic).\n",
    "    \n",
    "    Args:\n",
    "        response: The raw LLM response text\n",
    "        expect_list: True for list responses, False for dict responses\n",
    "    \n",
    "    Returns:\n",
    "        - parsed_data: List/Dict or None\n",
    "        - parse_metrics: dict with parse_success, json_start_pos, json_end_pos\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'parse_success': False,\n",
    "        'json_start_pos': -1,\n",
    "        'json_end_pos': -1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Matching original logic: find JSON boundaries\n",
    "        if expect_list:\n",
    "            start_char, end_char = '[', ']'\n",
    "        else:\n",
    "            start_char, end_char = '{', '}'\n",
    "        \n",
    "        # Extract JSON portion\n",
    "        start_pos = response.find(start_char)\n",
    "        end_pos = response.rfind(end_char)\n",
    "        \n",
    "        if start_pos != -1 and end_pos != -1:\n",
    "            json_str = response[start_pos:end_pos + 1]\n",
    "            metrics['json_start_pos'] = start_pos\n",
    "            metrics['json_end_pos'] = end_pos + 1\n",
    "            \n",
    "            parsed = json.loads(json_str)\n",
    "            \n",
    "            # Handle wrapped responses\n",
    "            if isinstance(parsed, dict) and len(parsed.keys()) == 1:\n",
    "                parsed = list(parsed.values())[0]\n",
    "            \n",
    "            metrics['parse_success'] = True\n",
    "            return parsed, metrics\n",
    "        \n",
    "        # Fallback: manual parsing (matching original fallback logic)\n",
    "        if not expect_list and start_char not in response and end_char not in response:\n",
    "            return None, metrics\n",
    "            \n",
    "        split_response = response.split(\"{\")\n",
    "        response_json = []\n",
    "        for s in split_response[1:]:\n",
    "            split_s = s.split(\"}\")\n",
    "            if len(split_s) > 1:\n",
    "                content = split_s[0]\n",
    "                attributes = content.split(\",\")\n",
    "                elements = {}\n",
    "                for attr in attributes:\n",
    "                    knv = attr.split(\":\")\n",
    "                    if len(knv) > 1:\n",
    "                        parsed_k = \"%s\" % knv[0].replace('\"','').strip()\n",
    "                        parsed_v = \"%s\" % knv[1].replace('\"','').strip()\n",
    "                        elements[parsed_k] = parsed_v\n",
    "                \n",
    "                if elements:\n",
    "                    response_json.append(elements)\n",
    "        \n",
    "        if response_json:\n",
    "            metrics['parse_success'] = True\n",
    "            if expect_list:\n",
    "                return response_json, metrics\n",
    "            else:\n",
    "                return response_json[0] if response_json else {}, metrics\n",
    "        \n",
    "        return None, metrics\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        metrics['parse_error'] = str(e)\n",
    "        return None, metrics\n",
    "    except Exception as e:\n",
    "        metrics['parse_error'] = str(e)\n",
    "        return None, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2aaafa",
   "metadata": {},
   "source": [
    "## Strategy 1: Full Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e49a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_table(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch entire table in one query (matching original B004 approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names and build response format (matching B004)\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    fields_json = []\n",
    "    for field in norm_fields:\n",
    "        fields_json.append(f'\"{field}\": \"{field}\"')\n",
    "    response_format = ', '.join(fields_json)\n",
    "    \n",
    "    # Build prompt matching original B004 template exactly\n",
    "    system_msg = \"You are a retriever of facts.\"\n",
    "    user_msg = f\"\"\"List {table_title} - as many as possible to fit into response.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "    \n",
    "    print(f\"  Fetching full table...\")\n",
    "    \n",
    "    # Call LLM with separate system and user messages\n",
    "    result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "    \n",
    "    page_data = {\n",
    "        'page_number': 1,\n",
    "        'system_msg': system_msg,\n",
    "        'user_msg': user_msg,\n",
    "        'prompt': user_msg,  # For backwards compatibility\n",
    "        'raw_response': result.get('response', ''),\n",
    "        'error': result.get('error'),\n",
    "        **result['metrics']\n",
    "    }\n",
    "    \n",
    "    if result['success']:\n",
    "        parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "        page_data.update(parse_metrics)\n",
    "        page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "        page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "    else:\n",
    "        page_data['parse_success'] = False\n",
    "        page_data['rows_returned'] = 0\n",
    "        page_data['parsed_data'] = []\n",
    "    \n",
    "    return {\n",
    "        'pages': [page_data],\n",
    "        'all_rows': page_data['parsed_data']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b578b05",
   "metadata": {},
   "source": [
    "## Strategy 2: Row by Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a971c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_row_by_row(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch each row individually using key values from plan (matching original B008 approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    key_columns = config['key_columns']\n",
    "    key_values = config['key_values']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    norm_keys = [normalize_field(k) for k in key_columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit if set\n",
    "    keys_to_fetch = key_values[:MAX_PAGES] if MAX_PAGES else key_values\n",
    "    total_keys = len(key_values)\n",
    "    \n",
    "    if MAX_PAGES and len(key_values) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} rows out of {total_keys}\")\n",
    "    \n",
    "    for page_num, key_combo in enumerate(keys_to_fetch, 1):\n",
    "        # Build WHERE clause for this row\n",
    "        key_conditions = []\n",
    "        for key in key_columns:\n",
    "            norm_key = normalize_field(key)\n",
    "            key_value = key_combo.get(norm_key, 'UNKNOWN')\n",
    "            key_conditions.append(f\"{key} = {key_value}\")\n",
    "        row_key = '(' + ', '.join(key_conditions) + ')'\n",
    "        \n",
    "        # Build response format with key values filled in (matching B008)\n",
    "        fields_json = []\n",
    "        for field in norm_fields:\n",
    "            # Use key values where available, field names otherwise\n",
    "            if field in key_combo:\n",
    "                fields_json.append(f'\"{field}\": \"{key_combo[field]}\"')\n",
    "            else:\n",
    "                fields_json.append(f'\"{field}\": \"{field}\"')\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original B008 template exactly\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        key_column_desc = f\"The key column{'s' if len(key_columns) > 1 else ''} in the table {'are' if len(key_columns) > 1 else 'is'} {', '.join(key_columns)}\"\n",
    "        user_msg = f\"\"\"We want to create a table with the detailed information about {table_title}.\n",
    "Columns in the table are {', '.join(columns)}.\n",
    "{key_column_desc}.\n",
    "Retrieve a single row whose key is {row_key}.\n",
    "The response will be formatted as JSON dictionary shown below.\n",
    "Pay special attention to wrap all property names and values in double quotes!\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "{{\n",
    "    {response_format}\n",
    "}}\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching row {page_num}/{len(keys_to_fetch)}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'key_values': key_combo,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            # Parse as dict (single row), then wrap in list\n",
    "            parsed_dict, parse_metrics = parse_json_response(result['response'], expect_list=False)\n",
    "            page_data.update(parse_metrics)\n",
    "            \n",
    "            if parsed_dict:\n",
    "                parsed_data = [parsed_dict]  # Wrap dict in list\n",
    "                page_data['rows_returned'] = 1\n",
    "                page_data['parsed_data'] = parsed_data\n",
    "                all_rows.extend(parsed_data)\n",
    "            else:\n",
    "                page_data['rows_returned'] = 0\n",
    "                page_data['parsed_data'] = []\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6174b1d",
   "metadata": {},
   "source": [
    "## Strategy 3: Attribute-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6eebb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_attribute_based(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch pages by partition values (matching original approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    partition_column = config['partition_column']\n",
    "    partition_values = config['partition_values']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit\n",
    "    values_to_fetch = partition_values[:MAX_PAGES] if MAX_PAGES else partition_values\n",
    "    total_values = len(partition_values)\n",
    "    \n",
    "    if MAX_PAGES and len(partition_values) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} partitions out of {total_values}\")\n",
    "    \n",
    "    for page_num, value in enumerate(values_to_fetch, 1):\n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        user_msg = f\"\"\"List rows from {table_title} where {partition_column} = {value}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching partition {page_num}/{len(values_to_fetch)}: {partition_column}={value}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'partition_value': value,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9e075",
   "metadata": {},
   "source": [
    "## Strategy 4: Classic Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "921ace95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_classic_pagination(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Iterative offset-based pagination (matching original approach).\n",
    "    Stops when LLM returns empty result or hits MAX_PAGINATION_PAGES failsafe.\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    page_size = config['page_size']\n",
    "    sort_order = config['sort_order']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    offset = 0\n",
    "    page_num = 0\n",
    "    \n",
    "    max_pages = min(MAX_PAGES, MAX_PAGINATION_PAGES) if MAX_PAGES else MAX_PAGINATION_PAGES\n",
    "    \n",
    "    while page_num < max_pages:\n",
    "        page_num += 1\n",
    "        \n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        sort_desc = ', '.join(sort_order)\n",
    "        user_msg = f\"\"\"List rows {offset + 1} to {offset + page_size} from {table_title}, sorted by {sort_desc}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "If there are no more rows at this offset, respond with an empty list: []\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching page {page_num} (offset {offset}, size {page_size})...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'offset': offset,\n",
    "            'page_size': page_size,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data and len(parsed_data) > 0:\n",
    "                all_rows.extend(parsed_data)\n",
    "                offset += page_size\n",
    "            else:\n",
    "                # Empty result, stop pagination\n",
    "                print(f\"  Stopping: Empty result at offset {offset}\")\n",
    "                page_data['stop_reason'] = 'empty_result'\n",
    "                pages.append(page_data)\n",
    "                break\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    if page_num >= max_pages:\n",
    "        print(f\"  Stopping: Hit max pages limit ({max_pages})\")\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfb96f",
   "metadata": {},
   "source": [
    "## Strategy 5: Range-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17c11e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_range_based(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch pages by defined ranges (matching original approach).\n",
    "    \"\"\"\n",
    "    meta = plan['metadata']\n",
    "    config = plan['pagination_config']\n",
    "    \n",
    "    table_title = meta['table_title']\n",
    "    columns = meta['columns']\n",
    "    partition_column = config['partition_column']\n",
    "    ranges = config['ranges']\n",
    "    table_id = plan['table_id']\n",
    "    \n",
    "    # Normalize field names\n",
    "    num_fields = len(columns)\n",
    "    norm_fields = [normalize_field(f) for f in columns]\n",
    "    \n",
    "    pages = []\n",
    "    all_rows = []\n",
    "    \n",
    "    # Apply MAX_PAGES limit\n",
    "    ranges_to_fetch = ranges[:MAX_PAGES] if MAX_PAGES else ranges\n",
    "    total_ranges = len(ranges)\n",
    "    \n",
    "    if MAX_PAGES and len(ranges) > MAX_PAGES:\n",
    "        print(f\"  Limited to first {MAX_PAGES} ranges out of {total_ranges}\")\n",
    "    \n",
    "    for page_num, range_spec in enumerate(ranges_to_fetch, 1):\n",
    "        # Handle different range types\n",
    "        if 'category' in range_spec:\n",
    "            # Semantic/category-based range\n",
    "            category = range_spec['category']\n",
    "            filter_condition = f\"{partition_column} represents {category}\"\n",
    "            range_display = f\"{category}\"\n",
    "        else:\n",
    "            # Numeric/alphabetical range with gte/lt\n",
    "            gte = range_spec.get('gte', '')\n",
    "            lt = range_spec.get('lt', '')\n",
    "            filter_condition = f\"{partition_column} >= {gte} and {partition_column} < {lt}\"\n",
    "            range_display = f\"[{gte}, {lt})\"\n",
    "        \n",
    "        # Build response format\n",
    "        fields_json = [f'\"{field}\": \"{field}\"' for field in norm_fields]\n",
    "        response_format = ', '.join(fields_json)\n",
    "        \n",
    "        # Build prompt matching original approach\n",
    "        system_msg = \"You are a retriever of facts.\"\n",
    "        user_msg = f\"\"\"List rows from {table_title} where {filter_condition}.\n",
    "The response will be formatted as JSON shown below.\n",
    "Each element of the response will contain {num_fields} fields: {', '.join(columns)}.\n",
    "Do not output any additional text that is not in JSON format.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "[{{\n",
    "    {response_format}\n",
    "}}]\"\"\"\n",
    "        \n",
    "        print(f\"  Fetching range {page_num}/{len(ranges_to_fetch)}: {partition_column} {range_display}...\")\n",
    "        \n",
    "        result = call_llm_with_retries(system_msg=system_msg, user_msg=user_msg)\n",
    "        \n",
    "        page_data = {\n",
    "            'page_number': page_num,\n",
    "            'range': range_spec,\n",
    "            'system_msg': system_msg,\n",
    "            'user_msg': user_msg,\n",
    "            'prompt': user_msg,\n",
    "            'raw_response': result.get('response', ''),\n",
    "            'error': result.get('error'),\n",
    "            **result['metrics']\n",
    "        }\n",
    "        \n",
    "        if result['success']:\n",
    "            parsed_data, parse_metrics = parse_json_response(result['response'], expect_list=True)\n",
    "            page_data.update(parse_metrics)\n",
    "            page_data['rows_returned'] = len(parsed_data) if parsed_data else 0\n",
    "            page_data['parsed_data'] = parsed_data if parsed_data else []\n",
    "            \n",
    "            if parsed_data:\n",
    "                all_rows.extend(parsed_data)\n",
    "        else:\n",
    "            page_data['parse_success'] = False\n",
    "            page_data['rows_returned'] = 0\n",
    "            page_data['parsed_data'] = []\n",
    "        \n",
    "        pages.append(page_data)\n",
    "    \n",
    "    return {\n",
    "        'pages': pages,\n",
    "        'all_rows': all_rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17e489",
   "metadata": {},
   "source": [
    "## Main Execution: Process All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07b59480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping full_table: directory not found\n",
      "Skipping row_by_row: directory not found\n",
      "Skipping attribute_based: directory not found\n",
      "Skipping classic_pagination: directory not found\n",
      "Found 81 strategy-table combinations\n",
      "\n",
      "Filtering to tables with plans for all 3 strategies:\n",
      "  Total unique tables: 27\n",
      "  Tables with all strategies: 27\n",
      "  Filtered to 81 strategy-table combinations\n",
      "Limited to 3 combinations for 1 tables\n",
      "Processing 3 strategy-table combinations...\n"
     ]
    }
   ],
   "source": [
    "# Strategy function mapping\n",
    "STRATEGY_FUNCTIONS = {\n",
    "    'full_table': fetch_full_table,\n",
    "    'row_by_row': fetch_row_by_row,\n",
    "    'attribute_based': fetch_attribute_based,\n",
    "    'classic_pagination': fetch_classic_pagination,\n",
    "    'range_based_alphabetic': fetch_range_based,\n",
    "    'range_based_semantic': fetch_range_based,\n",
    "    'range_based_unrestricted': fetch_range_based\n",
    "}\n",
    "\n",
    "# Load all strategies\n",
    "strategies_to_process = []\n",
    "range_based_variants = ['alphabetic', 'semantic', 'unrestricted']\n",
    "\n",
    "for variant in range_based_variants:\n",
    "    strategy_name = f'range_based_{variant}'\n",
    "    strategy_dir = LATEST_STRATEGY_DIR / 'range_based'\n",
    "    if not strategy_dir.exists():\n",
    "        print(f\"Skipping {strategy_name}: directory not found\")\n",
    "        continue\n",
    "    \n",
    "    json_files = sorted(strategy_dir.glob('*.json'))\n",
    "    # Filter out error files\n",
    "    json_files = [f for f in json_files if not f.name.startswith('_')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            plan = json.load(f)\n",
    "        # Create a copy of the plan for this variant\n",
    "        variant_plan = plan.copy()\n",
    "        variant_plan['strategy'] = strategy_name\n",
    "        variant_plan['pagination_config'] = plan[f'pagination_config_{variant}']\n",
    "        strategies_to_process.append((strategy_name, variant_plan))\n",
    "\n",
    "# Now load other strategies\n",
    "for strategy_name in ['full_table', 'row_by_row', 'attribute_based', 'classic_pagination']:\n",
    "    strategy_dir = LATEST_STRATEGY_DIR / strategy_name\n",
    "    if not strategy_dir.exists():\n",
    "        print(f\"Skipping {strategy_name}: directory not found\")\n",
    "        continue\n",
    "    \n",
    "    json_files = sorted(strategy_dir.glob('*.json'))\n",
    "    # Filter out error files\n",
    "    json_files = [f for f in json_files if not f.name.startswith('_')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            plan = json.load(f)\n",
    "        strategies_to_process.append((strategy_name, plan))\n",
    "\n",
    "print(f\"Found {len(strategies_to_process)} strategy-table combinations\")\n",
    "\n",
    "# Filter to only tables that have plans for ALL range-based strategies (apples-to-apples comparison)\n",
    "from collections import Counter\n",
    "all_strategies = ['range_based_alphabetic', 'range_based_semantic', 'range_based_unrestricted']\n",
    "table_strategy_counts = Counter(plan['table_id'] for _, plan in strategies_to_process)\n",
    "tables_with_all_strategies = [table_id for table_id, count in table_strategy_counts.items() \n",
    "                               if count == len(all_strategies)]\n",
    "\n",
    "print(f\"\\nFiltering to tables with plans for all {len(all_strategies)} strategies:\")\n",
    "print(f\"  Total unique tables: {len(table_strategy_counts)}\")\n",
    "print(f\"  Tables with all strategies: {len(tables_with_all_strategies)}\")\n",
    "\n",
    "strategies_to_process = [(s, p) for s, p in strategies_to_process \n",
    "                         if p['table_id'] in tables_with_all_strategies]\n",
    "print(f\"  Filtered to {len(strategies_to_process)} strategy-table combinations\")\n",
    "\n",
    "# Apply MAX_TABLES limit across all strategies\n",
    "if MAX_TABLES:\n",
    "    # Group by table_id to ensure we process complete sets\n",
    "    table_ids = list(set(p['table_id'] for _, p in strategies_to_process))[:MAX_TABLES]\n",
    "    strategies_to_process = [(s, p) for s, p in strategies_to_process if p['table_id'] in table_ids]\n",
    "    print(f\"Limited to {len(strategies_to_process)} combinations for {len(table_ids)} tables\")\n",
    "\n",
    "print(f\"Processing {len(strategies_to_process)} strategy-table combinations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e75e9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: processing/2_fetching/20251015_025930\n",
      "\n",
      "Grouped into 1 tables with strategies\n",
      "\n",
      "======================================================================\n",
      "[1/1] Processing table: 8_south_african_class_15f_4_8_2\n",
      "======================================================================\n",
      "Strategies: range_based_alphabetic, range_based_semantic, range_based_unrestricted\n",
      "  Fetching range 1/4: Builder [A, G)...\n",
      "  Fetching range 1/3: Builder South African Railways workshops...\n",
      "  Fetching range 1/11: SAR No. [2900, 3000)...\n",
      "  Fetching range 2/11: SAR No. [3000, 3100)...\n",
      "  Fetching range 2/11: SAR No. [3000, 3100)...\n",
      "  Fetching range 2/3: Builder North British Locomotive Company...\n",
      "  Fetching range 2/3: Builder North British Locomotive Company...\n",
      "  Fetching range 2/4: Builder [G, N)...\n",
      "  Fetching range 2/4: Builder [G, N)...\n",
      "  Fetching range 3/3: Builder Beyer Peacock & Company...\n",
      "  Fetching range 3/3: Builder Beyer Peacock & Company...\n",
      "  Fetching range 3/11: SAR No. [3100, 3200)...\n",
      "  Fetching range 3/11: SAR No. [3100, 3200)...\n",
      "  Fetching range 3/4: Builder [N, T)...\n",
      "  Fetching range 3/4: Builder [N, T)...\n",
      "  ⚠ range_based_semantic/8_south_african_class_15f_4_8_2: No rows to save\n",
      "  ⚠ range_based_semantic/8_south_african_class_15f_4_8_2: No rows to save\n",
      "  Fetching range 4/11: SAR No. [3200, 3300)...\n",
      "  Fetching range 4/11: SAR No. [3200, 3300)...\n",
      "  Fetching range 4/4: Builder [T, Z)...\n",
      "  Fetching range 4/4: Builder [T, Z)...\n",
      "  Fetching range 5/11: SAR No. [3300, 3400)...\n",
      "  Fetching range 5/11: SAR No. [3300, 3400)...\n",
      "  Fetching range 6/11: SAR No. [3400, 3500)...\n",
      "  Fetching range 6/11: SAR No. [3400, 3500)...\n",
      "  ✓ range_based_alphabetic/8_south_african_class_15f_4_8_2: 2 rows, 4841 tokens, 102.99s\n",
      "  ✓ range_based_alphabetic/8_south_african_class_15f_4_8_2: 2 rows, 4841 tokens, 102.99s\n",
      "  Fetching range 7/11: SAR No. [3500, 3600)...\n",
      "  Fetching range 7/11: SAR No. [3500, 3600)...\n",
      "  Fetching range 8/11: SAR No. [3600, 3700)...\n",
      "  Fetching range 8/11: SAR No. [3600, 3700)...\n",
      "  Fetching range 9/11: SAR No. [3700, 3800)...\n",
      "  Fetching range 9/11: SAR No. [3700, 3800)...\n",
      "  Fetching range 10/11: SAR No. [3800, 3900)...\n",
      "  Fetching range 10/11: SAR No. [3800, 3900)...\n",
      "  Fetching range 11/11: SAR No. [3900, 4000)...\n",
      "  Fetching range 11/11: SAR No. [3900, 4000)...\n",
      "  ⚠ range_based_unrestricted/8_south_african_class_15f_4_8_2: No rows to save\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Processed: 3 strategy-table combinations\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "======================================================================\n",
      "  ⚠ range_based_unrestricted/8_south_african_class_15f_4_8_2: No rows to save\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Processed: 3 strategy-table combinations\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = OUTPUT_ROOT / timestamp\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Create subdirectories for each strategy\n",
    "for strategy_name in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_dir = output_dir / strategy_name\n",
    "    strategy_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Group strategies by table_id for parallel execution\n",
    "from collections import defaultdict\n",
    "tables_with_strategies = defaultdict(list)\n",
    "for strategy_name, plan in strategies_to_process:\n",
    "    table_id = plan['table_id']\n",
    "    tables_with_strategies[table_id].append((strategy_name, plan))\n",
    "\n",
    "print(f\"\\nGrouped into {len(tables_with_strategies)} tables with strategies\")\n",
    "\n",
    "# Process each table with parallel strategy execution\n",
    "results_summary = []\n",
    "\n",
    "\n",
    "def process_strategy(strategy_name: str, plan: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single strategy-table combination.\"\"\"\n",
    "    table_id = plan['table_id']\n",
    "    table_name = plan['table_name']\n",
    "    \n",
    "    try:\n",
    "        # Execute fetching strategy\n",
    "        fetch_func = STRATEGY_FUNCTIONS[strategy_name]\n",
    "        fetch_result = fetch_func(plan)\n",
    "        \n",
    "        pages = fetch_result['pages']\n",
    "        all_rows = fetch_result['all_rows']\n",
    "        \n",
    "        # Calculate aggregated metrics\n",
    "        total_pages = len(pages)\n",
    "        successful_pages = sum(1 for p in pages if p.get('parse_success', False))\n",
    "        failed_pages = total_pages - successful_pages\n",
    "        \n",
    "        total_latency = sum(p.get('latency', 0) for p in pages)\n",
    "        avg_latency = total_latency / total_pages if total_pages > 0 else 0\n",
    "        \n",
    "        total_tokens = sum(p.get('total_tokens', 0) for p in pages)\n",
    "        total_llm_calls = sum(1 + p.get('retry_count', 0) for p in pages)\n",
    "        \n",
    "        total_rows_fetched = len(all_rows)\n",
    "        \n",
    "        # Check for duplicate rows\n",
    "        unique_rows = set()\n",
    "        duplicate_count = 0\n",
    "        for row in all_rows:\n",
    "            row_key = tuple(sorted(row.items()))\n",
    "            if row_key in unique_rows:\n",
    "                duplicate_count += 1\n",
    "            unique_rows.add(row_key)\n",
    "        \n",
    "        # Check column consistency\n",
    "        if all_rows:\n",
    "            column_sets = [set(row.keys()) for row in all_rows]\n",
    "            columns_consistent = all(cs == column_sets[0] for cs in column_sets)\n",
    "        else:\n",
    "            columns_consistent = True\n",
    "        \n",
    "        error_rate = failed_pages / total_pages if total_pages > 0 else 0\n",
    "        \n",
    "        # Build execution summary\n",
    "        execution_summary = {\n",
    "            'table_id': table_id,\n",
    "            'table_name': table_name,\n",
    "            'strategy': strategy_name,\n",
    "            'metadata': plan['metadata'],\n",
    "            'pagination_config': plan['pagination_config'],\n",
    "            'execution_metadata': {\n",
    "                'timestamp': timestamp,\n",
    "                'total_pages': total_pages,\n",
    "                'successful_pages': successful_pages,\n",
    "                'failed_pages': failed_pages,\n",
    "                'total_llm_calls': total_llm_calls,\n",
    "                'total_latency': round(total_latency, 3),\n",
    "                'avg_latency': round(avg_latency, 3),\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_rows_fetched': total_rows_fetched,\n",
    "                'unique_rows': len(unique_rows),\n",
    "                'duplicate_rows': duplicate_count,\n",
    "                'columns_consistent': columns_consistent,\n",
    "                'error_rate': round(error_rate, 4)\n",
    "            },\n",
    "            'pages': pages\n",
    "        }\n",
    "        \n",
    "        # Save JSON log\n",
    "        json_path = output_dir / strategy_name / f\"{table_id}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(execution_summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save CSV if we have data (matching B004 approach: normalize columns to match reference)\n",
    "        if all_rows:\n",
    "            try:\n",
    "                # Load reference table to get proper column names\n",
    "                ref_csv_path = LATEST_DATA_DIR / f\"{table_id}.csv\"\n",
    "                df_ref = pd.read_csv(ref_csv_path)\n",
    "                \n",
    "                # Create dataframe from fetched rows\n",
    "                df = pd.DataFrame(all_rows)\n",
    "                \n",
    "                # Normalize fetched column names\n",
    "                df.columns = [normalize_field(col) for col in df.columns]\n",
    "                \n",
    "                # Normalize reference column names to create mapping\n",
    "                norm_ref_cols = [normalize_field(col) for col in df_ref.columns]\n",
    "                \n",
    "                # Ensure fetched df has same columns as reference (reorder and add missing)\n",
    "                missing_cols = [col for col in norm_ref_cols if col not in df.columns]\n",
    "                for col in missing_cols:\n",
    "                    df[col] = None  # Add missing columns with None\n",
    "                \n",
    "                # Reorder to match reference\n",
    "                df = df[norm_ref_cols]\n",
    "                \n",
    "                # Restore original column names from reference\n",
    "                df.columns = df_ref.columns\n",
    "                \n",
    "                # Drop duplicates based on key columns\n",
    "                key_columns = plan['metadata']['key_columns']\n",
    "                if key_columns:\n",
    "                    df = df.drop_duplicates(subset=key_columns)\n",
    "                \n",
    "                csv_path = output_dir / strategy_name / f\"{table_id}.csv\"\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "                print(f\"  ✓ {strategy_name}/{table_id}: {len(df)} rows, {total_tokens} tokens, {total_latency:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ {strategy_name}/{table_id}: CSV save failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"  ⚠ {strategy_name}/{table_id}: No rows to save\")\n",
    "        \n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'table_id': table_id,\n",
    "            'success': True,\n",
    "            **execution_summary['execution_metadata']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {strategy_name}/{table_id}: Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'table_id': table_id,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Process each table with parallel strategies\n",
    "for table_num, (table_id, strategies) in enumerate(tables_with_strategies.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{table_num}/{len(tables_with_strategies)}] Processing table: {table_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Strategies: {', '.join(s for s, _ in strategies)}\")\n",
    "    \n",
    "    if PARALLEL_STRATEGIES:\n",
    "        # Execute strategies in parallel\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {\n",
    "                executor.submit(process_strategy, strategy_name, plan): (strategy_name, plan['table_id'])\n",
    "                for strategy_name, plan in strategies\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                results_summary.append(result)\n",
    "    else:\n",
    "        # Sequential execution (for debugging)\n",
    "        for strategy_name, plan in strategies:\n",
    "            print(f\"\\n  {strategy_name.upper()}\")\n",
    "            result = process_strategy(strategy_name, plan)\n",
    "            results_summary.append(result)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed: {len(results_summary)} strategy-table combinations\")\n",
    "print(f\"Successful: {sum(1 for r in results_summary if r['success'])}\")\n",
    "print(f\"Failed: {sum(1 for r in results_summary if not r['success'])}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb5616",
   "metadata": {},
   "source": [
    "## Save Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9c0ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXECUTION SUMMARY\n",
      "======================================================================\n",
      "Total execution time: 3.79 minutes\n",
      "Tables processed: 1\n",
      "Strategy combinations: 3 successful, 0 failed\n",
      "Total pages fetched: 18\n",
      "Total LLM calls: 18\n",
      "Total tokens used: 17,400\n",
      "Total rows fetched: 2\n",
      "Avg latency per page: 21.28s\n",
      "Avg tokens per call: 966.7\n",
      "\n",
      "Final summary saved to processing/2_fetching/20251015_025930/_summary.json\n",
      "All results saved to processing/2_fetching/20251015_025930\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate aggregate statistics across all strategies\n",
    "execution_end_time = datetime.now()\n",
    "# Parse timestamp: format is YYYYMMDD_HHMMSS\n",
    "execution_start_time = datetime.strptime(timestamp, '%Y%m%d_%H%M%S')\n",
    "\n",
    "total_execution_time = (execution_end_time - execution_start_time).total_seconds()\n",
    "\n",
    "# Aggregate metrics\n",
    "total_tables_processed = len(set(r['table_id'] for r in results_summary if r['success']))\n",
    "total_strategies_run = len([r for r in results_summary if r['success']])\n",
    "total_failures = len([r for r in results_summary if not r['success']])\n",
    "\n",
    "successful_results = [r for r in results_summary if r.get('success', False)]\n",
    "\n",
    "total_pages_fetched = sum(r.get('total_pages', 0) for r in successful_results)\n",
    "total_llm_calls_made = sum(r.get('total_llm_calls', 0) for r in successful_results)\n",
    "total_tokens_used = sum(r.get('total_tokens', 0) for r in successful_results)\n",
    "total_latency_seconds = sum(r.get('total_latency', 0) for r in successful_results)\n",
    "total_rows_fetched = sum(r.get('total_rows_fetched', 0) for r in successful_results)\n",
    "\n",
    "# Calculate averages\n",
    "avg_latency_per_page = total_latency_seconds / total_pages_fetched if total_pages_fetched > 0 else 0\n",
    "avg_tokens_per_call = total_tokens_used / total_llm_calls_made if total_llm_calls_made > 0 else 0\n",
    "avg_pages_per_strategy = total_pages_fetched / total_strategies_run if total_strategies_run > 0 else 0\n",
    "\n",
    "# Collect all errors\n",
    "errors_list = [\n",
    "    {\n",
    "        'strategy': r['strategy'],\n",
    "        'table_id': r['table_id'],\n",
    "        'error': r.get('error', 'Unknown error')\n",
    "    }\n",
    "    for r in results_summary if not r.get('success', False)\n",
    "]\n",
    "\n",
    "# Strategy breakdown\n",
    "strategy_breakdown = {}\n",
    "for strategy in STRATEGY_FUNCTIONS.keys():\n",
    "    strategy_results = [r for r in successful_results if r.get('strategy') == strategy]\n",
    "    if strategy_results:\n",
    "        strategy_breakdown[strategy] = {\n",
    "            'tables_processed': len(strategy_results),\n",
    "            'total_pages': sum(r.get('total_pages', 0) for r in strategy_results),\n",
    "            'total_tokens': sum(r.get('total_tokens', 0) for r in strategy_results),\n",
    "            'total_latency': round(sum(r.get('total_latency', 0) for r in strategy_results), 3),\n",
    "            'avg_latency': round(sum(r.get('avg_latency', 0) for r in strategy_results) / len(strategy_results), 3),\n",
    "            'total_rows': sum(r.get('total_rows_fetched', 0) for r in strategy_results),\n",
    "            'error_rate': round(sum(r.get('error_rate', 0) for r in strategy_results) / len(strategy_results), 4)\n",
    "        }\n",
    "\n",
    "# Build comprehensive summary\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'execution_time': {\n",
    "        'start': timestamp,\n",
    "        'end': execution_end_time.strftime('%Y%m%d_%H%M%S'),\n",
    "        'total_seconds': round(total_execution_time, 2),\n",
    "        'total_minutes': round(total_execution_time / 60, 2)\n",
    "    },\n",
    "    'configuration': {\n",
    "        'strategy_directory': str(LATEST_STRATEGY_DIR),\n",
    "        'max_tables': MAX_TABLES,\n",
    "        'max_pages': MAX_PAGES,\n",
    "        'max_retries': MAX_RETRIES,\n",
    "        'max_pagination_pages': MAX_PAGINATION_PAGES,\n",
    "        'model': MODEL\n",
    "    },\n",
    "    'aggregate_metrics': {\n",
    "        'total_tables_processed': total_tables_processed,\n",
    "        'total_strategy_table_combinations': total_strategies_run,\n",
    "        'total_failures': total_failures,\n",
    "        'success_rate': round(total_strategies_run / (total_strategies_run + total_failures), 4) if (total_strategies_run + total_failures) > 0 else 0,\n",
    "        'total_pages_fetched': total_pages_fetched,\n",
    "        'total_llm_calls': total_llm_calls_made,\n",
    "        'total_tokens_used': total_tokens_used,\n",
    "        'total_latency_seconds': round(total_latency_seconds, 2),\n",
    "        'total_rows_fetched': total_rows_fetched,\n",
    "        'avg_latency_per_page': round(avg_latency_per_page, 3),\n",
    "        'avg_tokens_per_call': round(avg_tokens_per_call, 1),\n",
    "        'avg_pages_per_strategy': round(avg_pages_per_strategy, 1)\n",
    "    },\n",
    "    'strategy_breakdown': strategy_breakdown,\n",
    "    'errors': errors_list,\n",
    "    'detailed_results': results_summary\n",
    "}\n",
    "\n",
    "summary_file = output_dir / '_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total execution time: {summary['execution_time']['total_minutes']:.2f} minutes\")\n",
    "print(f\"Tables processed: {total_tables_processed}\")\n",
    "print(f\"Strategy combinations: {total_strategies_run} successful, {total_failures} failed\")\n",
    "print(f\"Total pages fetched: {total_pages_fetched}\")\n",
    "print(f\"Total LLM calls: {total_llm_calls_made}\")\n",
    "print(f\"Total tokens used: {total_tokens_used:,}\")\n",
    "print(f\"Total rows fetched: {total_rows_fetched}\")\n",
    "print(f\"Avg latency per page: {avg_latency_per_page:.2f}s\")\n",
    "print(f\"Avg tokens per call: {avg_tokens_per_call:.1f}\")\n",
    "if errors_list:\n",
    "    print(f\"\\n⚠ {len(errors_list)} errors occurred (see _summary.json for details)\")\n",
    "print(f\"\\nFinal summary saved to {summary_file}\")\n",
    "print(f\"All results saved to {output_dir}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496207f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The fetched data has been saved to `processing/2_fetching/<timestamp>/`.\n",
    "\n",
    "Each strategy subdirectory contains:\n",
    "- **JSON files**: Detailed execution logs with per-page metrics, prompts, responses, and aggregated statistics\n",
    "- **CSV files**: Aggregated table data ready for metrics calculation\n",
    "\n",
    "### Metrics Available:\n",
    "\n",
    "**Per-page metrics:**\n",
    "- latency, tokens (prompt/completion/total), retry_count\n",
    "- parse_success, rows_returned, JSON extraction position\n",
    "- timestamp, raw_response, parsed_data\n",
    "\n",
    "**Per-table-strategy metrics:**\n",
    "- total_pages, successful/failed_pages, total_llm_calls\n",
    "- total/avg latency, total_tokens\n",
    "- total_rows_fetched, unique_rows, duplicate_rows\n",
    "- columns_consistent, error_rate\n",
    "\n",
    "### To calculate accuracy metrics:\n",
    "Use the CSV files with the evaluation logic from `X101_Calculate_Metrics.ipynb` to compute:\n",
    "- Keys F1, Precision, Recall\n",
    "- Non-keys F1, Precision, Recall\n",
    "- Overall F1, Precision, Recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
