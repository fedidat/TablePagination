{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cfa657",
   "metadata": {},
   "source": [
    "# Stage 4: Analyze and Visualize Pagination Metrics\n",
    "\n",
    "This notebook loads all metrics.json files from the output folders, aggregates the results, and plots key metrics (accuracy, latency, error rate, etc.) per pagination method and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017475cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate all metrics.json files in the latest processing/2_fetched_pages/ folder\n",
    "fetched_root = 'processing/2_fetched_pages/'\n",
    "folders = [f for f in os.listdir(fetched_root) if os.path.isdir(os.path.join(fetched_root, f))]\n",
    "latest_folder = sorted(folders)[-1] if folders else None\n",
    "assert latest_folder, 'No fetched pages folder found.'\n",
    "metrics_files = glob.glob(os.path.join(fetched_root, latest_folder, '*_metrics.json'))\n",
    "print(f'Using folder: {latest_folder}, found {len(metrics_files)} metrics files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47372bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics from all files into a DataFrame\n",
    "records = []\n",
    "for fpath in metrics_files:\n",
    "    with open(fpath, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    meta = obj.get('meta', {})\n",
    "    for key, res in obj.get('results', {}).items():\n",
    "        method, model = key.split('_', 1) if '_' in key else (key, None)\n",
    "        rec = {\n",
    "            'table_id': meta.get('id'),\n",
    "            'table_name': meta.get('name'),\n",
    "            'method': method,\n",
    "            'model': model,\n",
    "            'row_count': res.get('row_count'),\n",
    "            'column_consistency': res.get('column_consistency'),\n",
    "            'error_rate': res.get('error_rate'),\n",
    "            'avg_latency': res.get('avg_latency'),\n",
    "            'avg_tokens': res.get('avg_tokens'),\n",
    "            'accuracy_recall': res.get('accuracy', {}).get('row_recall'),\n",
    "            'accuracy_precision': res.get('accuracy', {}).get('row_precision'),\n",
    "            'accuracy_f1': res.get('accuracy', {}).get('row_f1')\n",
    "        }\n",
    "        records.append(rec)\n",
    "df_metrics = pd.DataFrame(records)\n",
    "print(f'Aggregated metrics for {len(df_metrics)} table-method-model combinations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb38a31",
   "metadata": {},
   "source": [
    "## Plot Metrics by Pagination Method and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809226d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy (F1) by Method and Model\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=df_metrics, x='method', y='accuracy_f1', hue='model')\n",
    "plt.title('Row-level F1 Accuracy by Pagination Method and Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xlabel('Pagination Method')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Error Rate by Method and Model\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=df_metrics, x='method', y='error_rate', hue='model')\n",
    "plt.title('Error Rate by Pagination Method and Model')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.xlabel('Pagination Method')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Average Latency by Method and Model\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=df_metrics, x='method', y='avg_latency', hue='model')\n",
    "plt.title('Average Latency (seconds) by Pagination Method and Model')\n",
    "plt.ylabel('Latency (s)')\n",
    "plt.xlabel('Pagination Method')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487041b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Total Token Usage by Method and Model\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=df_metrics, x='method', y='total_tokens', hue='model')\n",
    "plt.title('Total Token Usage by Pagination Method and Model')\n",
    "plt.ylabel('Tokens')\n",
    "plt.xlabel('Pagination Method')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
