{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Starting Data\n",
    "\n",
    "- Parse `cfg.json`\n",
    "- Load each CSV from `benchmark_tables/` into a DataFrame\n",
    "- Keep only tables with more than 200 cells and up to 10 columns\n",
    "- Write one JSON per table with both metadata and table records under a timestamped folder in `processing/0_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81abc578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config at: /Users/bef/Desktop/TablePagination/benchmark_tables/cfg.json\n",
      "Tables directory: /Users/bef/Desktop/TablePagination/benchmark_tables\n",
      "Output base: /Users/bef/Desktop/TablePagination/processing/0_data\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('.')\n",
    "CFG_PATH = ROOT / 'benchmark_tables' / 'cfg.json'\n",
    "TABLES_DIR = ROOT / 'benchmark_tables'\n",
    "OUTPUT_ROOT = ROOT / 'processing' / '0_data'\n",
    "\n",
    "print('Using config at:', CFG_PATH.resolve())\n",
    "print('Tables directory:', TABLES_DIR.resolve())\n",
    "print('Output base:', OUTPUT_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4770797c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and validate cfg.json\n",
    "with open(CFG_PATH, 'r', encoding='utf-8') as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# Expect cfg to be a mapping of string keys to metadata dicts\n",
    "assert isinstance(cfg, dict) and len(cfg) > 0, 'cfg.json should be a non-empty object'\n",
    "len(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6370d6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('processing/0_data/20251003_115653')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare timestamped output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "out_dir = OUTPUT_ROOT / timestamp\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727a6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 74, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper: read a table safely\n",
    "def load_table(csv_path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(csv_path)\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin-1 if utf-8 fails\n",
    "        return pd.read_csv(csv_path, encoding='latin-1')\n",
    "\n",
    "# Convert DataFrame to JSON-serializable records\n",
    "def df_to_records(df: pd.DataFrame):\n",
    "    # Ensure NaNs become null in JSON\n",
    "    return json.loads(df.to_json(orient='records'))\n",
    "\n",
    "processed = []\n",
    "skipped = []\n",
    "errors = []\n",
    "\n",
    "for key, meta in cfg.items():\n",
    "    # Defensive checks\n",
    "    if not isinstance(meta, dict):\n",
    "        errors.append((key, 'invalid_meta'))\n",
    "        continue\n",
    "    csv_name = meta.get('file')\n",
    "    if not csv_name:\n",
    "        errors.append((key, 'missing_file'))\n",
    "        continue\n",
    "    csv_path = TABLES_DIR / csv_name\n",
    "    if not csv_path.exists():\n",
    "        errors.append((key, f'file_not_found: {csv_path}'))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = load_table(csv_path)\n",
    "    except Exception as e:\n",
    "        errors.append((key, f'read_error: {e}'))\n",
    "        continue\n",
    "\n",
    "    # Filter: only keep tables with > 200 cells and up to 10 columns\n",
    "    num_rows, num_cols = df.shape\n",
    "    total_cells = num_rows * num_cols\n",
    "    \n",
    "    if total_cells <= 200:\n",
    "        skipped.append((key, f'too_few_cells: {total_cells}'))\n",
    "        continue\n",
    "    \n",
    "    if num_cols > 10:\n",
    "        skipped.append((key, f'too_many_columns: {num_cols}'))\n",
    "        continue\n",
    "\n",
    "    # Build output payload\n",
    "    payload = {\n",
    "        'meta': {**meta, 'source_file': str(csv_path)},\n",
    "        'table': df_to_records(df)\n",
    "    }\n",
    "\n",
    "    # Filename: `<id>_<name>.json` with safe characters\n",
    "    id_part = str(meta.get('id', key))\n",
    "    name_part = str(meta.get('name', Path(csv_name).stem))\n",
    "    safe_name = ''.join(c if c.isalnum() or c in ('-', '_') else '_' for c in name_part)\n",
    "    out_path = out_dir / f'{id_part}_{safe_name}.json'\n",
    "\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(payload, f, ensure_ascii=False)\n",
    "    out_csv = out_dir / f'{id_part}_{safe_name}.csv'\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    processed.append((key, out_path.name, df.shape[0], df.shape[1]))\n",
    "\n",
    "len(processed), len(skipped), len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bfa11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: processing/0_data/20251003_115653\n",
      "Processed tables: 26\n",
      "Skipped (<=100 rows): 74\n",
      "Errors: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('0', '0_republican_straw_polls_2012.json', 113, 11),\n",
       " ('2', '2_belgium_demographics_1900_2011.json', 112, 9),\n",
       " ('3', '3_australia_demographics_1900_2010.json', 111, 9),\n",
       " ('4', '4_new_brunswick_parishes_2006_2011.json', 152, 7),\n",
       " ('5', '5_ice_hockey_2006.json', 244, 7)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary\n",
    "print('Output directory:', out_dir)\n",
    "print('Processed tables:', len(processed))\n",
    "print('Skipped (<=100 rows):', len(skipped))\n",
    "print('Errors:', len(errors))\n",
    "\n",
    "# Show a few sample outputs\n",
    "processed[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
